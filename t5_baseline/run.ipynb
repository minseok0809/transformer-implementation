{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Transaltion with T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development Envrionment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import fitz\n",
    "from PIL import Image\n",
    "from termcolor import colored\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae319cbaf444afdba72e1c10d31c697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c4fbe32c064a5a90c3b97904f646dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cacb5efd39024dd1bbd038faba45788e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 116654\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 888\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1138\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('./data/iwslt17.de.en')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Baseline Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/huggingface/transformers/archive/refs/heads/master.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[HuggingFace Transformers Official Gihub Repository](https://github.com/huggingface/transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ./master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip master.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify Baseline Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Weight Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```   \n",
    "model = AutoModelForSeq2SeqLM.from_config(\n",
    "        config=config,\n",
    "        trust_remote_code=args.trust_remote_code,\n",
    "    )\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pdf_page(title, load_path, page_num, crop_area):\n",
    "    \n",
    "    print(colored(title, attrs=['bold']))\n",
    "    print()\n",
    "    \n",
    "    file_handle = fitz.open(load_path)\n",
    "    \n",
    "    if page_num < 10:\n",
    "        num_path =  \"0\" + str(page_num + 1)\n",
    "    elif page_num >= 10:\n",
    "        num_path =  str(page_num)\n",
    "\n",
    "    save_path = 'paper_' + num_path + \".png\"\n",
    "\n",
    "    page = file_handle[page_num]\n",
    "    page_img = page.get_pixmap()\n",
    "    \n",
    "    page_img.save(save_path)\n",
    "    img = Image.open(save_path)\n",
    "    cropped_img = img.crop(crop_area)\n",
    "    cropped_img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQgJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCACCAaQDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+q0Go2NzdzWsF7by3MP+thjlVnj/AN4A5H41ZridA03VNF1HUL1LC+mgvp1LW909t5sbNI291dDzHgg7WO70HWgDr7a9tLwyi1uoZzE+yTypA2xvQ46H2qeuW8L6Ve2uua1qs8EttDqYgkW3uBF5sUiKyMuYyQUChMck/e+p2desbjU/Dup2FpP5Fzc2ssMUucbGZSAePQmgCxb31pduyW91BMycsscgYjnHOPcH8qnZlQZZgB6k4ryrXtK1mx8PafqcMT6U3h/w5eRTSLKoLSmBQiJtJyAybsnjp3yA630vxTqN5p2pOt7Pp6TySwIbxfMVHswocnfgqZSxAySAx4AOKAPUYZoriGOaCRJYpFDI6MGVlPIII6ikWeF53gWWNpo1DPGGBZQc4JHUA4OPoa8v07SPHMf9mxzpfxwxrp8cyR3kQUIIHW5wA/8Ae2dOc8r60yDTfiAdMmE1vdrqNxZ2Nv8AaFuoQYpF80TSHD/NhWUgdC2PegD1eivNb2w8cyWGuC3a/S48nGnKLiLkFItoZt/EissmTjB3HkggD0KxtjaWaQmaeUglt1w+5+STgkemcD2AoAsUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFIc4OOD2zXEW/ja7+2pZXMVstyUuIGVUbi7RyI4+T0kUFl74HbIoA7iiszTtat9RaaOHfI8KBmkVMJJyR8pzjqp4Jz0PQgmpD4v0qeOB0aUCcQmIMoG/wA2MyIBk9SoPHXPAzQBvEZ60dKxY/E9hPP5EHmvK7iOHCfLM2HOFOccCNyQSDgZ7jNWDxPI3hzQr+S2JuNVgVgE+5G5gaY5BOcfIfegDpKK5qz8Z2UmkxXNzHMlyY0Z7dI8scxCQlRnJG08d+2M8Vd8Ra4ND0Zr5I1ml6xxM+zzMAswBPfarYHc4HegDYorAv8AxNBHHaNp5W586WzLMBlRFPKEVs5HUbiOvTnqKWXxdp0SMdly7LIYzGsfzBhF5pBBPHyAnnuCOvFAG9RWVF4i0+XT7y9LukNoR5u9MHBRXUgd8q6kd+cdeKq3PjDTbRp0ljuvMtxK08axbjEIwjOTg9llRuM5B4yeKAN+iub/AOEwtFWSeVJYreJLxmHlhmcW8qxsykNjGSeMZOR6cuuPF1lHbXjbZoJLeGeRjLDuA8kqH4B5xvU8HkHg8HAB0VFY8/iSytrye1lSZZYduchcEsxCgHPU7ScHHAz0qTTddtdVuHhtorkFI0kdpYigXcAyg55zg9McYNAGpRXNLrOrS6bYX0MNo6zXKxPDhgzKZdh288FVBck5yAeBjNVT4vuWsBdLBEBNZm9gBB+4JAu1ueuGQ59c8cUAdfRWVqXiCy0m6SC68xS6oytgbSGkWPqT2Z0z6Bgaqz+J4YbwQiKWTf8AZgiKgBHmyOgfcWwVOzoOfz4AN+isPw34gGt2oEsZiu1hSaRQuFKszqCvJ4zGw5549CCZ4tWZtU8iUJFbmJpFd/lz8yhSDnkHOegxkDnNAGrRUU9wlugd1kIJx+7iZz+Sgmql7q8Nnot9qflzNHZwSTMjxtGWCqWIG4D0oA0KKxZNanhs9aY2v2i50xC4ihyPOPlBwB1IJJI79j3xVjSNUGovfxZRzZ3AgMkf3ZMxRyZHXtIB1PINAGlRWXcapJDqlvb7VWF5TGzuOOI2c4bPXjpjsavvcRpb+flnjwCDGpfIPoFyTQBLRUFvdx3JYRrMNvXzIXj/AC3AZrL0TXjqzQ5iCrcWMV9FjqEkLfKfcADn3PpQBt0VyEfjVhAr3NqyN9ntriYRxNI1uJhL8jIPmZkMRDY5AOcAA1tza7bWejW+pXRxDLEJWaIh0UbdxO4cEY9OvbNAGpRWGPFem/aGgbzlfzGiQMmPMdZRCVXn++yjnA+YH1wWHiGN9C/tHUStv/pUtvtA6lZmjUYyeTtGefXtQBuUVDNcpBGryJLhuyRNIR9QoNENwtxEzxLIMHGJY2j5+jAHHvQBNRXLy+K5IdLW5a3TzFsrq9dQTgrAygqPQnd17Y6Vt3bX7W9wLdIYyYm8qXeWYNg7Ts24POOM0AXaKo6U141mxvfv+bIEz1Me47c8DnGO361zl54wuNM11rC9W2SOO+jilYKwK20qfu5s5wB5v7sk8Z5oA7GisXS9fjvJ0s5Q32sqWfZGQiHCsEbk4ba69eM5x0xUq+ILRnmQRzl4JGikXaMowKDnngHepBPGOelAGrRXPDxbY3EKSWvnGNjanzmhJTE7IEHUHJDj6dT0wZU8U2Fx8lt5skkmBBiPKzZV2BU5wRiNiQSDwP7y5ANyiuYfxVJH4a0q/NruutQsjcALjy42EJlOcnOOMcVLB4wsGt0MyTrcjAkgWPLD5Y2LAZ5GJYzxk/N0yCAAdFRWNqGurpusrbXGxbVrXzd2Pm8wypGq9cclwPr3pr+KtOiigklW4jEyMyB4iCSqyMRjucRP0yOnPIyAbdFYUXiuxmuYrfybqOSUKVLxYHzq7Jnn+IRtj8M4zTbLxZYXH2KNmkL3MMMglERVP3iM69SSMhG45xjB6jIBv0Vn6TrNrrMUstoJfLjKjc64DbkV1I9QVdT+OOua0KACiiigAqg2iaa9y9w1nEZnuVumcjkyqgRX+oUAVfrjE8N61DBZJC1mJI7kzzyfaZNzD7QjgAlTkGJSnbHAzgnIB0unaZZacsn2JWVJHZipmZ1UkkkKCSFGSeFwKpQ+EtFgt3t0tX8l0SNo2uJGUoowq4LHgDoPYegrK0bwvqGn3dgJjbm1tYlVPJnZSjq0mSFCDcHV1yCRgj+LAqze+GrvUNTuGuLofZJrhnLJIyyeS1v5Rh6Yxv8A3gOevOM80AaA8MaSqKiwSqqTm4jxcyDypDuyU+b5AdzAhcAgkYxT20HSxp1jYtCy21jgWw89wU+Qxgbs5PyuV5PeseLw7qv2uxmu54bkqiPcMkrRbZlbO5AFOQy4UjK8KM5yafH4auV8G6fpjrbSX0EtrNMXkZkkaKRGY7iueVQjp3oAsWPh7w/dQQ3VlHNtVl8uVLmZWBjUxYzuBAwNpHfHOTzW0bOA3cdztbzYo2iTDkAKxBPy5x/COcZ4ri4fCetW4xG9n5bkGeD7Q4WY+bKxBJjOPlkXnB5QDpg1uXukXrw6XDB5U1vbxNDPDcXEgJyoAcOAWLLg9cE7icggUAOg8HaHbLCsNrKiwiMIouZcYjkMkYPzchWJIznGcdOKIfB2h28aRx2swRBtUG6lOB5Ri7t/cJX/APUKy5/DOsPKJVvIjJHO8uTKy/aAbqOZA/B27URoweeG4wOKjs/CuqJ9ta5lhV5IkS3aG5YmIieZyQGTH3JEGMEHZgjFAHRR+HtLjs7u0FsWgvFVJ0eR23gIEHUnHyqBx6Z681n6j4d08XiX07BLJIZ1uld3LTmURjLNnJ4jC4OcjA7Cm6JoepWeqfadQlgl2xKqSQyuAB5aKyeWR93chYEsevQEk1n33hTVbuKZWntpSxnz5kjATh7hJY942nBjRWQdevGBxQBrnw3oV9GWa2l23Ec/yGaWMlZmVpfl3DbkhSeBg+madeeGNEkiuXuLaZlmWZZds8pLCXaJBgNnnavTpjjHNVPE3h681SeOawFsjrYT2oZ5GjKM7RFSpVScDy2HbrVPUPDmtTXV39iNnBavA8MKC5kXGfI2EjYcY8uQcHHzdOTQBs3HhzRwk7yw3H76cTOyXExbzcrhgQ2QRtUZHQDHAyKv2mmWljcTzwI4ln2+azSs27aMDqTzjv375rlJfDGtebM8MtqvmyTOf9JkG7N0ksYPyfwoHX6tjoc1BeaNrGl291cLskEsrHZFJK+7N4ZUDgRn5BEdp46ZHTmgDorfwzapBbJPNcu1sX8toriWEYZt2GVHAb8addeG7OWyuLe3HkGdfLZ+X2puLFVBOFGSenHT0GDQUuoNKhtPshhjitlEbzSszs+WBDAqCBwpz1O7oMViaf4f1+GdDeGymtyx324upMITFCu9TszlWjkx3Pmk7s5yAdNqOj6fqwUX1sk21JEUnIIV12sOPUGorjQNMnnNzJbv5myJcpK64ETF48BTxgsenXODkVjQaBqohtBc/ZJHhnzcjz3K3qbJFDOCuFbLq2MEZXGeFxWsfCusWYiaa5t72WOeEyGed/8ASIltlidWypwfMUSd8kDPNAG1olho8cC3WjCQLLaRrHKzyMpiO50xvPPLsfXnB7VNcaHC+nyW9s3lSPEsQll3SlVU5AwT/IiuZsfCGrwWunW07Wbw20VtHIguZMOI7eSJuNnQsyH3A55qTRLPUofEdvFqEc0z2sMcbXG6Qr5nkKHILIAyEg45zuyfUAA7GWC3u08ueKKcKeVdQwBx6HocH9aq3a6Xp2nXCzxQQ2jIfOQRgBlOAcgDkcgH61z134W1KS71BoJLZYbl7hlXzGU7pEjCS8Lw6MjY9nJyDxU/iTw3e6qYmt2tpJF0+e0aSdyjbnaIhgVU/wDPNs/71AG4mkWiJIqiYeYSXYTuC+UCckHn5QAPTGevNQpDY2+rJHHLMLrDSyIrOQ+8Y3Sdv+WeFLdNuB6ViWvhvVItTjaaSBrGO4eWKNLl1MA84uu35OchtpXIGFA5BIqzqWgXtzr11qFubVEljs0wzENKIpZGkRsLwGV1HU524IxQBqS6QjRM0bL9pDPJFJMpkSN2OdwTcBkZ7Y+vJqWz06ODS4rG4WKdVGGzH8rHOc4JPfnqa5X/AIRHVP8ASd88MhZbcQFriTMQS5kkdR8vA8t0jB6nYM4pU8LaxCwMMlooWfzQouHwyreeaifc4AhyntnGMUAdTajToL6eztYoYrlI0kkSOLb8rFgpyBzyrflSW+j2VqQYI3jwEVdsjDCoSVUYP3QWPHTnHSuXt/DOtwIwJsXL20ED5nfokszHHyeki4yCODx0NdHodpd2enxR3yxG5WGJZZUmZ/NdUAZjlRjkH69aAJF0ayUqwWTzVK7ZTKxcbVKqNxOcAM3H+0T1JNRX3h3S9Rs4bS5t2MEMbRRokzphGXaRlSCRjjmtSigDEk8JaLKQz2shYM7q32mXKs8iyswO7g70VsjpjjFW4tFsILNLSOJxEkzXC5lcsJGYsW3E5ySzd+5HStCigCGa1t7mNUuII5lXkCRQ2PzpYbWC3iaOCGOFGOSsahefXipaKAMseHtMFqlt5DGJUkj2tK7ZSQgupJJJUkDI9q1KKKAGGJTOs2X3KpUDeduCQfu5wTx1xnr6mqd5omm6g873dnFM08At5C4yWjBLBfpkk1fooApQ6VZ2+o3F9EjrPcENLiV9jMFC7tmdobAAzjPFR3Gh6ddSTySW+JJ5IpJHjdkZmjOUOVIPGBWjRQBiweFNGtrQ2sVtKsJMB2/aZTjySDFg7sjbtX645zTl8L6SkUUSQTKsMpmhxcygxMQwOw7vlXDsNowMHGK2KKAMuXw7pc2nWlg9u/2a0j8qBVmcFU2GPG4HJG0kcmok8K6RHcrcpBMkynO9bqUE/IiYPzcgrGmQeDtBIzzWzRQBm6hoOnapKZbyF3kMXlBlmdMKHVxjaRghlUhhyCBzUFx4V0i7mE08E7yBQu77VLkja68/NycSyDPX5q2aKAMk+GtKM0cvkS+ZGIgrC4k48tWVP4uwdvrnnNQjwnpcMMa2kTwywRxJbuZpG8vylZY+C3O0O3B655rcooAyfD+jNodibY3BlXjC5chfpvZjz9f/AK+tRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFISFUkkADkk9qWmTQx3EEkMq7o5FKMM4yCMGgDG03xFFNYwPe5jma2S6lKxMEiRwWBYnOBgHOT1B9OJG8QRPJbx2tvLLJLdLbMGGzYTH5uTn/pnzj8Dg8Vb/six8qeIwBkngFvKGYnfGN2FOT/tt+dMTRbCO5+0LE5m83zS5lcndt255PoAKAJLHU7XUUeS1cyRr0cDhhkjI9eh/Q9CCXQ38M8ojRLkMehe2kQfmVAp1rZQWUCwQIViVQioWJCqOgAJ4FJFp1jBKJIbK3jkHRkiUEfiBQBSvdY2ahbWFnteZ7kQSsyErH+7MhBwR820A/8AAhnqKde2+pSRxYnR1WTdIkKmJnXawxksf4ip7fdNN1DRY5VurqyAi1JldoZS7BVlKBQxHI/hTsenSqkdl4kWd86jAYSxK5wWH7xyP4BkbPLBHc55HUgFk2uo/wDEtZmaS4t1VJX84rHJnZvZlHXgNtz0PYCtUzRCdYTIglZS6oWG4qCASB6AsPzHrXKDQ/ELwX8c13bH7UpJCSEAyeSqZP7vOCw3cYxgdckV11AGOvibTycETphnU7ozxsl8pz9A5A/HIyKYviS1iX/SfMBzcNuWM7VSGXy2J57ZXPr1FSN4Z0pwwaGYht+f9Jl/jk8xv4u7jP6dOKWTwzpUqOrwylXWZSPtMo4lcPIPvd2AP8qAHNrMEFvd3l1IsVrFOLdWYgZO4JyTwPnJHpxUaeIYPOuhNE8MUUscEbOQDLI6B9oXqMKynnpznGDVj+ybdhPHIC0MkhlCBiMFl2sODyDkn6sfalXRrBCCkGwiQSqVdhtbaVyOePlJHHY0AMi1yylvEtQ0iyuyqu+MgFijOBz32ox/D3GX6hq9rphAuS4yjyZVCQFUZYk+wBP/ANcjNaDQlhuLq6M+65ln86JypIh/drGMKWILbVxu6/MfU1O+kR3aONRf7UzRSQZxs/dvt3LgHvtHPWgBW1myS7+zM7+buCBfLJySjvgcekbH8B6jMUPiKwn8vY0uJFidSUIysrFUP4sCPwz05qY6LYG5S48phKjhwwlYfMFCZ68naAKamhabGIglttEIjCYduAilVHXoAx496AFvYNRkt3WK4i++hASMoxUOCw3bjyVDDtye1U5bTUm0yKMF2uYpHljPnlQSS+xXwcsoBUN6471uUUAQXl3BYWc13cyCOCFS7uegA71mt4ito7qRZEZLWO3jmadiMEyOUjUDqclTj8PUVoGyjZZVkaR1klEuDI3ykYxjngZGcDj2quuhaaiBEttqhY1AV2AAjIKY56ggHNADf7esRN5TmRG/dglkOB5j7E56HLcDHX6AkWbu/hs5LeOQOXuHMcYRSctgnHtwD/8AqBqkNCjOo3N5LKZJHEYgDbiItgbbkFiHIZ2YEjjI9Aasf2aZypv5vtXlktH8uzYSrKTwecqxFAEcev6fKImSVisqQyIdh5EpAT8yf5+hxGPEmnkA5mAO/kxkfccRv+TED37ZwcS/2Dpu6FhAwMKoqYkYcJu2g8843t19aU6DprRlDbnBCj/WNn5ZPMBznOd5yT370AXvOiE4h8xPNKlwm4biucZx6ZIqhFr1hP8AceQndtI8s5X94Y+fT5gR+B9K0ulUIdF0+CeWaODDS5DguxU5Z26E46yOf+BUAQL4j0+SJZEaV1ZolUrETu837hHsePpkE4FOXXrNrhIAtx5rsqBWhKncylgDnocK30xzgYNPt9C061dXigYFZFkG6VmAZU2KcE9lwPwB6ipJNJspZmleHLtKZid7D5zH5Wev9zj/AOvQBDHr+nyiJllYrKkLodh5EpAT8yf5+hxJZaxZ38vlQs4f5yFdCpOx9jdfRuKjOgadmFlgKtCqLGfMfgJu25Gecb26+tQ2GhNp1nEkF1i8CBZrkpuMhyWJwxO3LMzYB70AT32uWVhNPDK5M0Fubl41xkRjd83Xp8rc9M4HcZbBrcTeVDcRtFe+Sss1uvzGIlSdpPf7p/TOMinvotpcRyfbF+0SSoySOSRkMoVsAHjKgDj+ppl/osdzZXsVtI1tPdQmIzZZsHaVDFdwyQD1PoPSgCNfE2lvBFKJiBM6JGrKVZi6b1AB5yV5A69sZ4rQ2XPmOXljaIg4RIyr+3zbv6D8KrLoWnKm37PkE5bLsd33euTz9xBz2XFaNAGEbTUX8O2duyzfbIYU3t9oKlpAhHzMDlhuAJ55B/CtGC6kbU7mzlC5jijmUr6NuBHvgoTn0YVcqvHahL+e7Jy8qJGMdlXcR+OXb9KALFFFFABRRRQAVzuo+IJoNFutVt1iFvbTyRN5iO2RHIY3J2/dGVb5jkADJHPHRVnHRbRo5YXEht5HkcwiRgp8w5cEA/MCSTg56ntxQBLa3wnvby0YYltmXOP4lYZU/wAx/wABq5Va3s0t7i5uAd0tw4Z29gMKB7AD8yT3qzQAUUUUAFFB6V53e3Elo3iEQ6vd7tMubRIt96xwGWPcGye5Yn6+3FAHolFcWPF8pnjxHBLOsd4hRJ9oZobpIeFZsZYHIBPXjOCTWnc+JBFaaS4eCI6hCZElud0cZYKGCc8hmycd8KeD0oA6GiuLu/GtzbsNtrAA08kJEjFTAVuo4AX/AN8OXHTgdxzRceL763luIWGnCRLtrON3kKRmRYg+GYn5d2cAckYJ56UAdpRXJT+LLlLqS2EVvC4u47cPKSUCsZBu3A8nKYKnaQQR0wTUTxxeNZee+nxxSrtDwsxyQYDL5ynvHkFenYnPGKAO4orznVvHFxLoOrQCS3tbpbO4eK5SQgbhbQyqqnP3z5xxzz5bHHYdbpesvqN7qNsERTbY8tl+dXBLANuBwfu8jggg9sEgGzRXF6L4xu7620wyQwyyzpa+asYIZ/NQlnQZPyoQQR/st0xirF74kltdYubKE2wf7bHb7ppWIUNbtJnGeOUxgYzzQB1lFcRH49Nxod3qUS2aPb20cxtZZSHIeKOQN/u/OVzjqvXPAtHxdLDc3sdw1ii2ZEcxaQqyuTGFbbknY284JwPu88kqAdbRXCXfjlm0We58mIILGWXckpwZED7k3KcofkGCeDk4ORg2brxpLa3+oxGO1kithc+WBLtZjFHE/JJxgiQj22+/AB2VFcxNr93/AMIqNUSSzVjexxCVfnj8k3Cx7z83GUO7Gflzg9KxbTxnLYJJBcTQTI9zciKd5M+UgvHiBckj5QNoByOwz3oA9Borlda1q5s7DQL+QASyTM0tvbSllmxbTPsU8bgWVccelVn8cCC1tp5JNPkS4kzE0U2Q8W6FWbrgEeacjJPAOOTtAOzorhZPHF3FBFcOtgYnjuZiA5yFhnjixnPVt5PtgdaW48aSWUsyotqyRPdvJvmJOIZokwMngkSn2GKAO5orB0bXpdV1e/tttsIbVnjISUM6usjLyAehVVb23Y6g1kweNnNvpss7WIa9kQCNHOVRpVizknBwxJ49hgdaAO0orjdD8W3WovpFtJ9ja4ntIbi4+bYW3hwwjXJJKMoBHPU5290m15rLXryFrhXX+1vJRZJyBGPsHmYxnkbgeDxk56igDs6K4O38e3E8UUflWK3Mqo8bPNtiJaGOURlieGO9gDzwhOD0qzceNz/aE1lbGyaQXEcCM7kAFp3hbI65UqDjA69xhiAdnRXG6Xrt/GIYcrdJc397apKSzlJFuG2qefu+WHPtsHrU2vazNp+vyES74bSxFyLdWZTlpNhdsHlVHJyDwD3oA6yiuY07xJd3/iL+y/LtVRBMxlDE+aqMqgoM9DvweTgqw5rF1TxVfWVzNcm5ti1m2oE2xJQOkKqyhhnliO/bdnHagD0GisHRPED6xqV5Ci24ggeSMqJMyoyOVBZf7rDDA8cEdQc1zVn4tu4AzvcWtxNPJGql5GCgPetBwu4jgEHjGcYPrQB6HRXIjxPePqttZt9nhxqP2Wdiu5WQpPgq27GS0QGDyDkEdCdO815rPWv7PeKPD+VsdnxwwlZieOwiP4kUAbdFc0fFEkd00FxBFC8aSGQPIcrsjV2YccjLgY6kAmlu9f1Gys45biygWRrOS7aMSMSioilh0GTuYL75z2xQB0lFc9L4lNu8gnijWOK6MEjhzgAQ+YT07YdfcqPXhlt4mmnvYLc20ReRLdikcm5g0gYsOnRAhOe4x60AdJRWZrWqvpcEbRwec7scqDyqgctjqeSowMn5sgHGKzIddu0Mm7y5mnvZ4484jSBIsrgknksU3YznDE9BigDpqK5o+J5RcXcSWsc3lNKkflyfeZdgUMTgLudynPdSemcS3mt3CabpN3GYoFu7tI5WmQ4VCGx3GCSFHUjnjPBoA6CiuMt/Gzmx0e4uDZB9RWGTy0c/KsjpH1PBwzHOM9MYHWodO8aySNYx4tfJkjtSxaYs2ZnmTqTzgxA89cmgDuaK4XSfGsk/9lR7LYw3EFoWzMWfdLFI2NxPODFjnk7vbnV0bxJPqnh6+1bZZlYY98IScbT+6VyHbJC4Ysuf9nd0NAHS0VxD+NZorh3lMMVu0MO0TR+WY3aeWJy5LYwuxAcHGWHzYINSHxfqKXBje2sGZAgkjScn70DSbw2MbAVxnHQ57YIB2dFZei6zFqlpGzMsd0wctbthXUK5XO3J4yMZBIJ6EitSgAooooAKxJfFFhFcG22XD3HmCMRLH8xJEhHU8Z8p+uOnvW3WJF4T0aCdJ47eYSIwZT9qlOCPMxgbu3myf99ewoAhXxTBe32kx6cBPa3s/lSSlSAubZp1xz1xs7Y+Y9xRqPiiCy1OGLj7IrzJdTsvCGOLzMDnk9O3t1FT2XhTRtOltpLS1kjNsytEouJCoZYvJDbS2CfL+XJySAKW58K6NeTzzT2jOZy5kXznCMXTy2O0NtyV4zj3680APtvEFpdyeXDDctKDiRPK5jBbYGPqpIOCMjAJ6A1XtPFunXptRFHc/wCkrG6Zi5CSZ2OR12khhnHGDnA5q0nh/T47mG5Vbgzwp5Yka6lLOuc4clvnwem7OO1Mt/DOlWhtzBBLH5EC2yAXEmDGpJVW+b58EnG7OMnHWgCnNrOi6rPZxSGUTR3qiJD8hEwRWAPPJ2SgleuM8fKcPi8WWP8Ao6y+azTEDesJVV3TeSuckkfPgfjnpnE1t4T0a0iEUVtJ5YdJNj3EjgsgQISGY8gRpj/dpkHhnQpreGWGFpISqPE63UhBAlEykHdz8+Gz+HTigC3p+t2mqXE0NsJG8pA+8rhWUsygg/VGGOvH0qDRdfi1adoI43JW1iuTNtCqwkZ1AAySD+7brTdA0KbRlm829M7TEs+0OAzE5LYZ2wST2IH6Va07Q9P0qQPZwvGwhWDmZ2yiszKDuJzgu3PXnHSgDHsfGMdzfoJbSWG0lht3jYqCytLLJEofBIwSiAY7t6c1bTxdpsvkeUlzILhwkLLFlZMxtICDnGNqMeeRjkDIqVPCmjIQVtpBgxED7RJgeXIZYxjd0DknH4dOKq2lh4bhisPszlooJd9qPtMjpGTuiAGWIC/Oyhfu84AoAmHi3TDYyXx88W0eCZDF8pQrvDA9CMduvbGaq/8ACWCG8n+2W7RWaTzQB8D5BEhd3Y7uRgcAL+dIdE8MTWCWP717eJDHHGLqYnY6sm1fmyUIDAAccHHSrw8O6JfWgPkGe3nd7jJndg5kQqx+9yrKx46e1ADrfxHaXN9DZxwXXny+YQDFwAhQM27OCP3icgnr7GqeseItICXtlfpdrFCzxyugZRvSIXG1WUg58v5hj0I9q07bRLG0ube5jWdp7eJ4Y5JbmSQhHKlgdzHOdicnJ4qj/wAIta3N3q8moj7RHfzmRUDuvlg26QMOD1Kq3zDBw5H1AGWuu6TYRmFI7mNRN5TmRSx80pvCsSSclNpHblRwTinQ+MdMuBtRLnzjEJ1gMXztEU3iQDP3cfjnjGatz+H9PcySxwAXDAEMzsQXCFFYjOCQpxk88D0FVbDwnp1tp9nFcRmW6t4ViNwsjq7YTYcHdkKRn5c459eaAEi8XWN1PbR2cFzcRz3ItlmVQE3GJpcjJBI2jPA7ilsvFWn3NrZyb2fz1hBkWIqoeVdyKQSSpbI45xuAJ5q3H4c0qFVWK2aMLcC6UJK42yBNmVweBt+XA4x2plr4Y0eyWFbe0KrCsaoDK5H7sEITk8lQeCcngegwAVY/GelSaW+o/wCkLbpCk7Fo8ERNGZFf6FQffIxjJFQjxNcx6tNbyW6yQxTXKEQoTIyxxxuMDPJ+cj3wKnbwToD2y2xs5fKWFYAoupf9WsbRBfvdNjsPfPPNWh4fs4Jmu7NCl8GkkjlllkkUSOoUll3jcMKvHtxjrQBRhu9J0vRdQ12zhuJ4beBpyfOL70KCU7CzEYOR/kCtCHXrKe+jtBvWSSR4kZl+VpEGXTI7jn2+U4zinQaHZRaA2jPErWkkTxSooKBg+d+AD8oO49DxmnW+iafa3X2mKAiXe0gLSMwDsArNgnG4gcnqefU5AMa48Yx22okSWkqWUKXZnkKgsvkNGC4wfu4die/A+lbNnrVpfX81pB5jNFuBfb8hKtsYZ7EMMYOM9siq1z4V0e7a4aa2kb7Qk0coFxIoKy7fMGA2Bu2r09Pc1cstJs9PmnltUkQzu0jqZnZdzHLEKSQuTycAZJJoAy4/FtiFgWQTO0pADJDtUbpvJXOTx8+B+OemcPPi7TBZR3QW4ZZI96oseXPBLLtzncoHK9eQOScVIPCmjBUUW0mE27f9Jl4xL5w/i/56DP6dOKmk8PaXIExbtGyXD3KvDK8bCR87zuUg85OR0oAotrulSXy3LrdB45zZxFyVjaXDEhVJwWwDzjJzgZzitHTdatNVMv2XzGSNVcSFPldWzgqe/wB08dRxkcjLG8PaW1tNbtbsYJ5WlkjMrlWZuuRnpx06DtU1ho9lpsEkFpHIkT5+QzOwUHPCgk7Rz0GAKAKA8XaU1kt3G8kkJiM+Y1DYjBA3cH1YDHXIPHBxLD4lsZ3dEWbcofI2f3JjC3fswPPTHOcVT1XwdYXtlOtopt7uVWXz/NkO7dt3FwGG/wC4v3s8j3NWm8KaO8kkht5Q0j+YdtzKMN5omyuG+X94N3GOfqaAIk8X6XLtMYnZWjikDbAAfMzsGSRycH6YOaty69axWVhc+VcsL5gkEYiIcsUZ8FTjacKetQR+EdFigaFLaVVby+RdS7l8skptbdlSCT0x1x0rQl0y0n+x+YjsbOQSwnzWyH2lck5+bhm65zmgDOi8V6XM0CRtIXuUjktl283COTtZOcY+UnnBA5OMipdK8Q2ms3FwltHJ5cEaO0rgAZJYFcZyCChByOooTwxo8aIiWYURlPKxI2YtrFlCHPygEngYGOOnFWLLRrDTpnltIPKZ41jfDsQwBYjIJwTlmyepzyaAMi58WwSJbR6dGz3F0YGiMiceVLu2S43DIOxuMgjvjNTa54jfSpxbR2zSzKLd3cgbCsk6xEAZzu5J9OlWYvDGjwi3EdqyC3Mfk4mcbBHu2KOfujc2F6c9Kmv9D0/UrhZ7qF2kUIMrM6ZCuJFztIzhlB//AFmgCtbeKNOurmG3iExlkco6iPJiYM6YfHT5o3GRkcdeRmtf6ppWmS6rGsE01wTHJdRI/LF9kakbmGBjbyOPlbuCKvWnhvS7G8N3bQyxzMWLEXEhDlnaQllLYb5ncjIONxxin3mg6bqE0kt1bmSR0KbjIwKg7SduD8vKKeMcjNAGbBr2jWLSR20FycyrECiFg7lzHgHOAQwI5x1z05q9/wAJBZHTLW+RJpEuZfJjRU+fzMkbSCeCCpB9DVW78K2zW6xWTSQA3qXbhppGGRJ5jBQW+TceTjHJzzWi2jWDW1tbeQRFbSiaILIy4cZO4kHJ5JJznJPNAGVpniDRVgt4bFZzFcybo8gnc0sTXOSWOeVDHnvxUc3jaxfTpJraOYSGHzITNFhWJhaZM85wyq35YOMjN2Dwlots1sYbWRPsyqsWLmXC7Y2jB+9ydjsuTzg+woHhHRBGkf2RyiKiqpuJCMJG0aj739xmHvnnmgCvH4y077OfPS4W6jUNJbiIl9u1WLKv8Qwy9MnnGM8VradqkOp+eYI5gkMrwl3XAZlZlYD1wVP6VW/4RnSt0TiCVZYm3LKtxIr/AHQuCwbJXAA2k44HFWbPSrSwYm2WVAXlkK+c7KWkYM5wTjqOPTJxjJyAcz4k8UT6Ve3ljdxQG22wyDAYM9s5ZZ2yG4aMKWPtjuau6LqFha3IskEjyzyEI675MJ8xj3szE8qpIPT5h0JGdq70qxv5llurWOWRYZLcMwziOTG9fx2j8qjh0TT7e9W7ghaKVYkiwkrhCq5C5QHaSM8EjP5CgCtN4l063uLiCXzQ1u0iSYTIBSJZT06/I4I/KqcWuaJaX812ryb7xElmkB3oiBWCucEgKQh5HHQnqK0ZPD2mS6hLfNDKLiQszMlxIvJjEZIAbAOwAZHPA7imJ4X0eKVnS0K742jkRZXCSKWZjvTO1vmdzyDjcaAG6Rqh1PUtRVrfy1tjGsRZcOVeNX55Pr7dBnpWzVDTNGstIEv2NJV80KHMk7yE7RtXliegGKv0AFFFFABXP2uj6gl6TPc7rYOzptmfP+u3jI7/ACqi9f73Xcc9BRQBy1toOreWkd1dqxzamSRJ3y5jO6RsYAy5JB9sf3RVp9Cn/tFZUmxbrLEBGZnJ8pI29f4i5APqq8ntW/RQBy6aLrKW9kDdRmWGGBJSJnAYpIGc9OdwVR2xlvWjRo723VNRuI7rynM4W3AZpCHl3IXVsbdqKBj1dvXnqKKAOe1DT9R1SU3VrN9nUwSRpDOOMlGAJGMg7iO+Co5BOMRzaZPpkMs8cTSWlrZKkFnbs2VKB8hQFyS2VGQRnH59LRQBycPh/WP7Oiha/wBjBIhxIzAsqbSWyMncWYnGCNqYIIzW/f2Ut35flXHlbc55kGf++HX9c1dooApCzmXTTbeeDIf42DMCM8g7mJ5HHWuatPC2pJbPHcy2u+dYEkMLNtjEdw8uVyOSRIR7ECuyooA5Cz8K3lpc6NcGWNjpMEMCKGP74JHLGWPHBxKCB6gj3rd0OyuNM0q3sZvLYQRKokRydxx83BAwM9OvHpWlRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAH//2Q==",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=420x130>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=400x180>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=400x330>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_path = r\"Attention is all you need.pdf\"\n",
    "\n",
    "pdf_list = [['Optimizer', load_path, 6, (100, 500, 520, 630)],\n",
    "            ['Model Size', load_path, 7, (100, 70, 500, 250)],\n",
    "            ['Configuration', load_path, 8, (100, 60, 500, 390)],\n",
    "            ]\n",
    "for pdf_arg in pdf_list:\n",
    "    print_pdf_page(pdf_arg[0], pdf_arg[1], pdf_arg[2], pdf_arg[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer of Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It leads to performance degradation of T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` \n",
    "training_args.lr_scheduler_type = 'inverse_sqrt'\n",
    "training_args.adam_beta1 = 0.9\n",
    "training_args.adam_beta2 = 0.98\n",
    "training_args.adam_epsilon = 1e-9\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric of Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default Evaluation Metric for T5 is sacrebleu and for Transformer is bleu\n",
    "<br>Default Evaluation Metric for Transformer is bleu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` \n",
    "# metric = evaluate.load(\"sacrebleu\", cache_dir=model_args.cache_dir)\n",
    "metric = evaluate.load(\"bleu\", cache_dir=model_args.cache_dir)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer-Big of Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5-Small is same as Transformer-Base\n",
    "<br>There is no T5-Big\n",
    "<br>This is configuration for T5-Big same as Transformer-Big"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` \n",
    "model.config.d_ff = 4096\n",
    "model.config.d_model = 1024\n",
    "model.config.num_heads = 16\n",
    "model.config.dropout_rate = 0.3\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp run_translation_base_huggingface_example.py transformers-main/examples/pytorch/translation/run_translation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp run_translation_big_huggingface_example.py transformers-main/examples/pytorch/translation/run_translation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp run_translation_no_trainer_base_huggingface_example.py transformers-main/examples/pytorch/translation/run_translation_no_trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp run_translation_no_trainer_big_huggingface_example.py transformers-main/examples/pytorch/translation/run_translation_no_trainer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Basline Code with Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a219d50c0e4d40b38e4f7370f9cffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-11 13:29:20.177673: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-11 13:29:20.742045: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using custom data configuration default-2098d2201d7f61e4\n",
      "05/11/2024 13:29:22 - INFO - datasets.builder - Using custom data configuration default-2098d2201d7f61e4\n",
      "Loading Dataset Infos from /opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "05/11/2024 13:29:22 - INFO - datasets.info - Loading Dataset Infos from /opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "05/11/2024 13:29:22 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2098d2201d7f61e4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7\n",
      "05/11/2024 13:29:22 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2098d2201d7f61e4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-2098d2201d7f61e4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)\n",
      "05/11/2024 13:29:22 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-2098d2201d7f61e4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2098d2201d7f61e4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7\n",
      "05/11/2024 13:29:22 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2098d2201d7f61e4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:726] 2024-05-11 13:29:23,231 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-05-11 13:29:23,234 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"google-t5/t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.41.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2102] 2024-05-11 13:29:23,451 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2102] 2024-05-11 13:29:23,451 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2102] 2024-05-11 13:29:23,451 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2102] 2024-05-11 13:29:23,451 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2102] 2024-05-11 13:29:23,451 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:926] 2024-05-11 13:29:23,520 >> Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2098d2201d7f61e4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-f854f94dfc37d22e.arrow\n",
      "05/11/2024 13:29:24 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2098d2201d7f61e4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-f854f94dfc37d22e.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2098d2201d7f61e4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e22f4b02c5657f5.arrow\n",
      "05/11/2024 13:29:24 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2098d2201d7f61e4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-5e22f4b02c5657f5.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2098d2201d7f61e4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-e3b54d2114074cb4.arrow\n",
      "05/11/2024 13:29:24 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-2098d2201d7f61e4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-e3b54d2114074cb4.arrow\n",
      "[INFO|trainer.py:2078] 2024-05-11 13:29:26,918 >> ***** Running training *****\n",
      "[INFO|trainer.py:2079] 2024-05-11 13:29:26,918 >>   Num examples = 116,654\n",
      "[INFO|trainer.py:2080] 2024-05-11 13:29:26,918 >>   Num Epochs = 100\n",
      "[INFO|trainer.py:2081] 2024-05-11 13:29:26,918 >>   Instantaneous batch size per device = 64\n",
      "[INFO|trainer.py:2084] 2024-05-11 13:29:26,918 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:2085] 2024-05-11 13:29:26,918 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2086] 2024-05-11 13:29:26,918 >>   Total optimization steps = 182,300\n",
      "[INFO|trainer.py:2087] 2024-05-11 13:29:26,919 >>   Number of trainable parameters = 60,506,624\n",
      "{'loss': 7.2532, 'grad_norm': 2.1429903507232666, 'learning_rate': 4.986286341195831e-05, 'epoch': 0.27}\n",
      "{'loss': 6.4724, 'grad_norm': 2.0887398719787598, 'learning_rate': 4.972572682391663e-05, 'epoch': 0.55}\n",
      "{'loss': 6.0973, 'grad_norm': 2.1614580154418945, 'learning_rate': 4.958859023587493e-05, 'epoch': 0.82}\n",
      "  1%|▎                                 | 1823/182300 [08:15<14:40:59,  3.41it/s][INFO|trainer.py:3388] 2024-05-11 13:37:42,857 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-1823\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 13:37:42,858 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-1823/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 13:37:42,858 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-1823/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 13:37:43,285 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-1823/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 13:37:43,286 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-1823/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 13:37:43,287 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-1823/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 13:37:43,288 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-1823/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 13:37:44,122 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 13:37:44,123 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 13:37:44,124 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "{'loss': 5.7872, 'grad_norm': 2.202652931213379, 'learning_rate': 4.9451453647833245e-05, 'epoch': 1.1}\n",
      "{'loss': 5.5399, 'grad_norm': 2.147414445877075, 'learning_rate': 4.931431705979155e-05, 'epoch': 1.37}\n",
      "{'loss': 5.3515, 'grad_norm': 2.1624412536621094, 'learning_rate': 4.917718047174987e-05, 'epoch': 1.65}\n",
      "{'loss': 5.1762, 'grad_norm': 2.155363082885742, 'learning_rate': 4.9040043883708175e-05, 'epoch': 1.92}\n",
      "  2%|▋                                 | 3646/182300 [19:18<18:33:00,  2.68it/s][INFO|trainer.py:3388] 2024-05-11 13:48:45,310 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-3646\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 13:48:45,311 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-3646/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 13:48:45,311 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-3646/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 13:48:45,734 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-3646/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 13:48:45,735 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-3646/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 13:48:45,736 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-3646/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 13:48:45,737 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-3646/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 13:48:47,090 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 13:48:47,091 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 13:48:47,113 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 13:48:47,122 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-1823] due to args.save_total_limit\n",
      "{'loss': 5.0364, 'grad_norm': 2.2420756816864014, 'learning_rate': 4.890290729566648e-05, 'epoch': 2.19}\n",
      "{'loss': 4.9178, 'grad_norm': 2.4155259132385254, 'learning_rate': 4.87657707076248e-05, 'epoch': 2.47}\n",
      "{'loss': 4.8449, 'grad_norm': 2.2149574756622314, 'learning_rate': 4.8628634119583105e-05, 'epoch': 2.74}\n",
      "  3%|█                                 | 5469/182300 [31:11<17:41:31,  2.78it/s][INFO|trainer.py:3388] 2024-05-11 14:00:38,393 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-5469\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 14:00:38,394 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-5469/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 14:00:38,394 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-5469/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 14:00:38,825 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-5469/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 14:00:38,826 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-5469/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 14:00:38,826 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-5469/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 14:00:38,827 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-5469/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 14:00:40,182 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 14:00:40,182 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 14:00:40,202 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 14:00:40,210 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-3646] due to args.save_total_limit\n",
      "{'loss': 4.762, 'grad_norm': 2.25925350189209, 'learning_rate': 4.849149753154142e-05, 'epoch': 3.02}\n",
      "{'loss': 4.6713, 'grad_norm': 2.315990686416626, 'learning_rate': 4.835436094349973e-05, 'epoch': 3.29}\n",
      "{'loss': 4.6267, 'grad_norm': 2.425288677215576, 'learning_rate': 4.821722435545804e-05, 'epoch': 3.57}\n",
      "{'loss': 4.579, 'grad_norm': 2.3451356887817383, 'learning_rate': 4.808008776741635e-05, 'epoch': 3.84}\n",
      "  4%|█▎                                | 7292/182300 [42:01<14:50:40,  3.27it/s][INFO|trainer.py:3388] 2024-05-11 14:11:28,699 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-7292\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 14:11:28,700 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-7292/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 14:11:28,701 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-7292/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 14:11:29,145 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-7292/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 14:11:29,147 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-7292/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 14:11:29,147 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-7292/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 14:11:29,148 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-7292/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 14:11:30,537 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 14:11:30,537 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 14:11:30,539 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 14:11:30,548 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-5469] due to args.save_total_limit\n",
      "{'loss': 4.5148, 'grad_norm': 2.306058645248413, 'learning_rate': 4.794295117937466e-05, 'epoch': 4.11}\n",
      "{'loss': 4.4659, 'grad_norm': 2.3386404514312744, 'learning_rate': 4.780581459133297e-05, 'epoch': 4.39}\n",
      "{'loss': 4.4327, 'grad_norm': 2.3117551803588867, 'learning_rate': 4.766867800329128e-05, 'epoch': 4.66}\n",
      "{'loss': 4.3947, 'grad_norm': 2.36466908454895, 'learning_rate': 4.753154141524959e-05, 'epoch': 4.94}\n",
      "  5%|█▋                                | 9115/182300 [53:55<15:56:17,  3.02it/s][INFO|trainer.py:3388] 2024-05-11 14:23:22,762 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-9115\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 14:23:22,762 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-9115/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 14:23:22,763 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-9115/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 14:23:23,216 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-9115/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 14:23:23,217 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-9115/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 14:23:23,217 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-9115/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 14:23:23,218 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-9115/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 14:23:24,662 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 14:23:24,663 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 14:23:24,665 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 14:23:24,674 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-7292] due to args.save_total_limit\n",
      "{'loss': 4.3441, 'grad_norm': 2.348733901977539, 'learning_rate': 4.73944048272079e-05, 'epoch': 5.21}\n",
      "{'loss': 4.3025, 'grad_norm': 2.9133706092834473, 'learning_rate': 4.7257268239166215e-05, 'epoch': 5.49}\n",
      "{'loss': 4.2785, 'grad_norm': 2.6369545459747314, 'learning_rate': 4.712013165112452e-05, 'epoch': 5.76}\n",
      "  6%|█▊                             | 10938/182300 [1:05:18<16:16:12,  2.93it/s][INFO|trainer.py:3388] 2024-05-11 14:34:45,356 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-10938\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 14:34:45,357 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-10938/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 14:34:45,357 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-10938/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 14:34:45,789 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-10938/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 14:34:45,791 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-10938/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 14:34:45,791 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-10938/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 14:34:45,792 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-10938/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 14:34:47,248 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 14:34:47,248 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 14:34:47,250 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 14:34:47,259 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-9115] due to args.save_total_limit\n",
      "{'loss': 4.2431, 'grad_norm': 2.7040719985961914, 'learning_rate': 4.698299506308283e-05, 'epoch': 6.03}\n",
      "{'loss': 4.1796, 'grad_norm': 2.7137389183044434, 'learning_rate': 4.6845858475041146e-05, 'epoch': 6.31}\n",
      "{'loss': 4.1432, 'grad_norm': 2.963534355163574, 'learning_rate': 4.670872188699945e-05, 'epoch': 6.58}\n",
      "{'loss': 4.1252, 'grad_norm': 2.8243420124053955, 'learning_rate': 4.6571585298957763e-05, 'epoch': 6.86}\n",
      "  7%|██▏                            | 12761/182300 [1:17:18<18:12:35,  2.59it/s][INFO|trainer.py:3388] 2024-05-11 14:46:45,397 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-12761\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 14:46:45,398 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-12761/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 14:46:45,398 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-12761/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 14:46:45,843 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-12761/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 14:46:45,845 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-12761/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 14:46:45,845 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-12761/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 14:46:45,846 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-12761/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 14:46:47,264 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 14:46:47,264 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 14:46:47,266 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 14:46:47,275 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-10938] due to args.save_total_limit\n",
      "{'loss': 4.0713, 'grad_norm': 3.03757905960083, 'learning_rate': 4.6434448710916076e-05, 'epoch': 7.13}\n",
      "{'loss': 4.0248, 'grad_norm': 3.007908821105957, 'learning_rate': 4.629731212287439e-05, 'epoch': 7.41}\n",
      "{'loss': 4.0099, 'grad_norm': 2.846788167953491, 'learning_rate': 4.6160175534832694e-05, 'epoch': 7.68}\n",
      "{'loss': 3.9728, 'grad_norm': 2.961183786392212, 'learning_rate': 4.6023038946791006e-05, 'epoch': 7.95}\n",
      "  8%|██▍                            | 14584/182300 [1:29:03<17:23:20,  2.68it/s][INFO|trainer.py:3388] 2024-05-11 14:58:30,743 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-14584\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 14:58:30,744 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-14584/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 14:58:30,744 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-14584/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 14:58:31,175 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-14584/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 14:58:31,176 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-14584/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 14:58:31,177 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-14584/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 14:58:31,178 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-14584/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 14:58:32,581 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 14:58:32,581 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 14:58:32,583 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 14:58:32,594 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-12761] due to args.save_total_limit\n",
      "{'loss': 3.9118, 'grad_norm': 3.066025733947754, 'learning_rate': 4.588590235874932e-05, 'epoch': 8.23}\n",
      "{'loss': 3.8828, 'grad_norm': 2.9394822120666504, 'learning_rate': 4.5748765770707624e-05, 'epoch': 8.5}\n",
      "{'loss': 3.8832, 'grad_norm': 3.012153387069702, 'learning_rate': 4.5611629182665936e-05, 'epoch': 8.78}\n",
      "  9%|██▊                            | 16407/182300 [1:40:14<14:17:17,  3.23it/s][INFO|trainer.py:3388] 2024-05-11 15:09:41,698 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-16407\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 15:09:41,699 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-16407/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 15:09:41,699 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-16407/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 15:09:42,137 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-16407/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 15:09:42,138 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-16407/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 15:09:42,138 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-16407/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 15:09:42,139 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-16407/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 15:09:43,552 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 15:09:43,552 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 15:09:43,554 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 15:09:43,563 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-14584] due to args.save_total_limit\n",
      "{'loss': 3.8481, 'grad_norm': 2.899332046508789, 'learning_rate': 4.547449259462425e-05, 'epoch': 9.05}\n",
      "{'loss': 3.7773, 'grad_norm': 3.164444923400879, 'learning_rate': 4.533735600658256e-05, 'epoch': 9.33}\n",
      "{'loss': 3.7689, 'grad_norm': 3.017282009124756, 'learning_rate': 4.5200219418540867e-05, 'epoch': 9.6}\n",
      "{'loss': 3.752, 'grad_norm': 3.367647647857666, 'learning_rate': 4.506308283049918e-05, 'epoch': 9.87}\n",
      " 10%|███                            | 18230/182300 [1:51:00<16:17:51,  2.80it/s][INFO|trainer.py:3388] 2024-05-11 15:20:27,536 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-18230\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 15:20:27,537 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-18230/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 15:20:27,537 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-18230/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 15:20:27,960 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-18230/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 15:20:27,961 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-18230/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 15:20:27,961 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-18230/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 15:20:27,962 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-18230/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 15:20:29,335 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 15:20:29,335 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 15:20:29,338 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 15:20:29,350 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-16407] due to args.save_total_limit\n",
      "{'loss': 3.7182, 'grad_norm': 2.9855947494506836, 'learning_rate': 4.492594624245749e-05, 'epoch': 10.15}\n",
      "{'loss': 3.674, 'grad_norm': 3.505870819091797, 'learning_rate': 4.47888096544158e-05, 'epoch': 10.42}\n",
      "{'loss': 3.651, 'grad_norm': 3.438145160675049, 'learning_rate': 4.465167306637411e-05, 'epoch': 10.7}\n",
      "{'loss': 3.6479, 'grad_norm': 3.0687413215637207, 'learning_rate': 4.451453647833242e-05, 'epoch': 10.97}\n",
      " 11%|███▍                           | 20053/182300 [2:01:56<15:16:13,  2.95it/s][INFO|trainer.py:3388] 2024-05-11 15:31:23,085 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-20053\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 15:31:23,086 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-20053/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 15:31:23,086 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-20053/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 15:31:23,506 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-20053/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 15:31:23,508 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-20053/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 15:31:23,508 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-20053/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 15:31:23,509 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-20053/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 15:31:24,865 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 15:31:24,865 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 15:31:24,867 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 15:31:24,876 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-18230] due to args.save_total_limit\n",
      "{'loss': 3.5693, 'grad_norm': 3.2287588119506836, 'learning_rate': 4.4377399890290734e-05, 'epoch': 11.25}\n",
      "{'loss': 3.5667, 'grad_norm': 3.3848471641540527, 'learning_rate': 4.424026330224904e-05, 'epoch': 11.52}\n",
      "{'loss': 3.5589, 'grad_norm': 3.5464422702789307, 'learning_rate': 4.410312671420735e-05, 'epoch': 11.79}\n",
      " 12%|███▋                           | 21876/182300 [2:12:29<13:19:32,  3.34it/s][INFO|trainer.py:3388] 2024-05-11 15:41:56,923 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-21876\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 15:41:56,923 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-21876/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 15:41:56,924 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-21876/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 15:41:57,375 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-21876/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 15:41:57,377 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-21876/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 15:41:57,377 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-21876/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 15:41:57,378 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-21876/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 15:41:58,752 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 15:41:58,752 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 15:41:58,754 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 15:41:58,763 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-20053] due to args.save_total_limit\n",
      "{'loss': 3.5313, 'grad_norm': 3.6160085201263428, 'learning_rate': 4.3965990126165664e-05, 'epoch': 12.07}\n",
      "{'loss': 3.4855, 'grad_norm': 3.6420817375183105, 'learning_rate': 4.3828853538123976e-05, 'epoch': 12.34}\n",
      "{'loss': 3.4654, 'grad_norm': 3.583449363708496, 'learning_rate': 4.369171695008228e-05, 'epoch': 12.62}\n",
      "{'loss': 3.4512, 'grad_norm': 3.5506091117858887, 'learning_rate': 4.3554580362040594e-05, 'epoch': 12.89}\n",
      " 13%|████                           | 23699/182300 [2:23:03<13:55:01,  3.17it/s][INFO|trainer.py:3388] 2024-05-11 15:52:30,582 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-23699\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 15:52:30,582 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-23699/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 15:52:30,583 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-23699/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 15:52:31,018 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-23699/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 15:52:31,019 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-23699/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 15:52:31,020 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-23699/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 15:52:31,020 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-23699/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 15:52:32,405 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 15:52:32,405 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 15:52:32,407 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 15:52:32,416 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-21876] due to args.save_total_limit\n",
      "{'loss': 3.4055, 'grad_norm': 4.101990699768066, 'learning_rate': 4.341744377399891e-05, 'epoch': 13.17}\n",
      "{'loss': 3.3755, 'grad_norm': 4.144250392913818, 'learning_rate': 4.328030718595721e-05, 'epoch': 13.44}\n",
      "{'loss': 3.3725, 'grad_norm': 3.6288070678710938, 'learning_rate': 4.3143170597915525e-05, 'epoch': 13.71}\n",
      "{'loss': 3.3693, 'grad_norm': 3.5927882194519043, 'learning_rate': 4.300603400987384e-05, 'epoch': 13.99}\n",
      " 14%|████▎                          | 25522/182300 [2:33:29<13:25:27,  3.24it/s][INFO|trainer.py:3388] 2024-05-11 16:02:56,372 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-25522\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 16:02:56,373 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-25522/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 16:02:56,374 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-25522/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 16:02:56,793 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-25522/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 16:02:56,795 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-25522/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 16:02:56,795 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-25522/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 16:02:56,796 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-25522/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 16:02:58,160 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 16:02:58,160 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 16:02:58,162 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 16:02:58,179 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-23699] due to args.save_total_limit\n",
      "{'loss': 3.2955, 'grad_norm': 3.405404567718506, 'learning_rate': 4.286889742183215e-05, 'epoch': 14.26}\n",
      "{'loss': 3.3038, 'grad_norm': 4.086198329925537, 'learning_rate': 4.2731760833790455e-05, 'epoch': 14.54}\n",
      "{'loss': 3.2561, 'grad_norm': 3.3961052894592285, 'learning_rate': 4.259462424574877e-05, 'epoch': 14.81}\n",
      " 15%|████▋                          | 27345/182300 [2:43:57<15:41:29,  2.74it/s][INFO|trainer.py:3388] 2024-05-11 16:13:24,045 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-27345\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 16:13:24,046 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-27345/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 16:13:24,046 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-27345/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 16:13:24,483 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-27345/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 16:13:24,484 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-27345/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 16:13:24,484 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-27345/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 16:13:24,485 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-27345/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 16:13:25,920 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 16:13:25,920 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 16:13:25,922 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 16:13:25,931 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-25522] due to args.save_total_limit\n",
      "{'loss': 3.2539, 'grad_norm': 3.6080105304718018, 'learning_rate': 4.245748765770708e-05, 'epoch': 15.09}\n",
      "{'loss': 3.2148, 'grad_norm': 3.597956657409668, 'learning_rate': 4.2320351069665385e-05, 'epoch': 15.36}\n",
      "{'loss': 3.2031, 'grad_norm': 3.466057062149048, 'learning_rate': 4.21832144816237e-05, 'epoch': 15.63}\n",
      "{'loss': 3.1768, 'grad_norm': 4.239918231964111, 'learning_rate': 4.204607789358201e-05, 'epoch': 15.91}\n",
      " 16%|████▉                          | 29168/182300 [2:54:17<13:09:07,  3.23it/s][INFO|trainer.py:3388] 2024-05-11 16:23:44,841 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-29168\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 16:23:44,842 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-29168/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 16:23:44,843 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-29168/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 16:23:45,275 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-29168/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 16:23:45,276 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-29168/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 16:23:45,276 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-29168/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 16:23:45,277 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-29168/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 16:23:46,666 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 16:23:46,666 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 16:23:46,668 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 16:23:46,674 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-27345] due to args.save_total_limit\n",
      "{'loss': 3.1258, 'grad_norm': 4.533541202545166, 'learning_rate': 4.190894130554032e-05, 'epoch': 16.18}\n",
      "{'loss': 3.1162, 'grad_norm': 3.8643674850463867, 'learning_rate': 4.177180471749863e-05, 'epoch': 16.46}\n",
      "{'loss': 3.1159, 'grad_norm': 3.710988998413086, 'learning_rate': 4.163466812945694e-05, 'epoch': 16.73}\n",
      " 17%|█████▎                         | 30991/182300 [3:04:21<14:02:26,  2.99it/s][INFO|trainer.py:3388] 2024-05-11 16:33:48,361 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-30991\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 16:33:48,362 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-30991/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 16:33:48,363 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-30991/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 16:33:48,800 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-30991/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 16:33:48,801 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-30991/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 16:33:48,802 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-30991/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 16:33:48,803 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-30991/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 16:33:50,187 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 16:33:50,187 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 16:33:50,189 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 16:33:50,199 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-29168] due to args.save_total_limit\n",
      "{'loss': 3.1137, 'grad_norm': 3.6405742168426514, 'learning_rate': 4.149753154141525e-05, 'epoch': 17.0}\n",
      "{'loss': 3.0327, 'grad_norm': 4.127532482147217, 'learning_rate': 4.136039495337356e-05, 'epoch': 17.28}\n",
      "{'loss': 3.0611, 'grad_norm': 3.6551403999328613, 'learning_rate': 4.122325836533188e-05, 'epoch': 17.55}\n",
      "{'loss': 3.0233, 'grad_norm': 3.6437666416168213, 'learning_rate': 4.108612177729018e-05, 'epoch': 17.83}\n",
      " 18%|█████▌                         | 32814/182300 [3:14:23<15:48:49,  2.63it/s][INFO|trainer.py:3388] 2024-05-11 16:43:50,291 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-32814\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 16:43:50,292 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-32814/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 16:43:50,292 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-32814/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 16:43:50,721 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-32814/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 16:43:50,722 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-32814/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 16:43:50,722 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-32814/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 16:43:50,723 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-32814/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 16:43:52,091 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 16:43:52,091 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 16:43:52,093 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 16:43:52,116 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-30991] due to args.save_total_limit\n",
      "{'loss': 3.0099, 'grad_norm': 4.008886814117432, 'learning_rate': 4.0948985189248495e-05, 'epoch': 18.1}\n",
      "{'loss': 2.9663, 'grad_norm': 3.779545783996582, 'learning_rate': 4.08118486012068e-05, 'epoch': 18.38}\n",
      "{'loss': 2.9628, 'grad_norm': 3.7845826148986816, 'learning_rate': 4.067471201316512e-05, 'epoch': 18.65}\n",
      "{'loss': 2.9649, 'grad_norm': 3.866852283477783, 'learning_rate': 4.0537575425123425e-05, 'epoch': 18.92}\n",
      " 19%|█████▉                         | 34637/182300 [3:23:54<10:39:29,  3.85it/s][INFO|trainer.py:3388] 2024-05-11 16:53:21,063 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-34637\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 16:53:21,064 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-34637/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 16:53:21,064 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-34637/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 16:53:21,497 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-34637/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 16:53:21,498 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-34637/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 16:53:21,499 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-34637/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 16:53:21,500 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-34637/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 16:53:22,876 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 16:53:22,876 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 16:53:22,878 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 16:53:22,887 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-32814] due to args.save_total_limit\n",
      "{'loss': 2.9199, 'grad_norm': 3.9092442989349365, 'learning_rate': 4.040043883708173e-05, 'epoch': 19.2}\n",
      "{'loss': 2.9078, 'grad_norm': 4.038732528686523, 'learning_rate': 4.026330224904005e-05, 'epoch': 19.47}\n",
      "{'loss': 2.8907, 'grad_norm': 3.717470645904541, 'learning_rate': 4.0126165660998355e-05, 'epoch': 19.75}\n",
      " 20%|██████▏                        | 36460/182300 [3:33:27<11:39:51,  3.47it/s][INFO|trainer.py:3388] 2024-05-11 17:02:54,826 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-36460\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 17:02:54,827 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-36460/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 17:02:54,827 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-36460/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 17:02:55,250 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-36460/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 17:02:55,252 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-36460/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 17:02:55,252 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-36460/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 17:02:55,253 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-36460/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 17:02:56,609 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 17:02:56,609 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 17:02:56,611 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 17:02:56,631 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-34637] due to args.save_total_limit\n",
      "{'loss': 2.8892, 'grad_norm': 3.9166011810302734, 'learning_rate': 3.998902907295667e-05, 'epoch': 20.02}\n",
      "{'loss': 2.8274, 'grad_norm': 3.9473681449890137, 'learning_rate': 3.985189248491497e-05, 'epoch': 20.3}\n",
      "{'loss': 2.8371, 'grad_norm': 4.139565467834473, 'learning_rate': 3.971475589687329e-05, 'epoch': 20.57}\n",
      "{'loss': 2.8401, 'grad_norm': 3.7124762535095215, 'learning_rate': 3.95776193088316e-05, 'epoch': 20.84}\n",
      " 21%|██████▌                        | 38283/182300 [3:43:27<13:29:59,  2.96it/s][INFO|trainer.py:3388] 2024-05-11 17:12:54,135 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-38283\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 17:12:54,136 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-38283/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 17:12:54,137 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-38283/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 17:12:54,571 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-38283/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 17:12:54,573 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-38283/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 17:12:54,573 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-38283/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 17:12:54,574 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-38283/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 17:12:56,006 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 17:12:56,006 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 17:12:56,008 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 17:12:56,018 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-36460] due to args.save_total_limit\n",
      "{'loss': 2.8083, 'grad_norm': 3.772149085998535, 'learning_rate': 3.9440482720789904e-05, 'epoch': 21.12}\n",
      "{'loss': 2.7767, 'grad_norm': 4.185425758361816, 'learning_rate': 3.930334613274822e-05, 'epoch': 21.39}\n",
      "{'loss': 2.7723, 'grad_norm': 3.800649881362915, 'learning_rate': 3.916620954470653e-05, 'epoch': 21.67}\n",
      "{'loss': 2.774, 'grad_norm': 3.800741195678711, 'learning_rate': 3.902907295666484e-05, 'epoch': 21.94}\n",
      " 22%|██████▊                        | 40106/182300 [3:53:13<11:23:05,  3.47it/s][INFO|trainer.py:3388] 2024-05-11 17:22:40,155 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-40106\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 17:22:40,155 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-40106/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 17:22:40,156 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-40106/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 17:22:40,567 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-40106/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 17:22:40,568 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-40106/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 17:22:40,568 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-40106/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 17:22:40,569 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-40106/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 17:22:41,895 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 17:22:41,915 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 17:22:41,917 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 17:22:41,926 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-38283] due to args.save_total_limit\n",
      "{'loss': 2.7329, 'grad_norm': 3.7834713459014893, 'learning_rate': 3.889193636862315e-05, 'epoch': 22.22}\n",
      "{'loss': 2.7181, 'grad_norm': 4.18643045425415, 'learning_rate': 3.8754799780581465e-05, 'epoch': 22.49}\n",
      "{'loss': 2.7148, 'grad_norm': 3.758415460586548, 'learning_rate': 3.861766319253977e-05, 'epoch': 22.76}\n",
      " 23%|███████▏                       | 41929/182300 [4:02:43<11:07:47,  3.50it/s][INFO|trainer.py:3388] 2024-05-11 17:32:10,043 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-41929\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 17:32:10,044 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-41929/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 17:32:10,045 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-41929/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 17:32:10,467 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-41929/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 17:32:10,468 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-41929/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 17:32:10,468 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-41929/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 17:32:10,469 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-41929/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 17:32:11,829 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 17:32:11,829 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 17:32:11,830 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 17:32:11,836 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-40106] due to args.save_total_limit\n",
      "{'loss': 2.7025, 'grad_norm': 4.028139114379883, 'learning_rate': 3.8480526604498076e-05, 'epoch': 23.04}\n",
      "{'loss': 2.6542, 'grad_norm': 3.779428243637085, 'learning_rate': 3.8343390016456395e-05, 'epoch': 23.31}\n",
      "{'loss': 2.6662, 'grad_norm': 3.6896047592163086, 'learning_rate': 3.82062534284147e-05, 'epoch': 23.59}\n",
      "{'loss': 2.6697, 'grad_norm': 3.527114152908325, 'learning_rate': 3.806911684037301e-05, 'epoch': 23.86}\n",
      " 24%|███████▍                       | 43752/182300 [4:12:45<11:25:38,  3.37it/s][INFO|trainer.py:3388] 2024-05-11 17:42:12,746 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-43752\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 17:42:12,747 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-43752/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 17:42:12,748 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-43752/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 17:42:13,179 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-43752/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 17:42:13,180 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-43752/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 17:42:13,181 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-43752/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 17:42:13,182 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-43752/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 17:42:14,561 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 17:42:14,562 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 17:42:14,564 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 17:42:14,576 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-41929] due to args.save_total_limit\n",
      "{'loss': 2.6443, 'grad_norm': 3.832408905029297, 'learning_rate': 3.7931980252331326e-05, 'epoch': 24.14}\n",
      "{'loss': 2.6098, 'grad_norm': 3.912022352218628, 'learning_rate': 3.779484366428964e-05, 'epoch': 24.41}\n",
      "{'loss': 2.6187, 'grad_norm': 3.832465648651123, 'learning_rate': 3.7657707076247944e-05, 'epoch': 24.68}\n",
      "{'loss': 2.6216, 'grad_norm': 4.292030334472656, 'learning_rate': 3.752057048820625e-05, 'epoch': 24.96}\n",
      " 25%|███████▊                       | 45575/182300 [4:22:24<11:10:57,  3.40it/s][INFO|trainer.py:3388] 2024-05-11 17:51:51,828 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-45575\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 17:51:51,828 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-45575/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 17:51:51,829 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-45575/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 17:51:52,262 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-45575/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 17:51:52,263 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-45575/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 17:51:52,263 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-45575/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 17:51:52,264 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-45575/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 17:51:53,636 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 17:51:53,636 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 17:51:53,638 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 17:51:53,648 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-43752] due to args.save_total_limit\n",
      "{'loss': 2.5674, 'grad_norm': 3.9441139698028564, 'learning_rate': 3.738343390016457e-05, 'epoch': 25.23}\n",
      "{'loss': 2.5613, 'grad_norm': 3.8073363304138184, 'learning_rate': 3.7246297312122874e-05, 'epoch': 25.51}\n",
      "{'loss': 2.5744, 'grad_norm': 3.95381498336792, 'learning_rate': 3.7109160724081186e-05, 'epoch': 25.78}\n",
      " 26%|████████▎                       | 47398/182300 [4:32:22<9:54:58,  3.78it/s][INFO|trainer.py:3388] 2024-05-11 18:01:49,182 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-47398\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 18:01:49,183 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-47398/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 18:01:49,184 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-47398/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 18:01:49,612 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-47398/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 18:01:49,614 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-47398/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 18:01:49,614 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-47398/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 18:01:49,615 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-47398/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 18:01:50,987 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 18:01:50,988 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 18:01:50,989 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 18:01:50,995 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-45575] due to args.save_total_limit\n",
      "{'loss': 2.5581, 'grad_norm': 4.265843391418457, 'learning_rate': 3.69720241360395e-05, 'epoch': 26.06}\n",
      "{'loss': 2.5085, 'grad_norm': 3.7907886505126953, 'learning_rate': 3.683488754799781e-05, 'epoch': 26.33}\n",
      "{'loss': 2.5244, 'grad_norm': 3.9580938816070557, 'learning_rate': 3.6697750959956116e-05, 'epoch': 26.6}\n",
      "{'loss': 2.5283, 'grad_norm': 3.725271701812744, 'learning_rate': 3.656061437191443e-05, 'epoch': 26.88}\n",
      " 27%|████████▎                      | 49221/182300 [4:42:12<10:47:26,  3.43it/s][INFO|trainer.py:3388] 2024-05-11 18:11:39,142 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-49221\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 18:11:39,143 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-49221/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 18:11:39,143 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-49221/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 18:11:39,565 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-49221/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 18:11:39,566 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-49221/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 18:11:39,566 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-49221/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 18:11:39,567 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-49221/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 18:11:41,011 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 18:11:41,031 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 18:11:41,033 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 18:11:41,042 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-47398] due to args.save_total_limit\n",
      "{'loss': 2.4978, 'grad_norm': 4.27789831161499, 'learning_rate': 3.642347778387274e-05, 'epoch': 27.15}\n",
      "{'loss': 2.478, 'grad_norm': 4.248454570770264, 'learning_rate': 3.628634119583105e-05, 'epoch': 27.43}\n",
      "{'loss': 2.4797, 'grad_norm': 3.7782256603240967, 'learning_rate': 3.614920460778936e-05, 'epoch': 27.7}\n",
      "{'loss': 2.4872, 'grad_norm': 3.996277332305908, 'learning_rate': 3.601206801974767e-05, 'epoch': 27.98}\n",
      " 28%|████████▋                      | 51044/182300 [4:52:01<10:41:33,  3.41it/s][INFO|trainer.py:3388] 2024-05-11 18:21:28,198 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-51044\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 18:21:28,199 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-51044/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 18:21:28,199 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-51044/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 18:21:28,630 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-51044/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 18:21:28,631 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-51044/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 18:21:28,631 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-51044/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 18:21:28,632 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-51044/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 18:21:29,989 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 18:21:29,990 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 18:21:30,009 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 18:21:30,016 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-49221] due to args.save_total_limit\n",
      "{'loss': 2.4311, 'grad_norm': 4.143040657043457, 'learning_rate': 3.5874931431705984e-05, 'epoch': 28.25}\n",
      "{'loss': 2.4377, 'grad_norm': 3.6849589347839355, 'learning_rate': 3.573779484366429e-05, 'epoch': 28.52}\n",
      "{'loss': 2.4425, 'grad_norm': 3.621760606765747, 'learning_rate': 3.56006582556226e-05, 'epoch': 28.8}\n",
      " 29%|████████▉                      | 52867/182300 [5:02:03<10:27:33,  3.44it/s][INFO|trainer.py:3388] 2024-05-11 18:31:30,896 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-52867\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 18:31:30,897 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-52867/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 18:31:30,897 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-52867/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 18:31:31,314 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-52867/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 18:31:31,315 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-52867/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 18:31:31,315 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-52867/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 18:31:31,316 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-52867/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 18:31:32,838 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 18:31:32,838 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 18:31:32,841 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 18:31:32,852 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-51044] due to args.save_total_limit\n",
      "{'loss': 2.4287, 'grad_norm': 3.7394306659698486, 'learning_rate': 3.5463521667580914e-05, 'epoch': 29.07}\n",
      "{'loss': 2.3911, 'grad_norm': 3.782111167907715, 'learning_rate': 3.532638507953922e-05, 'epoch': 29.35}\n",
      "{'loss': 2.4084, 'grad_norm': 4.35050106048584, 'learning_rate': 3.518924849149753e-05, 'epoch': 29.62}\n",
      "{'loss': 2.3928, 'grad_norm': 3.8727004528045654, 'learning_rate': 3.5052111903455844e-05, 'epoch': 29.9}\n",
      " 30%|█████████▎                     | 54690/182300 [5:11:32<11:13:14,  3.16it/s][INFO|trainer.py:3388] 2024-05-11 18:40:59,916 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-54690\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 18:40:59,917 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-54690/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 18:40:59,917 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-54690/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 18:41:00,343 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-54690/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 18:41:00,344 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-54690/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 18:41:00,345 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-54690/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 18:41:00,346 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-54690/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 18:41:01,927 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 18:41:01,927 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 18:41:01,929 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 18:41:01,941 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-52867] due to args.save_total_limit\n",
      "{'loss': 2.3668, 'grad_norm': 3.974501371383667, 'learning_rate': 3.4914975315414157e-05, 'epoch': 30.17}\n",
      "{'loss': 2.3686, 'grad_norm': 3.7882275581359863, 'learning_rate': 3.477783872737246e-05, 'epoch': 30.44}\n",
      "{'loss': 2.3707, 'grad_norm': 3.8313581943511963, 'learning_rate': 3.4640702139330774e-05, 'epoch': 30.72}\n",
      "{'loss': 2.3632, 'grad_norm': 4.046344757080078, 'learning_rate': 3.450356555128909e-05, 'epoch': 30.99}\n",
      " 31%|█████████▌                     | 56513/182300 [5:20:54<10:06:54,  3.45it/s][INFO|trainer.py:3388] 2024-05-11 18:50:21,593 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-56513\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 18:50:21,594 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-56513/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 18:50:21,594 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-56513/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 18:50:22,025 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-56513/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 18:50:22,026 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-56513/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 18:50:22,026 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-56513/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 18:50:22,027 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-56513/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 18:50:23,475 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 18:50:23,495 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 18:50:23,497 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 18:50:23,506 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-54690] due to args.save_total_limit\n",
      "{'loss': 2.3229, 'grad_norm': 3.7165708541870117, 'learning_rate': 3.436642896324739e-05, 'epoch': 31.27}\n",
      "{'loss': 2.3262, 'grad_norm': 3.8072948455810547, 'learning_rate': 3.4229292375205705e-05, 'epoch': 31.54}\n",
      "{'loss': 2.3311, 'grad_norm': 3.8423380851745605, 'learning_rate': 3.409215578716402e-05, 'epoch': 31.82}\n",
      " 32%|██████████▏                     | 58336/182300 [5:30:35<9:27:53,  3.64it/s][INFO|trainer.py:3388] 2024-05-11 19:00:02,808 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-58336\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 19:00:02,809 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-58336/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 19:00:02,809 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-58336/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 19:00:03,245 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-58336/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 19:00:03,246 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-58336/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 19:00:03,246 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-58336/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 19:00:03,247 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-58336/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 19:00:04,802 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 19:00:04,803 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 19:00:04,805 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 19:00:04,817 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-56513] due to args.save_total_limit\n",
      "{'loss': 2.3213, 'grad_norm': 3.831343650817871, 'learning_rate': 3.395501919912233e-05, 'epoch': 32.09}\n",
      "{'loss': 2.2849, 'grad_norm': 3.8060476779937744, 'learning_rate': 3.3817882611080635e-05, 'epoch': 32.36}\n",
      "{'loss': 2.2979, 'grad_norm': 4.033987998962402, 'learning_rate': 3.368074602303895e-05, 'epoch': 32.64}\n",
      "{'loss': 2.2982, 'grad_norm': 3.870171546936035, 'learning_rate': 3.354360943499726e-05, 'epoch': 32.91}\n",
      " 33%|██████████▏                    | 60159/182300 [5:40:34<11:51:28,  2.86it/s][INFO|trainer.py:3388] 2024-05-11 19:10:01,045 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-60159\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 19:10:01,046 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-60159/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 19:10:01,046 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-60159/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 19:10:01,491 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-60159/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 19:10:01,492 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-60159/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 19:10:01,492 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-60159/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 19:10:01,493 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-60159/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 19:10:02,864 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 19:10:02,865 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 19:10:02,867 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 19:10:02,879 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-58336] due to args.save_total_limit\n",
      "{'loss': 2.264, 'grad_norm': 3.848620653152466, 'learning_rate': 3.340647284695557e-05, 'epoch': 33.19}\n",
      "{'loss': 2.2568, 'grad_norm': 4.048386573791504, 'learning_rate': 3.326933625891388e-05, 'epoch': 33.46}\n",
      "{'loss': 2.2602, 'grad_norm': 4.029069900512695, 'learning_rate': 3.313219967087219e-05, 'epoch': 33.74}\n",
      " 34%|██████████▌                    | 61982/182300 [5:50:19<10:57:49,  3.05it/s][INFO|trainer.py:3388] 2024-05-11 19:19:46,289 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-61982\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 19:19:46,290 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-61982/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 19:19:46,291 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-61982/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 19:19:46,704 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-61982/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 19:19:46,705 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-61982/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 19:19:46,705 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-61982/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 19:19:46,706 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-61982/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 19:19:48,070 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 19:19:48,070 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 19:19:48,072 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 19:19:48,081 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-60159] due to args.save_total_limit\n",
      "{'loss': 2.2763, 'grad_norm': 3.759799003601074, 'learning_rate': 3.29950630828305e-05, 'epoch': 34.01}\n",
      "{'loss': 2.2137, 'grad_norm': 4.440002918243408, 'learning_rate': 3.285792649478881e-05, 'epoch': 34.28}\n",
      "{'loss': 2.233, 'grad_norm': 3.961390972137451, 'learning_rate': 3.272078990674713e-05, 'epoch': 34.56}\n",
      "{'loss': 2.2414, 'grad_norm': 3.916156768798828, 'learning_rate': 3.258365331870543e-05, 'epoch': 34.83}\n",
      " 35%|██████████▊                    | 63805/182300 [6:00:02<10:32:40,  3.12it/s][INFO|trainer.py:3388] 2024-05-11 19:29:29,662 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-63805\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 19:29:29,663 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-63805/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 19:29:29,663 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-63805/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 19:29:30,090 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-63805/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 19:29:30,091 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-63805/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 19:29:30,091 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-63805/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 19:29:30,093 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-63805/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 19:29:31,466 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 19:29:31,466 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 19:29:31,469 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 19:29:31,480 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-61982] due to args.save_total_limit\n",
      "{'loss': 2.2137, 'grad_norm': 3.746572732925415, 'learning_rate': 3.2446516730663745e-05, 'epoch': 35.11}\n",
      "{'loss': 2.1907, 'grad_norm': 3.8424971103668213, 'learning_rate': 3.230938014262205e-05, 'epoch': 35.38}\n",
      "{'loss': 2.2104, 'grad_norm': 4.250007152557373, 'learning_rate': 3.217224355458036e-05, 'epoch': 35.66}\n",
      "{'loss': 2.2134, 'grad_norm': 3.760779857635498, 'learning_rate': 3.2035106966538675e-05, 'epoch': 35.93}\n",
      " 36%|███████████▌                    | 65628/182300 [6:09:53<9:04:25,  3.57it/s][INFO|trainer.py:3388] 2024-05-11 19:39:20,643 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-65628\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 19:39:20,644 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-65628/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 19:39:20,644 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-65628/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 19:39:21,064 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-65628/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 19:39:21,066 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-65628/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 19:39:21,066 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-65628/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 19:39:21,067 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-65628/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 19:39:22,438 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 19:39:22,439 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 19:39:22,441 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 19:39:22,453 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-63805] due to args.save_total_limit\n",
      "{'loss': 2.1714, 'grad_norm': 4.189092636108398, 'learning_rate': 3.189797037849698e-05, 'epoch': 36.2}\n",
      "{'loss': 2.1635, 'grad_norm': 3.6505000591278076, 'learning_rate': 3.17608337904553e-05, 'epoch': 36.48}\n",
      "{'loss': 2.1832, 'grad_norm': 3.7696096897125244, 'learning_rate': 3.1623697202413605e-05, 'epoch': 36.75}\n",
      " 37%|███████████▊                    | 67451/182300 [6:19:48<9:29:22,  3.36it/s][INFO|trainer.py:3388] 2024-05-11 19:49:15,854 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-67451\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 19:49:15,856 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-67451/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 19:49:15,856 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-67451/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 19:49:16,289 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-67451/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 19:49:16,290 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-67451/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 19:49:16,290 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-67451/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 19:49:16,291 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-67451/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 19:49:17,643 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 19:49:17,643 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 19:49:17,646 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 19:49:17,654 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-65628] due to args.save_total_limit\n",
      "{'loss': 2.1753, 'grad_norm': 3.744976282119751, 'learning_rate': 3.148656061437192e-05, 'epoch': 37.03}\n",
      "{'loss': 2.1326, 'grad_norm': 3.9759116172790527, 'learning_rate': 3.134942402633022e-05, 'epoch': 37.3}\n",
      "{'loss': 2.1423, 'grad_norm': 3.719237804412842, 'learning_rate': 3.1212287438288536e-05, 'epoch': 37.58}\n",
      "{'loss': 2.1602, 'grad_norm': 4.290117263793945, 'learning_rate': 3.107515085024685e-05, 'epoch': 37.85}\n",
      " 38%|████████████▏                   | 69274/182300 [6:29:19<9:29:15,  3.31it/s][INFO|trainer.py:3388] 2024-05-11 19:58:46,340 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-69274\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 19:58:46,341 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-69274/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 19:58:46,341 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-69274/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 19:58:46,760 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-69274/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 19:58:46,761 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-69274/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 19:58:46,761 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-69274/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 19:58:46,762 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-69274/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 19:58:48,108 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 19:58:48,109 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 19:58:48,110 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 19:58:48,128 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-67451] due to args.save_total_limit\n",
      "{'loss': 2.1349, 'grad_norm': 4.060342311859131, 'learning_rate': 3.0938014262205153e-05, 'epoch': 38.12}\n",
      "{'loss': 2.1018, 'grad_norm': 4.048706531524658, 'learning_rate': 3.080087767416347e-05, 'epoch': 38.4}\n",
      "{'loss': 2.1284, 'grad_norm': 4.130014896392822, 'learning_rate': 3.066374108612178e-05, 'epoch': 38.67}\n",
      "{'loss': 2.1255, 'grad_norm': 3.893848419189453, 'learning_rate': 3.052660449808009e-05, 'epoch': 38.95}\n",
      " 39%|████████████▍                   | 71097/182300 [6:39:16<9:06:59,  3.39it/s][INFO|trainer.py:3388] 2024-05-11 20:08:42,954 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-71097\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 20:08:42,955 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-71097/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 20:08:42,955 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-71097/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 20:08:43,388 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-71097/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 20:08:43,389 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-71097/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 20:08:43,390 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-71097/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 20:08:43,391 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-71097/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 20:08:44,819 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 20:08:44,819 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 20:08:44,821 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 20:08:44,838 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-69274] due to args.save_total_limit\n",
      "{'loss': 2.0848, 'grad_norm': 3.6254563331604004, 'learning_rate': 3.03894679100384e-05, 'epoch': 39.22}\n",
      "{'loss': 2.0933, 'grad_norm': 4.06374979019165, 'learning_rate': 3.025233132199671e-05, 'epoch': 39.5}\n",
      "{'loss': 2.11, 'grad_norm': 3.763274908065796, 'learning_rate': 3.011519473395502e-05, 'epoch': 39.77}\n",
      " 40%|████████████▊                   | 72920/182300 [6:48:51<9:59:33,  3.04it/s][INFO|trainer.py:3388] 2024-05-11 20:18:18,908 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-72920\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 20:18:18,909 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-72920/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 20:18:18,909 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-72920/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 20:18:19,327 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-72920/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 20:18:19,328 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-72920/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 20:18:19,328 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-72920/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 20:18:19,329 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-72920/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 20:18:20,674 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 20:18:20,675 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 20:18:20,676 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 20:18:20,686 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-71097] due to args.save_total_limit\n",
      "{'loss': 2.0918, 'grad_norm': 4.014530181884766, 'learning_rate': 2.997805814591333e-05, 'epoch': 40.04}\n",
      "{'loss': 2.0559, 'grad_norm': 3.6787962913513184, 'learning_rate': 2.9840921557871642e-05, 'epoch': 40.32}\n",
      "{'loss': 2.067, 'grad_norm': 3.752711057662964, 'learning_rate': 2.970378496982995e-05, 'epoch': 40.59}\n",
      "{'loss': 2.0792, 'grad_norm': 3.795217752456665, 'learning_rate': 2.9566648381788263e-05, 'epoch': 40.87}\n",
      " 41%|█████████████                   | 74743/182300 [6:58:38<9:21:35,  3.19it/s][INFO|trainer.py:3388] 2024-05-11 20:28:05,317 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-74743\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 20:28:05,318 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-74743/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 20:28:05,318 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-74743/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 20:28:05,771 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-74743/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 20:28:05,772 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-74743/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 20:28:05,772 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-74743/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 20:28:05,773 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-74743/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 20:28:07,172 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 20:28:07,172 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 20:28:07,174 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 20:28:07,183 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-72920] due to args.save_total_limit\n",
      "{'loss': 2.0608, 'grad_norm': 3.7484569549560547, 'learning_rate': 2.9429511793746572e-05, 'epoch': 41.14}\n",
      "{'loss': 2.0337, 'grad_norm': 3.601229190826416, 'learning_rate': 2.9292375205704885e-05, 'epoch': 41.42}\n",
      "{'loss': 2.0426, 'grad_norm': 3.9707863330841064, 'learning_rate': 2.9155238617663194e-05, 'epoch': 41.69}\n",
      "{'loss': 2.0571, 'grad_norm': 3.9523677825927734, 'learning_rate': 2.9018102029621502e-05, 'epoch': 41.96}\n",
      " 42%|█████████████▍                  | 76566/182300 [7:08:16<9:32:08,  3.08it/s][INFO|trainer.py:3388] 2024-05-11 20:37:43,770 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-76566\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 20:37:43,771 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-76566/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 20:37:43,771 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-76566/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 20:37:44,211 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-76566/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 20:37:44,212 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-76566/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 20:37:44,213 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-76566/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 20:37:44,214 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-76566/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 20:37:45,741 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 20:37:45,741 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 20:37:45,743 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 20:37:45,751 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-74743] due to args.save_total_limit\n",
      "{'loss': 2.0225, 'grad_norm': 4.046602725982666, 'learning_rate': 2.8880965441579815e-05, 'epoch': 42.24}\n",
      "{'loss': 2.0185, 'grad_norm': 4.059443950653076, 'learning_rate': 2.8743828853538124e-05, 'epoch': 42.51}\n",
      "{'loss': 2.0342, 'grad_norm': 4.066934108734131, 'learning_rate': 2.8606692265496436e-05, 'epoch': 42.79}\n",
      " 43%|█████████████▊                  | 78389/182300 [7:17:57<8:55:14,  3.24it/s][INFO|trainer.py:3388] 2024-05-11 20:47:24,649 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-78389\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 20:47:24,650 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-78389/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 20:47:24,650 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-78389/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 20:47:25,100 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-78389/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 20:47:25,102 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-78389/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 20:47:25,102 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-78389/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 20:47:25,103 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-78389/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 20:47:26,499 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 20:47:26,499 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 20:47:26,501 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 20:47:26,510 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-76566] due to args.save_total_limit\n",
      "{'loss': 2.0216, 'grad_norm': 3.811591386795044, 'learning_rate': 2.8469555677454745e-05, 'epoch': 43.06}\n",
      "{'loss': 1.989, 'grad_norm': 3.979374408721924, 'learning_rate': 2.833241908941306e-05, 'epoch': 43.34}\n",
      "{'loss': 1.9958, 'grad_norm': 3.67275333404541, 'learning_rate': 2.8195282501371366e-05, 'epoch': 43.61}\n",
      "{'loss': 2.0102, 'grad_norm': 3.790217399597168, 'learning_rate': 2.8058145913329675e-05, 'epoch': 43.88}\n",
      " 44%|██████████████                  | 80212/182300 [7:27:51<8:18:36,  3.41it/s][INFO|trainer.py:3388] 2024-05-11 20:57:18,485 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-80212\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 20:57:18,486 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-80212/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 20:57:18,486 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-80212/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 20:57:18,919 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-80212/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 20:57:18,921 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-80212/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 20:57:18,921 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-80212/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 20:57:18,922 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-80212/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 20:57:20,299 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 20:57:20,299 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 20:57:20,300 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 20:57:20,306 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-78389] due to args.save_total_limit\n",
      "{'loss': 1.9847, 'grad_norm': 3.9934496879577637, 'learning_rate': 2.7921009325287988e-05, 'epoch': 44.16}\n",
      "{'loss': 1.9752, 'grad_norm': 4.339521408081055, 'learning_rate': 2.7783872737246297e-05, 'epoch': 44.43}\n",
      "{'loss': 1.9883, 'grad_norm': 3.5851519107818604, 'learning_rate': 2.764673614920461e-05, 'epoch': 44.71}\n",
      "{'loss': 1.9896, 'grad_norm': 4.129305362701416, 'learning_rate': 2.7509599561162918e-05, 'epoch': 44.98}\n",
      " 45%|██████████████▍                 | 82035/182300 [7:37:40<8:12:10,  3.40it/s][INFO|trainer.py:3388] 2024-05-11 21:07:07,482 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-82035\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 21:07:07,482 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-82035/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 21:07:07,483 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-82035/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 21:07:07,909 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-82035/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 21:07:07,911 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-82035/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 21:07:07,911 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-82035/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 21:07:07,912 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-82035/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 21:07:09,368 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 21:07:09,368 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 21:07:09,369 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 21:07:09,397 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-80212] due to args.save_total_limit\n",
      "{'loss': 1.9465, 'grad_norm': 3.752852201461792, 'learning_rate': 2.7372462973121234e-05, 'epoch': 45.26}\n",
      "{'loss': 1.9589, 'grad_norm': 3.923309087753296, 'learning_rate': 2.723532638507954e-05, 'epoch': 45.53}\n",
      "{'loss': 1.9662, 'grad_norm': 4.141747951507568, 'learning_rate': 2.7098189797037848e-05, 'epoch': 45.8}\n",
      " 46%|██████████████▋                 | 83858/182300 [7:47:38<8:10:47,  3.34it/s][INFO|trainer.py:3388] 2024-05-11 21:17:05,483 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-83858\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 21:17:05,484 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-83858/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 21:17:05,484 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-83858/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 21:17:05,905 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-83858/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 21:17:05,907 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-83858/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 21:17:05,907 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-83858/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 21:17:05,908 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-83858/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 21:17:07,258 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 21:17:07,258 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 21:17:07,260 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 21:17:07,269 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-82035] due to args.save_total_limit\n",
      "{'loss': 1.9518, 'grad_norm': 4.118216514587402, 'learning_rate': 2.696105320899616e-05, 'epoch': 46.08}\n",
      "{'loss': 1.928, 'grad_norm': 4.061371326446533, 'learning_rate': 2.682391662095447e-05, 'epoch': 46.35}\n",
      "{'loss': 1.9456, 'grad_norm': 4.138849258422852, 'learning_rate': 2.6686780032912785e-05, 'epoch': 46.63}\n",
      "{'loss': 1.9465, 'grad_norm': 3.9675650596618652, 'learning_rate': 2.654964344487109e-05, 'epoch': 46.9}\n",
      " 47%|███████████████                 | 85681/182300 [7:57:19<7:38:12,  3.51it/s][INFO|trainer.py:3388] 2024-05-11 21:26:46,226 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-85681\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 21:26:46,227 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-85681/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 21:26:46,228 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-85681/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 21:26:46,665 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-85681/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 21:26:46,667 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-85681/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 21:26:46,667 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-85681/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 21:26:46,668 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-85681/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 21:26:48,071 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 21:26:48,092 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 21:26:48,094 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 21:26:48,103 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-83858] due to args.save_total_limit\n",
      "{'loss': 1.9293, 'grad_norm': 3.745779514312744, 'learning_rate': 2.6412506856829406e-05, 'epoch': 47.17}\n",
      "{'loss': 1.915, 'grad_norm': 3.6988871097564697, 'learning_rate': 2.6275370268787712e-05, 'epoch': 47.45}\n",
      "{'loss': 1.9199, 'grad_norm': 3.7044730186462402, 'learning_rate': 2.613823368074602e-05, 'epoch': 47.72}\n",
      "{'loss': 1.9243, 'grad_norm': 3.6700057983398438, 'learning_rate': 2.6001097092704337e-05, 'epoch': 48.0}\n",
      " 48%|███████████████▎                | 87504/182300 [8:07:00<7:32:05,  3.49it/s][INFO|trainer.py:3388] 2024-05-11 21:36:27,832 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-87504\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 21:36:27,833 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-87504/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 21:36:27,834 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-87504/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 21:36:28,264 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-87504/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 21:36:28,265 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-87504/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 21:36:28,265 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-87504/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 21:36:28,266 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-87504/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 21:36:29,624 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 21:36:29,625 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 21:36:29,626 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 21:36:29,647 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-85681] due to args.save_total_limit\n",
      "{'loss': 1.8846, 'grad_norm': 3.89973521232605, 'learning_rate': 2.5863960504662642e-05, 'epoch': 48.27}\n",
      "{'loss': 1.8999, 'grad_norm': 4.041015625, 'learning_rate': 2.5726823916620958e-05, 'epoch': 48.55}\n",
      "{'loss': 1.9085, 'grad_norm': 3.7937917709350586, 'learning_rate': 2.5589687328579264e-05, 'epoch': 48.82}\n",
      " 49%|███████████████▋                | 89327/182300 [8:16:44<7:29:49,  3.44it/s][INFO|trainer.py:3388] 2024-05-11 21:46:11,062 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-89327\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 21:46:11,063 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-89327/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 21:46:11,063 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-89327/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 21:46:11,484 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-89327/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 21:46:11,485 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-89327/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 21:46:11,486 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-89327/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 21:46:11,487 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-89327/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 21:46:12,999 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 21:46:13,000 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 21:46:13,002 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 21:46:13,014 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-87504] due to args.save_total_limit\n",
      "{'loss': 1.8934, 'grad_norm': 4.050382614135742, 'learning_rate': 2.545255074053758e-05, 'epoch': 49.09}\n",
      "{'loss': 1.8705, 'grad_norm': 3.809558391571045, 'learning_rate': 2.5315414152495888e-05, 'epoch': 49.37}\n",
      "{'loss': 1.8904, 'grad_norm': 3.6460201740264893, 'learning_rate': 2.51782775644542e-05, 'epoch': 49.64}\n",
      "{'loss': 1.8936, 'grad_norm': 3.959718704223633, 'learning_rate': 2.504114097641251e-05, 'epoch': 49.92}\n",
      " 50%|████████████████                | 91150/182300 [8:26:31<7:02:14,  3.60it/s][INFO|trainer.py:3388] 2024-05-11 21:55:58,080 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-91150\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 21:55:58,081 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-91150/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 21:55:58,082 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-91150/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 21:55:58,508 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-91150/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 21:55:58,510 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-91150/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 21:55:58,510 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-91150/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 21:55:58,511 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-91150/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 21:56:00,026 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 21:56:00,026 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 21:56:00,028 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 21:56:00,039 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-89327] due to args.save_total_limit\n",
      "{'loss': 1.8683, 'grad_norm': 3.786888837814331, 'learning_rate': 2.490400438837082e-05, 'epoch': 50.19}\n",
      "{'loss': 1.8634, 'grad_norm': 3.477952241897583, 'learning_rate': 2.476686780032913e-05, 'epoch': 50.47}\n",
      "{'loss': 1.8637, 'grad_norm': 3.998764991760254, 'learning_rate': 2.462973121228744e-05, 'epoch': 50.74}\n",
      " 51%|████████████████▎               | 92973/182300 [8:36:09<6:56:09,  3.58it/s][INFO|trainer.py:3388] 2024-05-11 22:05:36,314 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-92973\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 22:05:36,315 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-92973/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 22:05:36,316 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-92973/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 22:05:36,747 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-92973/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 22:05:36,748 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-92973/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 22:05:36,749 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-92973/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 22:05:36,750 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-92973/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 22:05:38,102 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 22:05:38,103 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 22:05:38,104 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 22:05:38,113 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-91150] due to args.save_total_limit\n",
      "{'loss': 1.8749, 'grad_norm': 4.029101848602295, 'learning_rate': 2.449259462424575e-05, 'epoch': 51.01}\n",
      "{'loss': 1.8377, 'grad_norm': 3.8711071014404297, 'learning_rate': 2.435545803620406e-05, 'epoch': 51.29}\n",
      "{'loss': 1.8397, 'grad_norm': 3.922783136367798, 'learning_rate': 2.421832144816237e-05, 'epoch': 51.56}\n",
      "{'loss': 1.8609, 'grad_norm': 4.025134086608887, 'learning_rate': 2.4081184860120682e-05, 'epoch': 51.84}\n",
      " 52%|████████████████▋               | 94796/182300 [8:45:47<7:15:20,  3.35it/s][INFO|trainer.py:3388] 2024-05-11 22:15:14,261 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-94796\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 22:15:14,262 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-94796/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 22:15:14,262 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-94796/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 22:15:14,687 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-94796/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 22:15:14,688 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-94796/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 22:15:14,688 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-94796/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 22:15:14,689 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-94796/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 22:15:16,033 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 22:15:16,033 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 22:15:16,035 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 22:15:16,044 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-92973] due to args.save_total_limit\n",
      "{'loss': 1.8407, 'grad_norm': 3.800508975982666, 'learning_rate': 2.394404827207899e-05, 'epoch': 52.11}\n",
      "{'loss': 1.8263, 'grad_norm': 3.944465160369873, 'learning_rate': 2.3806911684037304e-05, 'epoch': 52.39}\n",
      "{'loss': 1.8374, 'grad_norm': 4.014648914337158, 'learning_rate': 2.3669775095995613e-05, 'epoch': 52.66}\n",
      "{'loss': 1.8448, 'grad_norm': 4.08259916305542, 'learning_rate': 2.353263850795392e-05, 'epoch': 52.93}\n",
      " 53%|████████████████▉               | 96619/182300 [8:55:34<7:17:47,  3.26it/s][INFO|trainer.py:3388] 2024-05-11 22:25:01,123 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-96619\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 22:25:01,124 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-96619/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 22:25:01,124 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-96619/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 22:25:01,560 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-96619/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 22:25:01,562 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-96619/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 22:25:01,562 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-96619/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 22:25:01,563 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-96619/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 22:25:02,925 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 22:25:02,925 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 22:25:02,927 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 22:25:02,936 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-94796] due to args.save_total_limit\n",
      "{'loss': 1.8131, 'grad_norm': 3.941981792449951, 'learning_rate': 2.3395501919912234e-05, 'epoch': 53.21}\n",
      "{'loss': 1.8169, 'grad_norm': 3.8573715686798096, 'learning_rate': 2.3258365331870543e-05, 'epoch': 53.48}\n",
      "{'loss': 1.8145, 'grad_norm': 3.987938165664673, 'learning_rate': 2.3121228743828855e-05, 'epoch': 53.76}\n",
      " 54%|█████████████████▎              | 98442/182300 [9:05:17<6:09:44,  3.78it/s][INFO|trainer.py:3388] 2024-05-11 22:34:44,447 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-98442\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 22:34:44,448 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-98442/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 22:34:44,448 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-98442/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 22:34:44,886 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-98442/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 22:34:44,888 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-98442/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 22:34:44,888 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-98442/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 22:34:44,889 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-98442/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 22:34:46,272 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 22:34:46,273 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 22:34:46,274 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 22:34:46,281 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-96619] due to args.save_total_limit\n",
      "{'loss': 1.8213, 'grad_norm': 3.652238607406616, 'learning_rate': 2.2984092155787164e-05, 'epoch': 54.03}\n",
      "{'loss': 1.7938, 'grad_norm': 3.640587568283081, 'learning_rate': 2.2846955567745476e-05, 'epoch': 54.31}\n",
      "{'loss': 1.7963, 'grad_norm': 3.884443759918213, 'learning_rate': 2.2709818979703785e-05, 'epoch': 54.58}\n",
      "{'loss': 1.811, 'grad_norm': 4.245452880859375, 'learning_rate': 2.2572682391662098e-05, 'epoch': 54.85}\n",
      " 55%|█████████████████              | 100265/182300 [9:14:44<7:39:58,  2.97it/s][INFO|trainer.py:3388] 2024-05-11 22:44:11,671 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-100265\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 22:44:11,672 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-100265/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 22:44:11,672 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-100265/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 22:44:12,091 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-100265/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 22:44:12,093 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-100265/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 22:44:12,093 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-100265/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 22:44:12,094 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-100265/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 22:44:13,430 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 22:44:13,431 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 22:44:13,433 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 22:44:13,442 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-98442] due to args.save_total_limit\n",
      "{'loss': 1.7964, 'grad_norm': 3.97247576713562, 'learning_rate': 2.2435545803620407e-05, 'epoch': 55.13}\n",
      "{'loss': 1.7774, 'grad_norm': 3.8827009201049805, 'learning_rate': 2.2298409215578716e-05, 'epoch': 55.4}\n",
      "{'loss': 1.7884, 'grad_norm': 4.079446792602539, 'learning_rate': 2.2161272627537028e-05, 'epoch': 55.68}\n",
      "{'loss': 1.7904, 'grad_norm': 4.093244552612305, 'learning_rate': 2.2024136039495337e-05, 'epoch': 55.95}\n",
      " 56%|█████████████████▎             | 102088/182300 [9:24:22<5:47:54,  3.84it/s][INFO|trainer.py:3388] 2024-05-11 22:53:49,275 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-102088\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 22:53:49,276 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-102088/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 22:53:49,276 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-102088/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 22:53:49,703 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-102088/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 22:53:49,704 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-102088/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 22:53:49,705 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-102088/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 22:53:49,706 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-102088/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 22:53:51,057 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 22:53:51,058 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 22:53:51,060 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 22:53:51,068 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-100265] due to args.save_total_limit\n",
      "{'loss': 1.7655, 'grad_norm': 3.674686908721924, 'learning_rate': 2.188699945145365e-05, 'epoch': 56.23}\n",
      "{'loss': 1.7708, 'grad_norm': 4.042862892150879, 'learning_rate': 2.174986286341196e-05, 'epoch': 56.5}\n",
      "{'loss': 1.777, 'grad_norm': 4.069617748260498, 'learning_rate': 2.161272627537027e-05, 'epoch': 56.77}\n",
      " 57%|█████████████████▋             | 103911/182300 [9:33:54<7:01:51,  3.10it/s][INFO|trainer.py:3388] 2024-05-11 23:03:21,803 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-103911\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 23:03:21,804 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-103911/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 23:03:21,804 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-103911/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 23:03:22,226 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-103911/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 23:03:22,228 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-103911/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 23:03:22,228 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-103911/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 23:03:22,229 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-103911/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 23:03:23,567 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 23:03:23,567 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 23:03:23,569 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 23:03:23,581 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-102088] due to args.save_total_limit\n",
      "{'loss': 1.7702, 'grad_norm': 3.68752384185791, 'learning_rate': 2.1475589687328583e-05, 'epoch': 57.05}\n",
      "{'loss': 1.7508, 'grad_norm': 3.998215436935425, 'learning_rate': 2.133845309928689e-05, 'epoch': 57.32}\n",
      "{'loss': 1.7634, 'grad_norm': 4.300554275512695, 'learning_rate': 2.12013165112452e-05, 'epoch': 57.6}\n",
      "{'loss': 1.7603, 'grad_norm': 3.649411678314209, 'learning_rate': 2.106417992320351e-05, 'epoch': 57.87}\n",
      " 58%|█████████████████▉             | 105734/182300 [9:43:16<6:24:09,  3.32it/s][INFO|trainer.py:3388] 2024-05-11 23:12:43,095 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-105734\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 23:12:43,096 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-105734/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 23:12:43,096 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-105734/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 23:12:43,518 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-105734/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 23:12:43,519 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-105734/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 23:12:43,519 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-105734/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 23:12:43,520 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-105734/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 23:12:44,870 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 23:12:44,871 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 23:12:44,872 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 23:12:44,881 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-103911] due to args.save_total_limit\n",
      "{'loss': 1.7441, 'grad_norm': 4.17492151260376, 'learning_rate': 2.0927043335161822e-05, 'epoch': 58.15}\n",
      "{'loss': 1.7381, 'grad_norm': 3.8550057411193848, 'learning_rate': 2.0789906747120134e-05, 'epoch': 58.42}\n",
      "{'loss': 1.7475, 'grad_norm': 4.004961967468262, 'learning_rate': 2.0652770159078443e-05, 'epoch': 58.69}\n",
      "{'loss': 1.7532, 'grad_norm': 3.936483144760132, 'learning_rate': 2.0515633571036756e-05, 'epoch': 58.97}\n",
      " 59%|██████████████████▎            | 107557/182300 [9:52:35<5:53:46,  3.52it/s][INFO|trainer.py:3388] 2024-05-11 23:22:02,391 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-107557\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 23:22:02,392 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-107557/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 23:22:02,393 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-107557/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 23:22:02,827 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-107557/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 23:22:02,829 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-107557/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 23:22:02,829 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-107557/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 23:22:02,830 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-107557/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 23:22:04,204 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 23:22:04,205 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 23:22:04,206 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 23:22:04,216 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-105734] due to args.save_total_limit\n",
      "{'loss': 1.7232, 'grad_norm': 3.812488317489624, 'learning_rate': 2.037849698299506e-05, 'epoch': 59.24}\n",
      "{'loss': 1.7196, 'grad_norm': 4.185512542724609, 'learning_rate': 2.0241360394953374e-05, 'epoch': 59.52}\n",
      "{'loss': 1.7356, 'grad_norm': 4.278858184814453, 'learning_rate': 2.0104223806911686e-05, 'epoch': 59.79}\n",
      " 60%|██████████████████            | 109380/182300 [10:02:13<5:44:35,  3.53it/s][INFO|trainer.py:3388] 2024-05-11 23:31:39,960 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-109380\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 23:31:39,961 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-109380/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 23:31:39,961 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-109380/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 23:31:40,386 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-109380/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 23:31:40,388 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-109380/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 23:31:40,388 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-109380/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 23:31:40,389 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-109380/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 23:31:41,735 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 23:31:41,735 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 23:31:41,737 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 23:31:41,746 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-107557] due to args.save_total_limit\n",
      "{'loss': 1.7328, 'grad_norm': 4.104213714599609, 'learning_rate': 1.9967087218869995e-05, 'epoch': 60.07}\n",
      "{'loss': 1.7109, 'grad_norm': 4.215428352355957, 'learning_rate': 1.9829950630828307e-05, 'epoch': 60.34}\n",
      "{'loss': 1.7144, 'grad_norm': 4.020122528076172, 'learning_rate': 1.9692814042786616e-05, 'epoch': 60.61}\n",
      "{'loss': 1.7335, 'grad_norm': 3.9703729152679443, 'learning_rate': 1.955567745474493e-05, 'epoch': 60.89}\n",
      " 61%|██████████████████▎           | 111203/182300 [10:11:47<5:48:35,  3.40it/s][INFO|trainer.py:3388] 2024-05-11 23:41:14,685 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-111203\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 23:41:14,686 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-111203/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 23:41:14,686 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-111203/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 23:41:15,117 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-111203/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 23:41:15,118 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-111203/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 23:41:15,118 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-111203/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 23:41:15,119 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-111203/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 23:41:16,577 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 23:41:16,577 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 23:41:16,579 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 23:41:16,588 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-109380] due to args.save_total_limit\n",
      "{'loss': 1.7048, 'grad_norm': 3.900017023086548, 'learning_rate': 1.9418540866703238e-05, 'epoch': 61.16}\n",
      "{'loss': 1.6902, 'grad_norm': 4.7137627601623535, 'learning_rate': 1.9281404278661547e-05, 'epoch': 61.44}\n",
      "{'loss': 1.7104, 'grad_norm': 4.035908222198486, 'learning_rate': 1.914426769061986e-05, 'epoch': 61.71}\n",
      "{'loss': 1.7155, 'grad_norm': 4.041805744171143, 'learning_rate': 1.9007131102578168e-05, 'epoch': 61.99}\n",
      " 62%|██████████████████▌           | 113026/182300 [10:21:34<5:38:39,  3.41it/s][INFO|trainer.py:3388] 2024-05-11 23:51:01,000 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-113026\n",
      "[INFO|configuration_utils.py:471] 2024-05-11 23:51:01,001 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-113026/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-11 23:51:01,002 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-113026/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-11 23:51:01,428 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-113026/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 23:51:01,430 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-113026/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 23:51:01,430 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-113026/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 23:51:01,431 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-113026/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-11 23:51:02,965 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-11 23:51:02,965 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-11 23:51:02,967 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-11 23:51:02,976 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-111203] due to args.save_total_limit\n",
      "{'loss': 1.6851, 'grad_norm': 3.817702054977417, 'learning_rate': 1.886999451453648e-05, 'epoch': 62.26}\n",
      "{'loss': 1.6857, 'grad_norm': 3.8696234226226807, 'learning_rate': 1.873285792649479e-05, 'epoch': 62.53}\n",
      "{'loss': 1.7037, 'grad_norm': 3.909179925918579, 'learning_rate': 1.85957213384531e-05, 'epoch': 62.81}\n",
      " 63%|██████████████████▉           | 114849/182300 [10:31:15<5:11:34,  3.61it/s][INFO|trainer.py:3388] 2024-05-12 00:00:42,839 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-114849\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 00:00:42,839 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-114849/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 00:00:42,840 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-114849/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 00:00:43,265 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-114849/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 00:00:43,267 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-114849/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 00:00:43,267 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-114849/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 00:00:43,268 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-114849/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 00:00:44,614 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 00:00:44,615 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 00:00:44,616 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 00:00:44,625 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-113026] due to args.save_total_limit\n",
      "{'loss': 1.6936, 'grad_norm': 3.8557326793670654, 'learning_rate': 1.845858475041141e-05, 'epoch': 63.08}\n",
      "{'loss': 1.6786, 'grad_norm': 4.332828044891357, 'learning_rate': 1.832144816236972e-05, 'epoch': 63.36}\n",
      "{'loss': 1.6768, 'grad_norm': 4.454130172729492, 'learning_rate': 1.818431157432803e-05, 'epoch': 63.63}\n",
      "{'loss': 1.6905, 'grad_norm': 3.943071126937866, 'learning_rate': 1.804717498628634e-05, 'epoch': 63.91}\n",
      " 64%|███████████████████▏          | 116672/182300 [10:40:51<4:50:37,  3.76it/s][INFO|trainer.py:3388] 2024-05-12 00:10:18,307 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-116672\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 00:10:18,308 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-116672/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 00:10:18,309 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-116672/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 00:10:18,747 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-116672/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 00:10:18,748 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-116672/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 00:10:18,748 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-116672/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 00:10:18,749 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-116672/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 00:10:20,129 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 00:10:20,129 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 00:10:20,131 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 00:10:20,140 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-114849] due to args.save_total_limit\n",
      "{'loss': 1.663, 'grad_norm': 4.255739688873291, 'learning_rate': 1.7910038398244653e-05, 'epoch': 64.18}\n",
      "{'loss': 1.6603, 'grad_norm': 4.027384281158447, 'learning_rate': 1.7772901810202962e-05, 'epoch': 64.45}\n",
      "{'loss': 1.6701, 'grad_norm': 3.8232147693634033, 'learning_rate': 1.7635765222161274e-05, 'epoch': 64.73}\n",
      " 65%|███████████████████▌          | 118495/182300 [10:50:30<5:27:28,  3.25it/s][INFO|trainer.py:3388] 2024-05-12 00:19:57,318 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-118495\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 00:19:57,319 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-118495/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 00:19:57,319 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-118495/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 00:19:57,760 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-118495/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 00:19:57,761 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-118495/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 00:19:57,762 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-118495/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 00:19:57,763 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-118495/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 00:19:59,138 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 00:19:59,138 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 00:19:59,139 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 00:19:59,156 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-116672] due to args.save_total_limit\n",
      "{'loss': 1.6797, 'grad_norm': 3.83734130859375, 'learning_rate': 1.7498628634119583e-05, 'epoch': 65.0}\n",
      "{'loss': 1.6441, 'grad_norm': 3.9775922298431396, 'learning_rate': 1.7361492046077896e-05, 'epoch': 65.28}\n",
      "{'loss': 1.6534, 'grad_norm': 3.810086250305176, 'learning_rate': 1.7224355458036205e-05, 'epoch': 65.55}\n",
      "{'loss': 1.6651, 'grad_norm': 4.3292036056518555, 'learning_rate': 1.7087218869994513e-05, 'epoch': 65.83}\n",
      " 66%|███████████████████▊          | 120318/182300 [11:00:44<5:59:43,  2.87it/s][INFO|trainer.py:3388] 2024-05-12 00:30:11,784 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-120318\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 00:30:11,785 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-120318/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 00:30:11,786 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-120318/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 00:30:12,211 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-120318/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 00:30:12,212 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-120318/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 00:30:12,212 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-120318/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 00:30:12,213 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-120318/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 00:30:13,560 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 00:30:13,560 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 00:30:13,561 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 00:30:13,567 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-118495] due to args.save_total_limit\n",
      "{'loss': 1.654, 'grad_norm': 4.046462535858154, 'learning_rate': 1.6950082281952826e-05, 'epoch': 66.1}\n",
      "{'loss': 1.6383, 'grad_norm': 4.200257778167725, 'learning_rate': 1.6812945693911135e-05, 'epoch': 66.37}\n",
      "{'loss': 1.6571, 'grad_norm': 3.98045015335083, 'learning_rate': 1.6675809105869447e-05, 'epoch': 66.65}\n",
      "{'loss': 1.6486, 'grad_norm': 3.9323537349700928, 'learning_rate': 1.653867251782776e-05, 'epoch': 66.92}\n",
      " 67%|████████████████████          | 122141/182300 [11:11:00<5:14:38,  3.19it/s][INFO|trainer.py:3388] 2024-05-12 00:40:27,213 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-122141\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 00:40:27,214 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-122141/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 00:40:27,214 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-122141/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 00:40:27,646 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-122141/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 00:40:27,648 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-122141/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 00:40:27,648 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-122141/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 00:40:27,649 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-122141/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 00:40:29,004 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 00:40:29,004 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 00:40:29,006 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 00:40:29,019 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-120318] due to args.save_total_limit\n",
      "{'loss': 1.6288, 'grad_norm': 3.898150682449341, 'learning_rate': 1.640153592978607e-05, 'epoch': 67.2}\n",
      "{'loss': 1.6305, 'grad_norm': 3.8490869998931885, 'learning_rate': 1.6264399341744377e-05, 'epoch': 67.47}\n",
      "{'loss': 1.6393, 'grad_norm': 4.125833034515381, 'learning_rate': 1.6127262753702686e-05, 'epoch': 67.75}\n",
      " 68%|████████████████████▍         | 123964/182300 [11:21:37<5:19:58,  3.04it/s][INFO|trainer.py:3388] 2024-05-12 00:51:04,619 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-123964\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 00:51:04,620 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-123964/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 00:51:04,620 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-123964/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 00:51:05,051 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-123964/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 00:51:05,052 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-123964/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 00:51:05,052 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-123964/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 00:51:05,053 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-123964/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 00:51:06,624 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 00:51:06,625 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 00:51:06,627 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 00:51:06,635 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-122141] due to args.save_total_limit\n",
      "{'loss': 1.6441, 'grad_norm': 4.1837286949157715, 'learning_rate': 1.5990126165661e-05, 'epoch': 68.02}\n",
      "{'loss': 1.6144, 'grad_norm': 4.150059700012207, 'learning_rate': 1.585298957761931e-05, 'epoch': 68.29}\n",
      "{'loss': 1.6254, 'grad_norm': 4.325094223022461, 'learning_rate': 1.571585298957762e-05, 'epoch': 68.57}\n",
      "{'loss': 1.636, 'grad_norm': 3.9832139015197754, 'learning_rate': 1.5578716401535932e-05, 'epoch': 68.84}\n",
      " 69%|████████████████████▋         | 125787/182300 [11:32:03<5:25:46,  2.89it/s][INFO|trainer.py:3388] 2024-05-12 01:01:30,238 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-125787\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 01:01:30,239 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-125787/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 01:01:30,239 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-125787/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 01:01:30,677 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-125787/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 01:01:30,679 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-125787/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 01:01:30,679 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-125787/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 01:01:30,680 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-125787/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 01:01:32,163 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 01:01:32,163 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 01:01:32,165 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 01:01:32,171 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-123964] due to args.save_total_limit\n",
      "{'loss': 1.6183, 'grad_norm': 3.9516079425811768, 'learning_rate': 1.544157981349424e-05, 'epoch': 69.12}\n",
      "{'loss': 1.6116, 'grad_norm': 3.982802391052246, 'learning_rate': 1.5304443225452554e-05, 'epoch': 69.39}\n",
      "{'loss': 1.6183, 'grad_norm': 4.178645610809326, 'learning_rate': 1.516730663741086e-05, 'epoch': 69.67}\n",
      "{'loss': 1.6226, 'grad_norm': 4.045616149902344, 'learning_rate': 1.5030170049369171e-05, 'epoch': 69.94}\n",
      " 70%|█████████████████████         | 127610/182300 [11:42:21<4:59:12,  3.05it/s][INFO|trainer.py:3388] 2024-05-12 01:11:48,870 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-127610\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 01:11:48,871 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-127610/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 01:11:48,871 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-127610/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 01:11:49,309 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-127610/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 01:11:49,310 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-127610/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 01:11:49,310 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-127610/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 01:11:49,311 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-127610/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 01:11:50,772 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 01:11:50,772 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 01:11:50,774 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 01:11:50,783 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-125787] due to args.save_total_limit\n",
      "{'loss': 1.5999, 'grad_norm': 4.098151206970215, 'learning_rate': 1.4893033461327482e-05, 'epoch': 70.21}\n",
      "{'loss': 1.5946, 'grad_norm': 4.052021026611328, 'learning_rate': 1.4755896873285793e-05, 'epoch': 70.49}\n",
      "{'loss': 1.6172, 'grad_norm': 3.7580652236938477, 'learning_rate': 1.4618760285244103e-05, 'epoch': 70.76}\n",
      " 71%|█████████████████████▎        | 129433/182300 [11:52:21<4:49:55,  3.04it/s][INFO|trainer.py:3388] 2024-05-12 01:21:48,799 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-129433\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 01:21:48,800 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-129433/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 01:21:48,801 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-129433/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 01:21:49,234 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-129433/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 01:21:49,236 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-129433/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 01:21:49,236 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-129433/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 01:21:49,237 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-129433/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 01:21:50,602 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 01:21:50,602 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 01:21:50,603 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 01:21:50,610 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-127610] due to args.save_total_limit\n",
      "{'loss': 1.6121, 'grad_norm': 4.1435866355896, 'learning_rate': 1.4481623697202416e-05, 'epoch': 71.04}\n",
      "{'loss': 1.588, 'grad_norm': 3.739654302597046, 'learning_rate': 1.4344487109160726e-05, 'epoch': 71.31}\n",
      "{'loss': 1.5998, 'grad_norm': 4.380291938781738, 'learning_rate': 1.4207350521119034e-05, 'epoch': 71.59}\n",
      "{'loss': 1.5958, 'grad_norm': 3.7885782718658447, 'learning_rate': 1.4070213933077344e-05, 'epoch': 71.86}\n",
      " 72%|█████████████████████▌        | 131256/182300 [12:03:07<4:24:18,  3.22it/s][INFO|trainer.py:3388] 2024-05-12 01:32:33,976 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-131256\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 01:32:33,977 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-131256/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 01:32:33,977 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-131256/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 01:32:34,417 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-131256/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 01:32:34,419 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-131256/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 01:32:34,419 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-131256/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 01:32:34,420 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-131256/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 01:32:35,915 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 01:32:35,915 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 01:32:35,916 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 01:32:35,924 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-129433] due to args.save_total_limit\n",
      "{'loss': 1.5956, 'grad_norm': 4.236293792724609, 'learning_rate': 1.3933077345035655e-05, 'epoch': 72.13}\n",
      "{'loss': 1.5766, 'grad_norm': 4.205173015594482, 'learning_rate': 1.3795940756993966e-05, 'epoch': 72.41}\n",
      "{'loss': 1.5912, 'grad_norm': 4.034268379211426, 'learning_rate': 1.3658804168952278e-05, 'epoch': 72.68}\n",
      "{'loss': 1.5897, 'grad_norm': 3.9170260429382324, 'learning_rate': 1.3521667580910589e-05, 'epoch': 72.96}\n",
      " 73%|█████████████████████▉        | 133079/182300 [12:13:33<4:05:02,  3.35it/s][INFO|trainer.py:3388] 2024-05-12 01:43:00,458 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-133079\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 01:43:00,459 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-133079/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 01:43:00,459 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-133079/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 01:43:00,891 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-133079/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 01:43:00,893 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-133079/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 01:43:00,893 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-133079/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 01:43:00,894 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-133079/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 01:43:02,251 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 01:43:02,251 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 01:43:02,253 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 01:43:02,259 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-131256] due to args.save_total_limit\n",
      "{'loss': 1.5765, 'grad_norm': 3.925799608230591, 'learning_rate': 1.33845309928689e-05, 'epoch': 73.23}\n",
      "{'loss': 1.5775, 'grad_norm': 4.052227973937988, 'learning_rate': 1.324739440482721e-05, 'epoch': 73.51}\n",
      "{'loss': 1.5834, 'grad_norm': 4.2378034591674805, 'learning_rate': 1.3110257816785517e-05, 'epoch': 73.78}\n",
      " 74%|██████████████████████▏       | 134902/182300 [12:24:35<4:22:08,  3.01it/s][INFO|trainer.py:3388] 2024-05-12 01:54:02,287 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-134902\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 01:54:02,288 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-134902/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 01:54:02,289 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-134902/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 01:54:02,729 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-134902/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 01:54:02,730 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-134902/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 01:54:02,730 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-134902/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 01:54:02,731 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-134902/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 01:54:04,265 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 01:54:04,265 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 01:54:04,267 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 01:54:04,286 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-133079] due to args.save_total_limit\n",
      "{'loss': 1.5812, 'grad_norm': 4.073320388793945, 'learning_rate': 1.297312122874383e-05, 'epoch': 74.05}\n",
      "{'loss': 1.5588, 'grad_norm': 4.102873802185059, 'learning_rate': 1.283598464070214e-05, 'epoch': 74.33}\n",
      "{'loss': 1.5785, 'grad_norm': 4.223252773284912, 'learning_rate': 1.269884805266045e-05, 'epoch': 74.6}\n",
      "{'loss': 1.5686, 'grad_norm': 4.320130825042725, 'learning_rate': 1.2561711464618761e-05, 'epoch': 74.88}\n",
      " 75%|██████████████████████▌       | 136725/182300 [12:35:17<4:08:42,  3.05it/s][INFO|trainer.py:3388] 2024-05-12 02:04:44,326 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-136725\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 02:04:44,327 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-136725/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 02:04:44,327 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-136725/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 02:04:44,767 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-136725/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 02:04:44,769 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-136725/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 02:04:44,769 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-136725/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 02:04:44,770 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-136725/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 02:04:46,143 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 02:04:46,144 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 02:04:46,145 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 02:04:46,154 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-134902] due to args.save_total_limit\n",
      "{'loss': 1.5688, 'grad_norm': 4.706448078155518, 'learning_rate': 1.2424574876577072e-05, 'epoch': 75.15}\n",
      "{'loss': 1.5563, 'grad_norm': 4.024387359619141, 'learning_rate': 1.2287438288535381e-05, 'epoch': 75.43}\n",
      "{'loss': 1.5712, 'grad_norm': 3.9221880435943604, 'learning_rate': 1.2150301700493692e-05, 'epoch': 75.7}\n",
      "{'loss': 1.5654, 'grad_norm': 4.27291202545166, 'learning_rate': 1.2013165112452002e-05, 'epoch': 75.97}\n",
      " 76%|██████████████████████▊       | 138548/182300 [12:46:22<3:44:04,  3.25it/s][INFO|trainer.py:3388] 2024-05-12 02:15:48,924 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-138548\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 02:15:48,925 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-138548/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 02:15:48,925 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-138548/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 02:15:49,353 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-138548/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 02:15:49,354 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-138548/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 02:15:49,355 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-138548/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 02:15:49,356 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-138548/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 02:15:50,714 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 02:15:50,715 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 02:15:50,716 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 02:15:50,725 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-136725] due to args.save_total_limit\n",
      "{'loss': 1.5449, 'grad_norm': 4.373564720153809, 'learning_rate': 1.1876028524410313e-05, 'epoch': 76.25}\n",
      "{'loss': 1.5571, 'grad_norm': 4.030310153961182, 'learning_rate': 1.1738891936368624e-05, 'epoch': 76.52}\n",
      "{'loss': 1.5568, 'grad_norm': 4.002580165863037, 'learning_rate': 1.1601755348326934e-05, 'epoch': 76.8}\n",
      " 77%|███████████████████████       | 140371/182300 [12:57:23<4:10:19,  2.79it/s][INFO|trainer.py:3388] 2024-05-12 02:26:50,036 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-140371\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 02:26:50,037 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-140371/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 02:26:50,037 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-140371/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 02:26:50,467 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-140371/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 02:26:50,468 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-140371/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 02:26:50,469 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-140371/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 02:26:50,470 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-140371/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 02:26:51,823 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 02:26:51,824 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 02:26:51,825 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 02:26:51,834 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-138548] due to args.save_total_limit\n",
      "{'loss': 1.5545, 'grad_norm': 4.0623369216918945, 'learning_rate': 1.1464618760285245e-05, 'epoch': 77.07}\n",
      "{'loss': 1.5442, 'grad_norm': 4.049304485321045, 'learning_rate': 1.1327482172243554e-05, 'epoch': 77.35}\n",
      "{'loss': 1.5464, 'grad_norm': 3.891969680786133, 'learning_rate': 1.1190345584201866e-05, 'epoch': 77.62}\n",
      "{'loss': 1.5509, 'grad_norm': 4.165316104888916, 'learning_rate': 1.1053208996160177e-05, 'epoch': 77.89}\n",
      " 78%|███████████████████████▍      | 142194/182300 [13:08:33<3:49:48,  2.91it/s][INFO|trainer.py:3388] 2024-05-12 02:38:00,897 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-142194\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 02:38:00,898 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-142194/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 02:38:00,899 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-142194/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 02:38:01,355 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-142194/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 02:38:01,356 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-142194/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 02:38:01,356 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-142194/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 02:38:01,358 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-142194/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 02:38:02,762 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 02:38:02,762 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 02:38:02,764 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 02:38:02,773 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-140371] due to args.save_total_limit\n",
      "{'loss': 1.5375, 'grad_norm': 4.1472249031066895, 'learning_rate': 1.0916072408118487e-05, 'epoch': 78.17}\n",
      "{'loss': 1.5345, 'grad_norm': 4.173414707183838, 'learning_rate': 1.0778935820076796e-05, 'epoch': 78.44}\n",
      "{'loss': 1.5381, 'grad_norm': 3.9279398918151855, 'learning_rate': 1.0641799232035107e-05, 'epoch': 78.72}\n",
      "{'loss': 1.552, 'grad_norm': 4.222446441650391, 'learning_rate': 1.0504662643993418e-05, 'epoch': 78.99}\n",
      " 79%|███████████████████████▋      | 144017/182300 [13:19:57<4:04:24,  2.61it/s][INFO|trainer.py:3388] 2024-05-12 02:49:24,850 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-144017\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 02:49:24,851 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-144017/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 02:49:24,851 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-144017/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 02:49:25,285 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-144017/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 02:49:25,287 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-144017/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 02:49:25,287 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-144017/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 02:49:25,288 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-144017/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 02:49:26,670 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 02:49:26,671 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 02:49:26,672 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 02:49:26,682 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-142194] due to args.save_total_limit\n",
      "{'loss': 1.5217, 'grad_norm': 3.8020248413085938, 'learning_rate': 1.0367526055951728e-05, 'epoch': 79.26}\n",
      "{'loss': 1.5346, 'grad_norm': 3.8790934085845947, 'learning_rate': 1.0230389467910039e-05, 'epoch': 79.54}\n",
      "{'loss': 1.5354, 'grad_norm': 4.49297571182251, 'learning_rate': 1.009325287986835e-05, 'epoch': 79.81}\n",
      " 80%|████████████████████████      | 145840/182300 [13:31:20<3:30:19,  2.89it/s][INFO|trainer.py:3388] 2024-05-12 03:00:47,563 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-145840\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 03:00:47,564 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-145840/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 03:00:47,564 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-145840/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 03:00:48,003 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-145840/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 03:00:48,005 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-145840/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 03:00:48,005 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-145840/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 03:00:48,006 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-145840/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 03:00:49,391 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 03:00:49,391 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 03:00:49,392 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 03:00:49,398 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-144017] due to args.save_total_limit\n",
      "{'loss': 1.5326, 'grad_norm': 4.024161338806152, 'learning_rate': 9.95611629182666e-06, 'epoch': 80.09}\n",
      "{'loss': 1.5133, 'grad_norm': 3.997326374053955, 'learning_rate': 9.818979703784971e-06, 'epoch': 80.36}\n",
      "{'loss': 1.5252, 'grad_norm': 4.163906574249268, 'learning_rate': 9.68184311574328e-06, 'epoch': 80.64}\n",
      "{'loss': 1.5356, 'grad_norm': 4.333358287811279, 'learning_rate': 9.54470652770159e-06, 'epoch': 80.91}\n",
      " 81%|████████████████████████▎     | 147663/182300 [13:42:49<3:30:48,  2.74it/s][INFO|trainer.py:3388] 2024-05-12 03:12:16,918 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-147663\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 03:12:16,919 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-147663/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 03:12:16,919 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-147663/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 03:12:17,364 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-147663/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 03:12:17,365 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-147663/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 03:12:17,366 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-147663/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 03:12:17,367 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-147663/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 03:12:18,880 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 03:12:18,880 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 03:12:18,900 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 03:12:18,906 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-145840] due to args.save_total_limit\n",
      "{'loss': 1.5191, 'grad_norm': 4.201995372772217, 'learning_rate': 9.407569939659903e-06, 'epoch': 81.18}\n",
      "{'loss': 1.5168, 'grad_norm': 3.8196020126342773, 'learning_rate': 9.270433351618212e-06, 'epoch': 81.46}\n",
      "{'loss': 1.5214, 'grad_norm': 4.32403039932251, 'learning_rate': 9.133296763576522e-06, 'epoch': 81.73}\n",
      " 82%|████████████████████████▌     | 149486/182300 [13:54:06<2:58:01,  3.07it/s][INFO|trainer.py:3388] 2024-05-12 03:23:33,142 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-149486\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 03:23:33,143 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-149486/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 03:23:33,143 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-149486/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 03:23:33,577 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-149486/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 03:23:33,578 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-149486/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 03:23:33,579 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-149486/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 03:23:33,580 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-149486/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 03:23:35,213 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 03:23:35,213 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 03:23:35,215 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 03:23:35,222 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-147663] due to args.save_total_limit\n",
      "{'loss': 1.5242, 'grad_norm': 4.165477752685547, 'learning_rate': 8.996160175534833e-06, 'epoch': 82.01}\n",
      "{'loss': 1.5022, 'grad_norm': 4.155007362365723, 'learning_rate': 8.859023587493144e-06, 'epoch': 82.28}\n",
      "{'loss': 1.5172, 'grad_norm': 4.049638748168945, 'learning_rate': 8.721886999451453e-06, 'epoch': 82.56}\n",
      "{'loss': 1.5097, 'grad_norm': 4.376342296600342, 'learning_rate': 8.584750411409765e-06, 'epoch': 82.83}\n",
      " 83%|████████████████████████▉     | 151309/182300 [14:05:15<3:03:06,  2.82it/s][INFO|trainer.py:3388] 2024-05-12 03:34:42,581 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-151309\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 03:34:42,582 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-151309/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 03:34:42,583 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-151309/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 03:34:43,008 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-151309/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 03:34:43,010 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-151309/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 03:34:43,010 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-151309/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 03:34:43,011 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-151309/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 03:34:44,453 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 03:34:44,453 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 03:34:44,455 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 03:34:44,463 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-149486] due to args.save_total_limit\n",
      "{'loss': 1.5163, 'grad_norm': 4.413540363311768, 'learning_rate': 8.447613823368076e-06, 'epoch': 83.1}\n",
      "{'loss': 1.5011, 'grad_norm': 3.754805326461792, 'learning_rate': 8.310477235326386e-06, 'epoch': 83.38}\n",
      "{'loss': 1.5026, 'grad_norm': 4.167300224304199, 'learning_rate': 8.173340647284695e-06, 'epoch': 83.65}\n",
      "{'loss': 1.5101, 'grad_norm': 4.1614861488342285, 'learning_rate': 8.036204059243006e-06, 'epoch': 83.93}\n",
      " 84%|█████████████████████████▏    | 153132/182300 [14:16:43<2:47:22,  2.90it/s][INFO|trainer.py:3388] 2024-05-12 03:46:10,368 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-153132\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 03:46:10,369 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-153132/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 03:46:10,369 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-153132/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 03:46:10,821 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-153132/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 03:46:10,823 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-153132/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 03:46:10,823 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-153132/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 03:46:10,824 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-153132/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 03:46:12,214 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 03:46:12,215 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 03:46:12,217 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 03:46:12,226 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-151309] due to args.save_total_limit\n",
      "{'loss': 1.4981, 'grad_norm': 4.183162212371826, 'learning_rate': 7.899067471201317e-06, 'epoch': 84.2}\n",
      "{'loss': 1.4965, 'grad_norm': 4.0559539794921875, 'learning_rate': 7.761930883159627e-06, 'epoch': 84.48}\n",
      "{'loss': 1.5019, 'grad_norm': 4.252512454986572, 'learning_rate': 7.624794295117937e-06, 'epoch': 84.75}\n",
      " 85%|█████████████████████████▌    | 154955/182300 [14:28:16<2:39:29,  2.86it/s][INFO|trainer.py:3388] 2024-05-12 03:57:43,704 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-154955\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 03:57:43,705 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-154955/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 03:57:43,705 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-154955/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 03:57:44,129 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-154955/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 03:57:44,130 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-154955/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 03:57:44,131 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-154955/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 03:57:44,132 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-154955/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 03:57:45,484 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 03:57:45,485 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 03:57:45,486 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 03:57:45,497 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-153132] due to args.save_total_limit\n",
      "{'loss': 1.5138, 'grad_norm': 3.9115328788757324, 'learning_rate': 7.4876577070762485e-06, 'epoch': 85.02}\n",
      "{'loss': 1.492, 'grad_norm': 4.217545032501221, 'learning_rate': 7.350521119034559e-06, 'epoch': 85.3}\n",
      "{'loss': 1.4897, 'grad_norm': 3.974954128265381, 'learning_rate': 7.21338453099287e-06, 'epoch': 85.57}\n",
      "{'loss': 1.4996, 'grad_norm': 4.266519069671631, 'learning_rate': 7.07624794295118e-06, 'epoch': 85.85}\n",
      " 86%|█████████████████████████▊    | 156778/182300 [14:40:00<2:17:04,  3.10it/s][INFO|trainer.py:3388] 2024-05-12 04:09:27,325 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-156778\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 04:09:27,326 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-156778/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 04:09:27,326 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-156778/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 04:09:27,767 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-156778/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 04:09:27,768 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-156778/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 04:09:27,769 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-156778/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 04:09:27,770 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-156778/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 04:09:29,145 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 04:09:29,146 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 04:09:29,147 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 04:09:29,156 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-154955] due to args.save_total_limit\n",
      "{'loss': 1.4891, 'grad_norm': 3.8507697582244873, 'learning_rate': 6.93911135490949e-06, 'epoch': 86.12}\n",
      "{'loss': 1.4848, 'grad_norm': 4.050006866455078, 'learning_rate': 6.801974766867801e-06, 'epoch': 86.4}\n",
      "{'loss': 1.4946, 'grad_norm': 4.006500720977783, 'learning_rate': 6.664838178826111e-06, 'epoch': 86.67}\n",
      "{'loss': 1.495, 'grad_norm': 4.2527289390563965, 'learning_rate': 6.527701590784421e-06, 'epoch': 86.94}\n",
      " 87%|██████████████████████████    | 158601/182300 [14:51:34<2:18:57,  2.84it/s][INFO|trainer.py:3388] 2024-05-12 04:21:01,449 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-158601\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 04:21:01,450 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-158601/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 04:21:01,451 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-158601/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 04:21:01,877 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-158601/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 04:21:01,878 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-158601/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 04:21:01,879 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-158601/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 04:21:01,880 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-158601/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 04:21:03,218 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 04:21:03,218 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 04:21:03,220 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 04:21:03,229 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-156778] due to args.save_total_limit\n",
      "{'loss': 1.4834, 'grad_norm': 4.087696552276611, 'learning_rate': 6.390565002742732e-06, 'epoch': 87.22}\n",
      "{'loss': 1.48, 'grad_norm': 3.9683475494384766, 'learning_rate': 6.253428414701043e-06, 'epoch': 87.49}\n",
      "{'loss': 1.4799, 'grad_norm': 4.009182453155518, 'learning_rate': 6.116291826659353e-06, 'epoch': 87.77}\n",
      " 88%|██████████████████████████▍   | 160424/182300 [15:03:10<2:05:00,  2.92it/s][INFO|trainer.py:3388] 2024-05-12 04:32:37,876 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-160424\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 04:32:37,877 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-160424/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 04:32:37,877 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-160424/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 04:32:38,313 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-160424/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 04:32:38,315 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-160424/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 04:32:38,315 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-160424/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 04:32:38,316 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-160424/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 04:32:39,659 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 04:32:39,659 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 04:32:39,661 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 04:32:39,669 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-158601] due to args.save_total_limit\n",
      "{'loss': 1.4827, 'grad_norm': 3.9172310829162598, 'learning_rate': 5.979155238617663e-06, 'epoch': 88.04}\n",
      "{'loss': 1.4772, 'grad_norm': 3.920940399169922, 'learning_rate': 5.842018650575974e-06, 'epoch': 88.32}\n",
      "{'loss': 1.4831, 'grad_norm': 4.178516387939453, 'learning_rate': 5.704882062534284e-06, 'epoch': 88.59}\n",
      "{'loss': 1.4796, 'grad_norm': 4.068806171417236, 'learning_rate': 5.567745474492595e-06, 'epoch': 88.86}\n",
      " 89%|██████████████████████████▋   | 162247/182300 [15:14:53<1:59:55,  2.79it/s][INFO|trainer.py:3388] 2024-05-12 04:44:20,004 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-162247\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 04:44:20,005 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-162247/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 04:44:20,005 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-162247/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 04:44:20,436 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-162247/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 04:44:20,438 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-162247/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 04:44:20,438 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-162247/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 04:44:20,439 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-162247/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 04:44:21,916 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 04:44:21,917 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 04:44:21,919 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 04:44:21,930 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-160424] due to args.save_total_limit\n",
      "{'loss': 1.4734, 'grad_norm': 3.923023223876953, 'learning_rate': 5.430608886450905e-06, 'epoch': 89.14}\n",
      "{'loss': 1.4675, 'grad_norm': 4.0538411140441895, 'learning_rate': 5.293472298409216e-06, 'epoch': 89.41}\n",
      "{'loss': 1.4812, 'grad_norm': 4.289505481719971, 'learning_rate': 5.156335710367526e-06, 'epoch': 89.69}\n",
      "{'loss': 1.4761, 'grad_norm': 4.2184247970581055, 'learning_rate': 5.019199122325837e-06, 'epoch': 89.96}\n",
      " 90%|███████████████████████████   | 164070/182300 [15:26:54<2:00:57,  2.51it/s][INFO|trainer.py:3388] 2024-05-12 04:56:21,137 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-164070\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 04:56:21,138 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-164070/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 04:56:21,139 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-164070/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 04:56:21,568 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-164070/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 04:56:21,569 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-164070/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 04:56:21,569 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-164070/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 04:56:21,570 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-164070/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 04:56:22,940 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 04:56:22,940 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 04:56:22,942 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 04:56:22,950 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-162247] due to args.save_total_limit\n",
      "{'loss': 1.4659, 'grad_norm': 4.014777183532715, 'learning_rate': 4.8820625342841474e-06, 'epoch': 90.24}\n",
      "{'loss': 1.4701, 'grad_norm': 4.025433540344238, 'learning_rate': 4.744925946242457e-06, 'epoch': 90.51}\n",
      "{'loss': 1.4745, 'grad_norm': 4.117000102996826, 'learning_rate': 4.607789358200768e-06, 'epoch': 90.78}\n",
      " 91%|███████████████████████████▎  | 165893/182300 [15:38:56<1:47:02,  2.55it/s][INFO|trainer.py:3388] 2024-05-12 05:08:23,500 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-165893\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 05:08:23,501 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-165893/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 05:08:23,501 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-165893/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 05:08:23,930 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-165893/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 05:08:23,931 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-165893/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 05:08:23,932 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-165893/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 05:08:23,933 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-165893/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 05:08:25,530 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 05:08:25,530 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 05:08:25,532 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 05:08:25,541 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-164070] due to args.save_total_limit\n",
      "{'loss': 1.4693, 'grad_norm': 4.047626495361328, 'learning_rate': 4.4706527701590785e-06, 'epoch': 91.06}\n",
      "{'loss': 1.4624, 'grad_norm': 4.17887020111084, 'learning_rate': 4.333516182117389e-06, 'epoch': 91.33}\n",
      "{'loss': 1.4665, 'grad_norm': 4.2437639236450195, 'learning_rate': 4.196379594075699e-06, 'epoch': 91.61}\n",
      "{'loss': 1.4731, 'grad_norm': 3.7711315155029297, 'learning_rate': 4.0592430060340105e-06, 'epoch': 91.88}\n",
      " 92%|███████████████████████████▌  | 167716/182300 [15:50:39<1:26:37,  2.81it/s][INFO|trainer.py:3388] 2024-05-12 05:20:06,552 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-167716\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 05:20:06,553 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-167716/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 05:20:06,554 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-167716/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 05:20:06,996 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-167716/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 05:20:06,998 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-167716/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 05:20:06,998 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-167716/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 05:20:06,999 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-167716/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 05:20:08,371 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 05:20:08,371 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 05:20:08,373 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 05:20:08,382 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-165893] due to args.save_total_limit\n",
      "{'loss': 1.4642, 'grad_norm': 4.002791404724121, 'learning_rate': 3.92210641799232e-06, 'epoch': 92.16}\n",
      "{'loss': 1.4584, 'grad_norm': 4.0743231773376465, 'learning_rate': 3.7849698299506313e-06, 'epoch': 92.43}\n",
      "{'loss': 1.4623, 'grad_norm': 4.080685138702393, 'learning_rate': 3.647833241908941e-06, 'epoch': 92.7}\n",
      "{'loss': 1.4682, 'grad_norm': 4.304593563079834, 'learning_rate': 3.510696653867252e-06, 'epoch': 92.98}\n",
      " 93%|███████████████████████████▉  | 169539/182300 [16:02:38<1:20:37,  2.64it/s][INFO|trainer.py:3388] 2024-05-12 05:32:05,510 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-169539\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 05:32:05,511 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-169539/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 05:32:05,511 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-169539/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 05:32:05,948 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-169539/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 05:32:05,950 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-169539/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 05:32:05,950 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-169539/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 05:32:05,951 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-169539/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 05:32:07,382 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 05:32:07,382 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 05:32:07,384 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 05:32:07,424 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-167716] due to args.save_total_limit\n",
      "{'loss': 1.4535, 'grad_norm': 4.447428226470947, 'learning_rate': 3.3735600658255624e-06, 'epoch': 93.25}\n",
      "{'loss': 1.4592, 'grad_norm': 4.22756814956665, 'learning_rate': 3.236423477783873e-06, 'epoch': 93.53}\n",
      "{'loss': 1.4632, 'grad_norm': 4.293380260467529, 'learning_rate': 3.0992868897421833e-06, 'epoch': 93.8}\n",
      " 94%|████████████████████████████▏ | 171362/182300 [16:14:15<1:01:59,  2.94it/s][INFO|trainer.py:3388] 2024-05-12 05:43:42,704 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-171362\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 05:43:42,706 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-171362/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 05:43:42,706 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-171362/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 05:43:43,128 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-171362/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 05:43:43,130 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-171362/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 05:43:43,130 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-171362/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 05:43:43,131 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-171362/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 05:43:44,513 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 05:43:44,534 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 05:43:44,535 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 05:43:44,544 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-169539] due to args.save_total_limit\n",
      "{'loss': 1.4605, 'grad_norm': 4.07041072845459, 'learning_rate': 2.962150301700494e-06, 'epoch': 94.08}\n",
      "{'loss': 1.4551, 'grad_norm': 4.039161205291748, 'learning_rate': 2.825013713658804e-06, 'epoch': 94.35}\n",
      "{'loss': 1.456, 'grad_norm': 4.1246795654296875, 'learning_rate': 2.687877125617115e-06, 'epoch': 94.62}\n",
      "{'loss': 1.4512, 'grad_norm': 4.026761054992676, 'learning_rate': 2.550740537575425e-06, 'epoch': 94.9}\n",
      " 95%|██████████████████████████████▍ | 173185/182300 [16:26:11<49:13,  3.09it/s][INFO|trainer.py:3388] 2024-05-12 05:55:38,774 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-173185\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 05:55:38,775 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-173185/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 05:55:38,775 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-173185/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 05:55:39,212 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-173185/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 05:55:39,214 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-173185/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 05:55:39,214 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-173185/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 05:55:39,215 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-173185/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 05:55:40,666 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 05:55:40,666 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 05:55:40,668 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 05:55:40,680 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-171362] due to args.save_total_limit\n",
      "{'loss': 1.4575, 'grad_norm': 4.5864715576171875, 'learning_rate': 2.4136039495337357e-06, 'epoch': 95.17}\n",
      "{'loss': 1.4475, 'grad_norm': 4.117992401123047, 'learning_rate': 2.2764673614920463e-06, 'epoch': 95.45}\n",
      "{'loss': 1.4581, 'grad_norm': 4.155096530914307, 'learning_rate': 2.1393307734503565e-06, 'epoch': 95.72}\n",
      "{'loss': 1.4521, 'grad_norm': 4.28767204284668, 'learning_rate': 2.002194185408667e-06, 'epoch': 96.0}\n",
      " 96%|██████████████████████████████▋ | 175008/182300 [16:38:14<47:40,  2.55it/s][INFO|trainer.py:3388] 2024-05-12 06:07:41,845 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-175008\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 06:07:41,846 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-175008/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 06:07:41,846 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-175008/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 06:07:42,272 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-175008/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 06:07:42,274 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-175008/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 06:07:42,274 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-175008/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 06:07:42,275 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-175008/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 06:07:43,655 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 06:07:43,655 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 06:07:43,657 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 06:07:43,665 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-173185] due to args.save_total_limit\n",
      "{'loss': 1.4488, 'grad_norm': 4.1511077880859375, 'learning_rate': 1.8650575973669776e-06, 'epoch': 96.27}\n",
      "{'loss': 1.4534, 'grad_norm': 4.336985111236572, 'learning_rate': 1.727921009325288e-06, 'epoch': 96.54}\n",
      "{'loss': 1.45, 'grad_norm': 4.181045055389404, 'learning_rate': 1.5907844212835987e-06, 'epoch': 96.82}\n",
      " 97%|███████████████████████████████ | 176831/182300 [16:49:53<28:53,  3.15it/s][INFO|trainer.py:3388] 2024-05-12 06:19:20,042 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-176831\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 06:19:20,043 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-176831/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 06:19:20,043 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-176831/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 06:19:20,476 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-176831/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 06:19:20,478 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-176831/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 06:19:20,478 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-176831/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 06:19:20,479 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-176831/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 06:19:22,094 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 06:19:22,095 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 06:19:22,097 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 06:19:22,109 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-175008] due to args.save_total_limit\n",
      "{'loss': 1.4498, 'grad_norm': 4.217624187469482, 'learning_rate': 1.453647833241909e-06, 'epoch': 97.09}\n",
      "{'loss': 1.4507, 'grad_norm': 3.8873023986816406, 'learning_rate': 1.3165112452002194e-06, 'epoch': 97.37}\n",
      "{'loss': 1.451, 'grad_norm': 4.3951191902160645, 'learning_rate': 1.17937465715853e-06, 'epoch': 97.64}\n",
      "{'loss': 1.4484, 'grad_norm': 4.1204118728637695, 'learning_rate': 1.0422380691168404e-06, 'epoch': 97.92}\n",
      " 98%|███████████████████████████████▎| 178654/182300 [17:01:50<18:49,  3.23it/s][INFO|trainer.py:3388] 2024-05-12 06:31:17,188 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-178654\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 06:31:17,189 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-178654/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 06:31:17,189 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-178654/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 06:31:17,635 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-178654/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 06:31:17,637 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-178654/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 06:31:17,637 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-178654/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 06:31:17,638 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-178654/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 06:31:19,009 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 06:31:19,009 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 06:31:19,011 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 06:31:19,020 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-176831] due to args.save_total_limit\n",
      "{'loss': 1.4431, 'grad_norm': 4.278495788574219, 'learning_rate': 9.05101481075151e-07, 'epoch': 98.19}\n",
      "{'loss': 1.4493, 'grad_norm': 4.186399459838867, 'learning_rate': 7.679648930334613e-07, 'epoch': 98.46}\n",
      "{'loss': 1.4418, 'grad_norm': 4.110637187957764, 'learning_rate': 6.308283049917719e-07, 'epoch': 98.74}\n",
      " 99%|███████████████████████████████▋| 180477/182300 [17:13:38<09:29,  3.20it/s][INFO|trainer.py:3388] 2024-05-12 06:43:04,982 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-180477\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 06:43:04,983 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-180477/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 06:43:04,984 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-180477/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 06:43:05,422 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-180477/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 06:43:05,423 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-180477/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 06:43:05,423 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-180477/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 06:43:05,424 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-180477/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 06:43:06,774 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 06:43:06,775 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 06:43:06,776 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 06:43:06,782 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-178654] due to args.save_total_limit\n",
      "{'loss': 1.4407, 'grad_norm': 3.9559993743896484, 'learning_rate': 4.936917169500823e-07, 'epoch': 99.01}\n",
      "{'loss': 1.4421, 'grad_norm': 4.4722418785095215, 'learning_rate': 3.565551289083928e-07, 'epoch': 99.29}\n",
      "{'loss': 1.4469, 'grad_norm': 4.151792526245117, 'learning_rate': 2.1941854086670326e-07, 'epoch': 99.56}\n",
      "{'loss': 1.4463, 'grad_norm': 4.128389835357666, 'learning_rate': 8.228195282501371e-08, 'epoch': 99.84}\n",
      "100%|████████████████████████████████| 182300/182300 [17:25:28<00:00,  2.73it/s][INFO|trainer.py:3388] 2024-05-12 06:54:55,449 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037/checkpoint-182300\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 06:54:55,450 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-182300/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 06:54:55,450 >> Configuration saved in /t5-small-finetuned-iwslt2037/checkpoint-182300/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 06:54:55,884 >> Model weights saved in /t5-small-finetuned-iwslt2037/checkpoint-182300/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 06:54:55,886 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/checkpoint-182300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 06:54:55,886 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/checkpoint-182300/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 06:54:55,887 >> Copy vocab file to /t5-small-finetuned-iwslt2037/checkpoint-182300/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 06:54:57,484 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 06:54:57,484 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 06:54:57,487 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 06:54:57,498 >> Deleting older checkpoint [/t5-small-finetuned-iwslt2037/checkpoint-180477] due to args.save_total_limit\n",
      "[INFO|trainer.py:2329] 2024-05-12 06:54:57,623 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 62730.7046, 'train_samples_per_second': 185.96, 'train_steps_per_second': 2.906, 'train_loss': 2.261507166926833, 'epoch': 100.0}\n",
      "100%|████████████████████████████████| 182300/182300 [17:25:30<00:00,  2.91it/s]\n",
      "[INFO|trainer.py:4148] 2024-05-12 06:54:57,625 >> Waiting for the current checkpoint push to be finished, this might take a couple of minutes.\n",
      "[INFO|trainer.py:3388] 2024-05-12 06:55:20,996 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 06:55:20,997 >> Configuration saved in /t5-small-finetuned-iwslt2037/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 06:55:20,997 >> Configuration saved in /t5-small-finetuned-iwslt2037/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 06:55:21,935 >> Model weights saved in /t5-small-finetuned-iwslt2037/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 06:55:21,937 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 06:55:21,937 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 06:55:21,940 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|trainer.py:3388] 2024-05-12 06:55:21,951 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 06:55:21,953 >> Configuration saved in /t5-small-finetuned-iwslt2037/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 06:55:21,953 >> Configuration saved in /t5-small-finetuned-iwslt2037/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 06:55:22,843 >> Model weights saved in /t5-small-finetuned-iwslt2037/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 06:55:22,845 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 06:55:22,845 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 06:55:22,847 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|modelcard.py:450] 2024-05-12 06:55:23,102 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Sequence-to-sequence Language Modeling', 'type': 'text2text-generation'}}\n",
      "***** train metrics *****\n",
      "  epoch                    =       100.0\n",
      "  total_flos               = 294080203GF\n",
      "  train_loss               =      2.2615\n",
      "  train_runtime            = 17:25:30.70\n",
      "  train_samples            =      116654\n",
      "  train_samples_per_second =      185.96\n",
      "  train_steps_per_second   =       2.906\n",
      "05/12/2024 06:55:25 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3697] 2024-05-12 06:55:25,724 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3699] 2024-05-12 06:55:25,724 >>   Num examples = 888\n",
      "[INFO|trainer.py:3702] 2024-05-12 06:55:25,724 >>   Batch size = 128\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:04<00:00,  1.50it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =      100.0\n",
      "  eval_bleu               =     0.2502\n",
      "  eval_gen_len            =    26.2162\n",
      "  eval_loss               =       2.42\n",
      "  eval_runtime            = 0:00:05.44\n",
      "  eval_samples            =        888\n",
      "  eval_samples_per_second =    163.166\n",
      "  eval_steps_per_second   =      1.286\n",
      "05/12/2024 06:55:31 - INFO - __main__ - *** Predict ***\n",
      "[INFO|trainer.py:3697] 2024-05-12 06:55:31,169 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:3699] 2024-05-12 06:55:31,169 >>   Num examples = 1138\n",
      "[INFO|trainer.py:3702] 2024-05-12 06:55:31,169 >>   Batch size = 128\n",
      "100%|█████████████████████████████████████████████| 9/9 [00:04<00:00,  1.86it/s]\n",
      "***** predict metrics *****\n",
      "  predict_bleu               =     0.1921\n",
      "  predict_gen_len            =    20.8805\n",
      "  predict_loss               =     3.0907\n",
      "  predict_runtime            = 0:00:05.62\n",
      "  predict_samples            =       1138\n",
      "  predict_samples_per_second =    202.453\n",
      "  predict_steps_per_second   =      1.601\n",
      "[INFO|trainer.py:3388] 2024-05-12 06:55:36,814 >> Saving model checkpoint to /t5-small-finetuned-iwslt2037\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 06:55:36,815 >> Configuration saved in /t5-small-finetuned-iwslt2037/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 06:55:36,815 >> Configuration saved in /t5-small-finetuned-iwslt2037/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 06:55:37,806 >> Model weights saved in /t5-small-finetuned-iwslt2037/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 06:55:37,807 >> tokenizer config file saved in /t5-small-finetuned-iwslt2037/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 06:55:37,808 >> Special tokens file saved in /t5-small-finetuned-iwslt2037/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 06:55:37,809 >> Copy vocab file to /t5-small-finetuned-iwslt2037/spiece.model\n",
      "[INFO|modelcard.py:450] 2024-05-12 06:55:38,060 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Sequence-to-sequence Language Modeling', 'type': 'text2text-generation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 0.2502}]}\n",
      "events.out.tfevents.1715496931.0d573eeffc83.44348.1: 100%|█| 465/465 [00:00<00:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] =\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "!cp run_translation_base_huggingface_example.py transformers-main/examples/pytorch/translation/run_translation_base.py\n",
    "!python transformers-main/examples/pytorch/translation/run_translation_base.py \\\n",
    "    --model_name_or_path google-t5/t5-small \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --do_predict \\\n",
    "    --source_lang de \\\n",
    "    --target_lang en \\\n",
    "    --source_prefix \"translate German to English: \" \\\n",
    "    --train_file data/iwslt17.de.en/train-de-en.json \\\n",
    "    --validation_file data/iwslt17.de.en/validation-de-en.json \\\n",
    "    --test_file data/iwslt17.de.en/test-de-en.json \\\n",
    "    --output_dir /t5-small-scratch-iwslt2017 \\\n",
    "    --per_device_train_batch_size=64 \\\n",
    "    --per_device_eval_batch_size=128 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --predict_with_generate    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-12 13:36:54.920763: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-12 13:36:55.772207: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using custom data configuration default-2098d2201d7f61e4\n",
      "05/12/2024 13:36:59 - INFO - datasets.builder - Using custom data configuration default-2098d2201d7f61e4\n",
      "Loading Dataset Infos from /opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "05/12/2024 13:36:59 - INFO - datasets.info - Loading Dataset Infos from /opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "05/12/2024 13:36:59 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2098d2201d7f61e4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7\n",
      "05/12/2024 13:36:59 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2098d2201d7f61e4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-2098d2201d7f61e4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)\n",
      "05/12/2024 13:36:59 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-2098d2201d7f61e4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2098d2201d7f61e4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7\n",
      "05/12/2024 13:36:59 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-2098d2201d7f61e4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:726] 2024-05-12 13:36:59,320 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-05-12 13:36:59,323 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"google-t5/t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.41.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2102] 2024-05-12 13:36:59,525 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2102] 2024-05-12 13:36:59,525 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2102] 2024-05-12 13:36:59,525 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2102] 2024-05-12 13:36:59,526 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2102] 2024-05-12 13:36:59,526 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:926] 2024-05-12 13:36:59,595 >> Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Running tokenizer on train dataset:   0%|     | 0/116654 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-2098d2201d7f61e4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-8a70bffead7fbfc9.arrow\n",
      "05/12/2024 13:37:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-2098d2201d7f61e4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-8a70bffead7fbfc9.arrow\n",
      "Running tokenizer on train dataset: 100%|█| 116654/116654 [00:03<00:00, 29941.33\n",
      "Running tokenizer on validation dataset:   0%|   | 0/888 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-2098d2201d7f61e4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-c1a5d63a97f7eade.arrow\n",
      "05/12/2024 13:37:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-2098d2201d7f61e4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-c1a5d63a97f7eade.arrow\n",
      "Running tokenizer on validation dataset: 100%|█| 888/888 [00:00<00:00, 31703.63 \n",
      "Running tokenizer on prediction dataset:   0%|  | 0/1138 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-2098d2201d7f61e4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-1629553e5e34dd9a.arrow\n",
      "05/12/2024 13:37:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-2098d2201d7f61e4/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-1629553e5e34dd9a.arrow\n",
      "Running tokenizer on prediction dataset: 100%|█| 1138/1138 [00:00<00:00, 35107.0\n",
      "[INFO|trainer.py:2078] 2024-05-12 13:37:08,559 >> ***** Running training *****\n",
      "[INFO|trainer.py:2079] 2024-05-12 13:37:08,559 >>   Num examples = 116,654\n",
      "[INFO|trainer.py:2080] 2024-05-12 13:37:08,559 >>   Num Epochs = 100\n",
      "[INFO|trainer.py:2081] 2024-05-12 13:37:08,559 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:2084] 2024-05-12 13:37:08,559 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:2085] 2024-05-12 13:37:08,559 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2086] 2024-05-12 13:37:08,559 >>   Total optimization steps = 364,600\n",
      "[INFO|trainer.py:2087] 2024-05-12 13:37:08,559 >>   Number of trainable parameters = 209,093,632\n",
      "{'loss': 6.6471, 'grad_norm': 2.6796674728393555, 'learning_rate': 4.993143170597916e-05, 'epoch': 0.14}\n",
      "{'loss': 5.8655, 'grad_norm': 2.4919307231903076, 'learning_rate': 4.986286341195831e-05, 'epoch': 0.27}\n",
      "{'loss': 5.5305, 'grad_norm': 2.4707608222961426, 'learning_rate': 4.9794295117937464e-05, 'epoch': 0.41}\n",
      "{'loss': 5.3124, 'grad_norm': 2.745155096054077, 'learning_rate': 4.972572682391663e-05, 'epoch': 0.55}\n",
      "{'loss': 5.1281, 'grad_norm': 2.7914953231811523, 'learning_rate': 4.965715852989578e-05, 'epoch': 0.69}\n",
      "{'loss': 4.9858, 'grad_norm': 2.9527249336242676, 'learning_rate': 4.958859023587493e-05, 'epoch': 0.82}\n",
      "{'loss': 4.8776, 'grad_norm': 2.5611634254455566, 'learning_rate': 4.952002194185409e-05, 'epoch': 0.96}\n",
      "  1%|▎                                 | 3646/364600 [23:55<33:50:18,  2.96it/s][INFO|trainer.py:3388] 2024-05-12 14:01:04,086 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-3646\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 14:01:04,087 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-3646/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 14:01:04,088 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-3646/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 14:01:05,564 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-3646/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 14:01:05,565 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-3646/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 14:01:05,565 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-3646/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 14:01:05,566 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-3646/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 14:01:08,415 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 14:01:08,415 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 14:01:08,416 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "{'loss': 4.7393, 'grad_norm': 3.1537859439849854, 'learning_rate': 4.9451453647833245e-05, 'epoch': 1.1}\n",
      "{'loss': 4.67, 'grad_norm': 2.9716005325317383, 'learning_rate': 4.9382885353812394e-05, 'epoch': 1.23}\n",
      "{'loss': 4.5978, 'grad_norm': 3.2810921669006348, 'learning_rate': 4.931431705979155e-05, 'epoch': 1.37}\n",
      "{'loss': 4.5642, 'grad_norm': 3.3998653888702393, 'learning_rate': 4.9245748765770713e-05, 'epoch': 1.51}\n",
      "{'loss': 4.5106, 'grad_norm': 3.440001964569092, 'learning_rate': 4.917718047174987e-05, 'epoch': 1.65}\n",
      "{'loss': 4.4556, 'grad_norm': 3.469703435897827, 'learning_rate': 4.910861217772902e-05, 'epoch': 1.78}\n",
      "{'loss': 4.418, 'grad_norm': 3.6554129123687744, 'learning_rate': 4.9040043883708175e-05, 'epoch': 1.92}\n",
      "  2%|▋                                 | 7292/364600 [48:25<25:39:51,  3.87it/s][INFO|trainer.py:3388] 2024-05-12 14:25:33,774 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-7292\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 14:25:33,775 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-7292/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 14:25:33,775 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-7292/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 14:25:35,154 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-7292/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 14:25:35,156 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-7292/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 14:25:35,156 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-7292/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 14:25:35,157 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-7292/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 14:25:40,119 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 14:25:40,122 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 14:25:40,125 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 14:25:40,190 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-3646] due to args.save_total_limit\n",
      "{'loss': 4.3516, 'grad_norm': 3.7117955684661865, 'learning_rate': 4.897147558968733e-05, 'epoch': 2.06}\n",
      "{'loss': 4.2883, 'grad_norm': 3.9816272258758545, 'learning_rate': 4.890290729566648e-05, 'epoch': 2.19}\n",
      "{'loss': 4.2377, 'grad_norm': 3.5806946754455566, 'learning_rate': 4.8834339001645644e-05, 'epoch': 2.33}\n",
      "{'loss': 4.2398, 'grad_norm': 3.8521053791046143, 'learning_rate': 4.87657707076248e-05, 'epoch': 2.47}\n",
      "{'loss': 4.2187, 'grad_norm': 3.8129312992095947, 'learning_rate': 4.8697202413603956e-05, 'epoch': 2.61}\n",
      "{'loss': 4.1825, 'grad_norm': 3.7518739700317383, 'learning_rate': 4.8628634119583105e-05, 'epoch': 2.74}\n",
      "{'loss': 4.1529, 'grad_norm': 4.074005603790283, 'learning_rate': 4.856006582556226e-05, 'epoch': 2.88}\n",
      "  3%|▉                              | 10938/364600 [1:04:04<23:11:13,  4.24it/s][INFO|trainer.py:3388] 2024-05-12 14:41:13,467 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-10938\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 14:41:13,468 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-10938/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 14:41:13,469 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-10938/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 14:41:14,774 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-10938/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 14:41:14,775 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-10938/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 14:41:14,775 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-10938/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 14:41:14,802 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-10938/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 14:41:20,222 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 14:41:20,222 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 14:41:20,231 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 14:41:20,251 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-7292] due to args.save_total_limit\n",
      "{'loss': 4.1431, 'grad_norm': 3.723891258239746, 'learning_rate': 4.849149753154142e-05, 'epoch': 3.02}\n",
      "{'loss': 4.027, 'grad_norm': 4.242587089538574, 'learning_rate': 4.842292923752057e-05, 'epoch': 3.15}\n",
      "{'loss': 4.0418, 'grad_norm': 4.178415775299072, 'learning_rate': 4.835436094349973e-05, 'epoch': 3.29}\n",
      "{'loss': 4.0158, 'grad_norm': 4.147921085357666, 'learning_rate': 4.8285792649478886e-05, 'epoch': 3.43}\n",
      "{'loss': 4.014, 'grad_norm': 4.463027000427246, 'learning_rate': 4.821722435545804e-05, 'epoch': 3.57}\n",
      "{'loss': 3.9874, 'grad_norm': 3.6492116451263428, 'learning_rate': 4.814865606143719e-05, 'epoch': 3.7}\n",
      "{'loss': 3.99, 'grad_norm': 4.560110092163086, 'learning_rate': 4.808008776741635e-05, 'epoch': 3.84}\n",
      "{'loss': 3.9618, 'grad_norm': 3.869370222091675, 'learning_rate': 4.8011519473395504e-05, 'epoch': 3.98}\n",
      "  4%|█▏                             | 14584/364600 [1:19:31<20:38:40,  4.71it/s][INFO|trainer.py:3388] 2024-05-12 14:56:40,312 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-14584\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 14:56:40,313 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-14584/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 14:56:40,313 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-14584/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 14:56:41,700 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-14584/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 14:56:41,701 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-14584/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 14:56:41,701 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-14584/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 14:56:41,702 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-14584/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 14:56:47,764 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 14:56:47,764 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 14:56:47,774 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 14:56:47,788 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-10938] due to args.save_total_limit\n",
      "{'loss': 3.8752, 'grad_norm': 4.145757675170898, 'learning_rate': 4.794295117937466e-05, 'epoch': 4.11}\n",
      "{'loss': 3.8646, 'grad_norm': 3.9991416931152344, 'learning_rate': 4.7874382885353817e-05, 'epoch': 4.25}\n",
      "{'loss': 3.8571, 'grad_norm': 4.006019115447998, 'learning_rate': 4.780581459133297e-05, 'epoch': 4.39}\n",
      "{'loss': 3.8605, 'grad_norm': 3.8029978275299072, 'learning_rate': 4.773724629731213e-05, 'epoch': 4.53}\n",
      "{'loss': 3.8272, 'grad_norm': 4.239439010620117, 'learning_rate': 4.766867800329128e-05, 'epoch': 4.66}\n",
      "{'loss': 3.8232, 'grad_norm': 4.278761863708496, 'learning_rate': 4.7600109709270434e-05, 'epoch': 4.8}\n",
      "{'loss': 3.8141, 'grad_norm': 4.346251964569092, 'learning_rate': 4.753154141524959e-05, 'epoch': 4.94}\n",
      "  5%|█▌                             | 18230/364600 [1:35:01<20:24:44,  4.71it/s][INFO|trainer.py:3388] 2024-05-12 15:12:09,908 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-18230\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 15:12:09,909 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-18230/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 15:12:09,910 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-18230/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 15:12:11,313 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-18230/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 15:12:11,315 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-18230/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 15:12:11,315 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-18230/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 15:12:11,316 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-18230/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 15:12:17,784 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 15:12:17,793 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 15:12:17,802 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 15:12:17,832 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-14584] due to args.save_total_limit\n",
      "{'loss': 3.7563, 'grad_norm': 4.909966468811035, 'learning_rate': 4.746297312122875e-05, 'epoch': 5.07}\n",
      "{'loss': 3.7147, 'grad_norm': 4.102847099304199, 'learning_rate': 4.73944048272079e-05, 'epoch': 5.21}\n",
      "{'loss': 3.6985, 'grad_norm': 4.887523174285889, 'learning_rate': 4.732583653318706e-05, 'epoch': 5.35}\n",
      "{'loss': 3.7095, 'grad_norm': 4.481743812561035, 'learning_rate': 4.7257268239166215e-05, 'epoch': 5.49}\n",
      "{'loss': 3.7015, 'grad_norm': 4.673679828643799, 'learning_rate': 4.7188699945145365e-05, 'epoch': 5.62}\n",
      "{'loss': 3.6906, 'grad_norm': 4.764498233795166, 'learning_rate': 4.712013165112452e-05, 'epoch': 5.76}\n",
      "{'loss': 3.6911, 'grad_norm': 4.535381317138672, 'learning_rate': 4.705156335710368e-05, 'epoch': 5.9}\n",
      "  6%|█▊                             | 21876/364600 [1:50:29<20:40:37,  4.60it/s][INFO|trainer.py:3388] 2024-05-12 15:27:38,443 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-21876\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 15:27:38,443 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-21876/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 15:27:38,444 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-21876/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 15:27:39,821 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-21876/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 15:27:39,822 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-21876/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 15:27:39,822 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-21876/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 15:27:39,823 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-21876/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 15:27:45,608 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 15:27:45,618 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 15:27:45,630 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 15:27:45,648 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-18230] due to args.save_total_limit\n",
      "{'loss': 3.6483, 'grad_norm': 4.882272720336914, 'learning_rate': 4.698299506308283e-05, 'epoch': 6.03}\n",
      "{'loss': 3.5739, 'grad_norm': 4.5370354652404785, 'learning_rate': 4.691442676906199e-05, 'epoch': 6.17}\n",
      "{'loss': 3.5716, 'grad_norm': 4.514719486236572, 'learning_rate': 4.6845858475041146e-05, 'epoch': 6.31}\n",
      "{'loss': 3.5767, 'grad_norm': 5.22483491897583, 'learning_rate': 4.67772901810203e-05, 'epoch': 6.45}\n",
      "{'loss': 3.5682, 'grad_norm': 4.764497756958008, 'learning_rate': 4.670872188699945e-05, 'epoch': 6.58}\n",
      "{'loss': 3.5883, 'grad_norm': 4.6897406578063965, 'learning_rate': 4.664015359297861e-05, 'epoch': 6.72}\n",
      "{'loss': 3.5627, 'grad_norm': 4.739509105682373, 'learning_rate': 4.6571585298957763e-05, 'epoch': 6.86}\n",
      "{'loss': 3.5664, 'grad_norm': 4.331806182861328, 'learning_rate': 4.650301700493692e-05, 'epoch': 6.99}\n",
      "  7%|██▏                            | 25522/364600 [2:05:56<24:21:09,  3.87it/s][INFO|trainer.py:3388] 2024-05-12 15:43:05,356 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-25522\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 15:43:05,356 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-25522/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 15:43:05,357 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-25522/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 15:43:06,695 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-25522/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 15:43:06,696 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-25522/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 15:43:06,696 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-25522/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 15:43:06,697 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-25522/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 15:43:13,573 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 15:43:13,573 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 15:43:13,574 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 15:43:13,596 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-21876] due to args.save_total_limit\n",
      "{'loss': 3.4629, 'grad_norm': 4.402791500091553, 'learning_rate': 4.6434448710916076e-05, 'epoch': 7.13}\n",
      "{'loss': 3.4411, 'grad_norm': 4.822177410125732, 'learning_rate': 4.636588041689523e-05, 'epoch': 7.27}\n",
      "{'loss': 3.4691, 'grad_norm': 4.601207733154297, 'learning_rate': 4.629731212287439e-05, 'epoch': 7.41}\n",
      "{'loss': 3.4672, 'grad_norm': 5.0039215087890625, 'learning_rate': 4.622874382885354e-05, 'epoch': 7.54}\n",
      "{'loss': 3.465, 'grad_norm': 4.404879093170166, 'learning_rate': 4.6160175534832694e-05, 'epoch': 7.68}\n",
      "{'loss': 3.4425, 'grad_norm': 4.750667095184326, 'learning_rate': 4.609160724081185e-05, 'epoch': 7.82}\n",
      "{'loss': 3.4686, 'grad_norm': 5.396721363067627, 'learning_rate': 4.6023038946791006e-05, 'epoch': 7.95}\n",
      "  8%|██▍                            | 29168/364600 [2:21:24<22:46:10,  4.09it/s][INFO|trainer.py:3388] 2024-05-12 15:58:33,530 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-29168\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 15:58:33,531 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-29168/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 15:58:33,531 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-29168/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 15:58:34,882 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-29168/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 15:58:34,883 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-29168/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 15:58:34,883 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-29168/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 15:58:34,884 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-29168/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 15:58:41,263 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 15:58:41,263 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 15:58:41,265 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 15:58:41,295 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-25522] due to args.save_total_limit\n",
      "{'loss': 3.3741, 'grad_norm': 4.806807518005371, 'learning_rate': 4.595447065277016e-05, 'epoch': 8.09}\n",
      "{'loss': 3.336, 'grad_norm': 4.791159629821777, 'learning_rate': 4.588590235874932e-05, 'epoch': 8.23}\n",
      "{'loss': 3.3489, 'grad_norm': 5.24031925201416, 'learning_rate': 4.5817334064728475e-05, 'epoch': 8.37}\n",
      "{'loss': 3.3387, 'grad_norm': 4.839347839355469, 'learning_rate': 4.5748765770707624e-05, 'epoch': 8.5}\n",
      "{'loss': 3.3739, 'grad_norm': 5.201210021972656, 'learning_rate': 4.568019747668678e-05, 'epoch': 8.64}\n",
      "{'loss': 3.3543, 'grad_norm': 4.874946117401123, 'learning_rate': 4.5611629182665936e-05, 'epoch': 8.78}\n",
      "{'loss': 3.3685, 'grad_norm': 4.564042091369629, 'learning_rate': 4.554306088864509e-05, 'epoch': 8.91}\n",
      "  9%|██▊                            | 32814/364600 [2:36:53<18:55:12,  4.87it/s][INFO|trainer.py:3388] 2024-05-12 16:14:02,289 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-32814\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 16:14:02,290 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-32814/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 16:14:02,290 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-32814/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 16:14:03,690 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-32814/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 16:14:03,692 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-32814/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 16:14:03,692 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-32814/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 16:14:03,693 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-32814/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 16:14:09,943 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 16:14:09,943 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 16:14:09,945 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 16:14:09,984 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-29168] due to args.save_total_limit\n",
      "{'loss': 3.3079, 'grad_norm': 5.104782581329346, 'learning_rate': 4.547449259462425e-05, 'epoch': 9.05}\n",
      "{'loss': 3.2292, 'grad_norm': 5.117952823638916, 'learning_rate': 4.5405924300603405e-05, 'epoch': 9.19}\n",
      "{'loss': 3.2464, 'grad_norm': 5.0324387550354, 'learning_rate': 4.533735600658256e-05, 'epoch': 9.33}\n",
      "{'loss': 3.2425, 'grad_norm': 5.019642353057861, 'learning_rate': 4.526878771256171e-05, 'epoch': 9.46}\n",
      "{'loss': 3.257, 'grad_norm': 4.830804824829102, 'learning_rate': 4.5200219418540867e-05, 'epoch': 9.6}\n",
      "{'loss': 3.256, 'grad_norm': 4.778350830078125, 'learning_rate': 4.513165112452002e-05, 'epoch': 9.74}\n",
      "{'loss': 3.2568, 'grad_norm': 5.261332988739014, 'learning_rate': 4.506308283049918e-05, 'epoch': 9.87}\n",
      " 10%|███                            | 36460/364600 [2:52:23<22:32:02,  4.05it/s][INFO|trainer.py:3388] 2024-05-12 16:29:31,805 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-36460\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 16:29:31,806 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-36460/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 16:29:31,806 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-36460/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 16:29:33,145 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-36460/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 16:29:33,146 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-36460/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 16:29:33,146 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-36460/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 16:29:33,147 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-36460/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 16:29:37,793 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 16:29:37,793 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 16:29:37,794 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 16:29:37,819 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-32814] due to args.save_total_limit\n",
      "{'loss': 3.2595, 'grad_norm': 5.060239315032959, 'learning_rate': 4.4994514536478335e-05, 'epoch': 10.01}\n",
      "{'loss': 3.1429, 'grad_norm': 4.848392009735107, 'learning_rate': 4.492594624245749e-05, 'epoch': 10.15}\n",
      "{'loss': 3.1512, 'grad_norm': 5.145500183105469, 'learning_rate': 4.485737794843665e-05, 'epoch': 10.29}\n",
      "{'loss': 3.1507, 'grad_norm': 4.9423041343688965, 'learning_rate': 4.47888096544158e-05, 'epoch': 10.42}\n",
      "{'loss': 3.1502, 'grad_norm': 4.46920108795166, 'learning_rate': 4.472024136039495e-05, 'epoch': 10.56}\n",
      "{'loss': 3.1577, 'grad_norm': 4.90908670425415, 'learning_rate': 4.465167306637411e-05, 'epoch': 10.7}\n",
      "{'loss': 3.1667, 'grad_norm': 4.850174903869629, 'learning_rate': 4.4583104772353265e-05, 'epoch': 10.83}\n",
      "{'loss': 3.1676, 'grad_norm': 4.480921268463135, 'learning_rate': 4.451453647833242e-05, 'epoch': 10.97}\n",
      " 11%|███▍                           | 40106/364600 [3:07:47<22:09:16,  4.07it/s][INFO|trainer.py:3388] 2024-05-12 16:44:56,403 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-40106\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 16:44:56,404 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-40106/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 16:44:56,404 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-40106/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 16:44:57,733 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-40106/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 16:44:57,734 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-40106/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 16:44:57,734 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-40106/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 16:44:57,735 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-40106/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 16:45:02,037 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 16:45:02,037 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 16:45:02,039 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 16:45:02,063 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-36460] due to args.save_total_limit\n",
      "{'loss': 3.0481, 'grad_norm': 4.8438801765441895, 'learning_rate': 4.444596818431158e-05, 'epoch': 11.11}\n",
      "{'loss': 3.0403, 'grad_norm': 5.078440189361572, 'learning_rate': 4.4377399890290734e-05, 'epoch': 11.25}\n",
      "{'loss': 3.0463, 'grad_norm': 4.893128395080566, 'learning_rate': 4.430883159626989e-05, 'epoch': 11.38}\n",
      "{'loss': 3.0649, 'grad_norm': 5.149147033691406, 'learning_rate': 4.424026330224904e-05, 'epoch': 11.52}\n",
      "{'loss': 3.0579, 'grad_norm': 4.947761058807373, 'learning_rate': 4.4171695008228196e-05, 'epoch': 11.66}\n",
      "{'loss': 3.0662, 'grad_norm': 5.356738567352295, 'learning_rate': 4.410312671420735e-05, 'epoch': 11.79}\n",
      "{'loss': 3.0744, 'grad_norm': 5.635279655456543, 'learning_rate': 4.403455842018651e-05, 'epoch': 11.93}\n",
      " 12%|███▋                           | 43752/364600 [3:23:09<18:31:51,  4.81it/s][INFO|trainer.py:3388] 2024-05-12 17:00:18,411 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-43752\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 17:00:18,411 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-43752/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 17:00:18,412 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-43752/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 17:00:19,726 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-43752/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 17:00:19,727 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-43752/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 17:00:19,727 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-43752/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 17:00:19,728 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-43752/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 17:00:23,942 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 17:00:23,942 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 17:00:23,944 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 17:00:23,953 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-40106] due to args.save_total_limit\n",
      "{'loss': 3.0006, 'grad_norm': 5.142524242401123, 'learning_rate': 4.3965990126165664e-05, 'epoch': 12.07}\n",
      "{'loss': 2.9331, 'grad_norm': 4.920190334320068, 'learning_rate': 4.389742183214482e-05, 'epoch': 12.21}\n",
      "{'loss': 2.9692, 'grad_norm': 5.261963367462158, 'learning_rate': 4.3828853538123976e-05, 'epoch': 12.34}\n",
      "{'loss': 2.9458, 'grad_norm': 5.450014114379883, 'learning_rate': 4.3760285244103126e-05, 'epoch': 12.48}\n",
      "{'loss': 2.9547, 'grad_norm': 5.4277520179748535, 'learning_rate': 4.369171695008228e-05, 'epoch': 12.62}\n",
      "{'loss': 2.953, 'grad_norm': 5.046356201171875, 'learning_rate': 4.362314865606144e-05, 'epoch': 12.75}\n",
      "{'loss': 2.9482, 'grad_norm': 4.98581075668335, 'learning_rate': 4.3554580362040594e-05, 'epoch': 12.89}\n",
      " 13%|████                           | 47398/364600 [3:38:25<21:36:57,  4.08it/s][INFO|trainer.py:3388] 2024-05-12 17:15:34,497 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-47398\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 17:15:34,498 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-47398/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 17:15:34,498 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-47398/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 17:15:35,783 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-47398/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 17:15:35,784 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-47398/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 17:15:35,785 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-47398/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 17:15:35,786 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-47398/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 17:15:40,116 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 17:15:40,117 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 17:15:40,120 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 17:15:40,132 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-43752] due to args.save_total_limit\n",
      "{'loss': 2.9263, 'grad_norm': 5.0181450843811035, 'learning_rate': 4.348601206801975e-05, 'epoch': 13.03}\n",
      "{'loss': 2.8199, 'grad_norm': 5.356304168701172, 'learning_rate': 4.341744377399891e-05, 'epoch': 13.17}\n",
      "{'loss': 2.8353, 'grad_norm': 5.0527825355529785, 'learning_rate': 4.334887547997806e-05, 'epoch': 13.3}\n",
      "{'loss': 2.8235, 'grad_norm': 5.287441253662109, 'learning_rate': 4.328030718595721e-05, 'epoch': 13.44}\n",
      "{'loss': 2.8426, 'grad_norm': 5.292849540710449, 'learning_rate': 4.321173889193637e-05, 'epoch': 13.58}\n",
      "{'loss': 2.8293, 'grad_norm': 5.380087852478027, 'learning_rate': 4.3143170597915525e-05, 'epoch': 13.71}\n",
      "{'loss': 2.8535, 'grad_norm': 5.534645080566406, 'learning_rate': 4.307460230389468e-05, 'epoch': 13.85}\n",
      "{'loss': 2.8432, 'grad_norm': 5.294557571411133, 'learning_rate': 4.300603400987384e-05, 'epoch': 13.99}\n",
      " 14%|████▎                          | 51044/364600 [3:53:36<20:48:33,  4.19it/s][INFO|trainer.py:3388] 2024-05-12 17:30:45,552 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-51044\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 17:30:45,553 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-51044/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 17:30:45,553 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-51044/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 17:30:46,835 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-51044/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 17:30:46,836 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-51044/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 17:30:46,836 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-51044/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 17:30:46,837 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-51044/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 17:30:50,984 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 17:30:50,984 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 17:30:50,986 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 17:30:50,995 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-47398] due to args.save_total_limit\n",
      "{'loss': 2.7162, 'grad_norm': 5.039003849029541, 'learning_rate': 4.293746571585299e-05, 'epoch': 14.13}\n",
      "{'loss': 2.712, 'grad_norm': 5.455623149871826, 'learning_rate': 4.286889742183215e-05, 'epoch': 14.26}\n",
      "{'loss': 2.7246, 'grad_norm': 5.256813049316406, 'learning_rate': 4.28003291278113e-05, 'epoch': 14.4}\n",
      "{'loss': 2.7471, 'grad_norm': 5.521039962768555, 'learning_rate': 4.2731760833790455e-05, 'epoch': 14.54}\n",
      "{'loss': 2.7177, 'grad_norm': 5.75991153717041, 'learning_rate': 4.266319253976961e-05, 'epoch': 14.67}\n",
      "{'loss': 2.7111, 'grad_norm': 4.9295759201049805, 'learning_rate': 4.259462424574877e-05, 'epoch': 14.81}\n",
      "{'loss': 2.7263, 'grad_norm': 4.961513042449951, 'learning_rate': 4.252605595172792e-05, 'epoch': 14.95}\n",
      " 15%|████▋                          | 54690/364600 [4:08:42<20:51:32,  4.13it/s][INFO|trainer.py:3388] 2024-05-12 17:45:50,793 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-54690\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 17:45:50,794 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-54690/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 17:45:50,795 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-54690/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 17:45:52,139 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-54690/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 17:45:52,140 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-54690/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 17:45:52,140 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-54690/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 17:45:52,141 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-54690/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 17:45:56,491 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 17:45:56,491 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 17:45:56,493 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 17:45:56,505 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-51044] due to args.save_total_limit\n",
      "{'loss': 2.6599, 'grad_norm': 4.933211803436279, 'learning_rate': 4.245748765770708e-05, 'epoch': 15.09}\n",
      "{'loss': 2.6078, 'grad_norm': 5.510207176208496, 'learning_rate': 4.2388919363686236e-05, 'epoch': 15.22}\n",
      "{'loss': 2.6238, 'grad_norm': 5.186633110046387, 'learning_rate': 4.2320351069665385e-05, 'epoch': 15.36}\n",
      "{'loss': 2.6189, 'grad_norm': 5.6987690925598145, 'learning_rate': 4.225178277564454e-05, 'epoch': 15.5}\n",
      "{'loss': 2.6261, 'grad_norm': 5.060766696929932, 'learning_rate': 4.21832144816237e-05, 'epoch': 15.63}\n",
      "{'loss': 2.6096, 'grad_norm': 5.581600666046143, 'learning_rate': 4.2114646187602854e-05, 'epoch': 15.77}\n",
      "{'loss': 2.6243, 'grad_norm': 5.272013187408447, 'learning_rate': 4.204607789358201e-05, 'epoch': 15.91}\n",
      " 16%|████▉                          | 58336/364600 [4:23:54<17:51:11,  4.77it/s][INFO|trainer.py:3388] 2024-05-12 18:01:02,868 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-58336\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 18:01:02,869 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-58336/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 18:01:02,870 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-58336/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 18:01:04,166 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-58336/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 18:01:04,167 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-58336/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 18:01:04,167 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-58336/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 18:01:04,168 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-58336/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 18:01:08,469 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 18:01:08,469 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 18:01:08,471 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 18:01:08,479 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-54690] due to args.save_total_limit\n",
      "{'loss': 2.5654, 'grad_norm': 5.0031538009643555, 'learning_rate': 4.1977509599561166e-05, 'epoch': 16.04}\n",
      "{'loss': 2.4769, 'grad_norm': 5.4185872077941895, 'learning_rate': 4.190894130554032e-05, 'epoch': 16.18}\n",
      "{'loss': 2.4867, 'grad_norm': 5.633464336395264, 'learning_rate': 4.184037301151947e-05, 'epoch': 16.32}\n",
      "{'loss': 2.5209, 'grad_norm': 5.207147598266602, 'learning_rate': 4.177180471749863e-05, 'epoch': 16.46}\n",
      "{'loss': 2.5095, 'grad_norm': 5.337882995605469, 'learning_rate': 4.170323642347779e-05, 'epoch': 16.59}\n",
      "{'loss': 2.5256, 'grad_norm': 5.710779666900635, 'learning_rate': 4.163466812945694e-05, 'epoch': 16.73}\n",
      "{'loss': 2.5221, 'grad_norm': 4.833573818206787, 'learning_rate': 4.1566099835436096e-05, 'epoch': 16.87}\n",
      " 17%|█████▎                         | 61982/364600 [4:38:58<18:40:27,  4.50it/s][INFO|trainer.py:3388] 2024-05-12 18:16:07,099 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-61982\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 18:16:07,100 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-61982/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 18:16:07,100 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-61982/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 18:16:08,423 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-61982/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 18:16:08,424 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-61982/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 18:16:08,425 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-61982/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 18:16:08,426 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-61982/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 18:16:12,707 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 18:16:12,708 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 18:16:12,710 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 18:16:12,721 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-58336] due to args.save_total_limit\n",
      "{'loss': 2.5274, 'grad_norm': 4.590396404266357, 'learning_rate': 4.149753154141525e-05, 'epoch': 17.0}\n",
      "{'loss': 2.3651, 'grad_norm': 5.467580318450928, 'learning_rate': 4.142896324739441e-05, 'epoch': 17.14}\n",
      "{'loss': 2.3904, 'grad_norm': 5.374948024749756, 'learning_rate': 4.136039495337356e-05, 'epoch': 17.28}\n",
      "{'loss': 2.4162, 'grad_norm': 5.345193386077881, 'learning_rate': 4.1291826659352714e-05, 'epoch': 17.42}\n",
      "{'loss': 2.4228, 'grad_norm': 5.317601680755615, 'learning_rate': 4.122325836533188e-05, 'epoch': 17.55}\n",
      "{'loss': 2.398, 'grad_norm': 5.649726390838623, 'learning_rate': 4.1154690071311026e-05, 'epoch': 17.69}\n",
      "{'loss': 2.4126, 'grad_norm': 4.870903015136719, 'learning_rate': 4.108612177729018e-05, 'epoch': 17.83}\n",
      "{'loss': 2.4315, 'grad_norm': 5.537862300872803, 'learning_rate': 4.101755348326934e-05, 'epoch': 17.96}\n",
      " 18%|█████▌                         | 65628/364600 [4:53:59<23:04:38,  3.60it/s][INFO|trainer.py:3388] 2024-05-12 18:31:08,282 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-65628\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 18:31:08,282 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-65628/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 18:31:08,283 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-65628/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 18:31:09,578 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-65628/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 18:31:09,582 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-65628/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 18:31:09,582 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-65628/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 18:31:09,583 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-65628/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 18:31:13,915 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 18:31:13,915 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 18:31:13,917 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 18:31:13,926 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-61982] due to args.save_total_limit\n",
      "{'loss': 2.3059, 'grad_norm': 5.414814472198486, 'learning_rate': 4.0948985189248495e-05, 'epoch': 18.1}\n",
      "{'loss': 2.2847, 'grad_norm': 5.167638301849365, 'learning_rate': 4.0880416895227644e-05, 'epoch': 18.24}\n",
      "{'loss': 2.2914, 'grad_norm': 5.151243209838867, 'learning_rate': 4.08118486012068e-05, 'epoch': 18.38}\n",
      "{'loss': 2.3047, 'grad_norm': 5.785707473754883, 'learning_rate': 4.0743280307185963e-05, 'epoch': 18.51}\n",
      "{'loss': 2.3021, 'grad_norm': 4.904608249664307, 'learning_rate': 4.067471201316512e-05, 'epoch': 18.65}\n",
      "{'loss': 2.3305, 'grad_norm': 5.454782009124756, 'learning_rate': 4.060614371914427e-05, 'epoch': 18.79}\n",
      "{'loss': 2.3182, 'grad_norm': 5.2010650634765625, 'learning_rate': 4.0537575425123425e-05, 'epoch': 18.92}\n",
      " 19%|█████▉                         | 69274/364600 [5:09:00<15:54:37,  5.16it/s][INFO|trainer.py:3388] 2024-05-12 18:46:09,289 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-69274\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 18:46:09,289 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-69274/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 18:46:09,290 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-69274/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 18:46:10,633 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-69274/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 18:46:10,634 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-69274/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 18:46:10,635 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-69274/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 18:46:10,635 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-69274/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 18:46:14,961 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 18:46:14,981 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 18:46:14,982 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 18:46:14,988 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-65628] due to args.save_total_limit\n",
      "{'loss': 2.2601, 'grad_norm': 5.094666481018066, 'learning_rate': 4.046900713110258e-05, 'epoch': 19.06}\n",
      "{'loss': 2.1853, 'grad_norm': 5.217191696166992, 'learning_rate': 4.040043883708173e-05, 'epoch': 19.2}\n",
      "{'loss': 2.1981, 'grad_norm': 5.011998653411865, 'learning_rate': 4.033187054306089e-05, 'epoch': 19.34}\n",
      "{'loss': 2.2176, 'grad_norm': 5.134762287139893, 'learning_rate': 4.026330224904005e-05, 'epoch': 19.47}\n",
      "{'loss': 2.2141, 'grad_norm': 5.362982273101807, 'learning_rate': 4.0194733955019206e-05, 'epoch': 19.61}\n",
      "{'loss': 2.2004, 'grad_norm': 5.136562347412109, 'learning_rate': 4.0126165660998355e-05, 'epoch': 19.75}\n",
      "{'loss': 2.2177, 'grad_norm': 5.2206220626831055, 'learning_rate': 4.005759736697751e-05, 'epoch': 19.88}\n",
      " 20%|██████▏                        | 72920/364600 [5:23:59<17:04:18,  4.75it/s][INFO|trainer.py:3388] 2024-05-12 19:01:08,310 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-72920\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 19:01:08,311 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-72920/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 19:01:08,311 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-72920/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 19:01:09,559 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-72920/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 19:01:09,561 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-72920/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 19:01:09,561 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-72920/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 19:01:09,562 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-72920/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 19:01:13,864 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 19:01:13,864 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 19:01:13,866 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 19:01:13,875 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-69274] due to args.save_total_limit\n",
      "{'loss': 2.1987, 'grad_norm': 5.294692516326904, 'learning_rate': 3.998902907295667e-05, 'epoch': 20.02}\n",
      "{'loss': 2.0699, 'grad_norm': 5.572756767272949, 'learning_rate': 3.992046077893582e-05, 'epoch': 20.16}\n",
      "{'loss': 2.0924, 'grad_norm': 5.801488876342773, 'learning_rate': 3.985189248491497e-05, 'epoch': 20.3}\n",
      "{'loss': 2.114, 'grad_norm': 5.149176120758057, 'learning_rate': 3.9783324190894136e-05, 'epoch': 20.43}\n",
      "{'loss': 2.1022, 'grad_norm': 5.202007293701172, 'learning_rate': 3.971475589687329e-05, 'epoch': 20.57}\n",
      "{'loss': 2.1327, 'grad_norm': 4.496254920959473, 'learning_rate': 3.964618760285244e-05, 'epoch': 20.71}\n",
      "{'loss': 2.1265, 'grad_norm': 5.123493194580078, 'learning_rate': 3.95776193088316e-05, 'epoch': 20.84}\n",
      "{'loss': 2.1341, 'grad_norm': 5.082859516143799, 'learning_rate': 3.9509051014810754e-05, 'epoch': 20.98}\n",
      " 21%|██████▌                        | 76566/364600 [5:39:00<19:57:15,  4.01it/s][INFO|trainer.py:3388] 2024-05-12 19:16:09,322 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-76566\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 19:16:09,323 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-76566/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 19:16:09,323 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-76566/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 19:16:10,667 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-76566/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 19:16:10,668 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-76566/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 19:16:10,669 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-76566/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 19:16:10,670 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-76566/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 19:16:15,058 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 19:16:15,059 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 19:16:15,061 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 19:16:15,069 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-72920] due to args.save_total_limit\n",
      "{'loss': 2.0016, 'grad_norm': 4.651580810546875, 'learning_rate': 3.9440482720789904e-05, 'epoch': 21.12}\n",
      "{'loss': 2.0007, 'grad_norm': 5.409528732299805, 'learning_rate': 3.9371914426769066e-05, 'epoch': 21.26}\n",
      "{'loss': 2.0057, 'grad_norm': 5.502586841583252, 'learning_rate': 3.930334613274822e-05, 'epoch': 21.39}\n",
      "{'loss': 2.0159, 'grad_norm': 5.030213356018066, 'learning_rate': 3.923477783872738e-05, 'epoch': 21.53}\n",
      "{'loss': 2.0237, 'grad_norm': 4.999740123748779, 'learning_rate': 3.916620954470653e-05, 'epoch': 21.67}\n",
      "{'loss': 2.0275, 'grad_norm': 5.182149887084961, 'learning_rate': 3.9097641250685684e-05, 'epoch': 21.8}\n",
      "{'loss': 2.0473, 'grad_norm': 5.282116889953613, 'learning_rate': 3.902907295666484e-05, 'epoch': 21.94}\n",
      " 22%|██████▊                        | 80212/364600 [5:53:59<17:08:51,  4.61it/s][INFO|trainer.py:3388] 2024-05-12 19:31:08,338 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-80212\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 19:31:08,339 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-80212/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 19:31:08,339 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-80212/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 19:31:09,642 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-80212/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 19:31:09,643 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-80212/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 19:31:09,643 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-80212/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 19:31:09,644 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-80212/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 19:31:13,880 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 19:31:13,881 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 19:31:13,882 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 19:31:13,906 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-76566] due to args.save_total_limit\n",
      "{'loss': 1.9621, 'grad_norm': 4.748703956604004, 'learning_rate': 3.896050466264399e-05, 'epoch': 22.08}\n",
      "{'loss': 1.9008, 'grad_norm': 4.832570552825928, 'learning_rate': 3.889193636862315e-05, 'epoch': 22.22}\n",
      "{'loss': 1.9096, 'grad_norm': 5.199923992156982, 'learning_rate': 3.882336807460231e-05, 'epoch': 22.35}\n",
      "{'loss': 1.9247, 'grad_norm': 5.1267499923706055, 'learning_rate': 3.8754799780581465e-05, 'epoch': 22.49}\n",
      "{'loss': 1.9216, 'grad_norm': 4.7476606369018555, 'learning_rate': 3.8686231486560615e-05, 'epoch': 22.63}\n",
      "{'loss': 1.9493, 'grad_norm': 5.416210174560547, 'learning_rate': 3.861766319253977e-05, 'epoch': 22.76}\n",
      "{'loss': 1.9391, 'grad_norm': 5.211349010467529, 'learning_rate': 3.854909489851893e-05, 'epoch': 22.9}\n",
      " 23%|███████▏                       | 83858/364600 [6:08:55<17:01:57,  4.58it/s][INFO|trainer.py:3388] 2024-05-12 19:46:04,159 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-83858\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 19:46:04,160 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-83858/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 19:46:04,160 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-83858/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 19:46:05,504 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-83858/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 19:46:05,506 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-83858/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 19:46:05,506 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-83858/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 19:46:05,507 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-83858/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 19:46:09,881 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 19:46:09,881 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 19:46:09,883 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 19:46:09,892 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-80212] due to args.save_total_limit\n",
      "{'loss': 1.9047, 'grad_norm': 5.296257495880127, 'learning_rate': 3.8480526604498076e-05, 'epoch': 23.04}\n",
      "{'loss': 1.803, 'grad_norm': 5.259824752807617, 'learning_rate': 3.841195831047724e-05, 'epoch': 23.18}\n",
      "{'loss': 1.8172, 'grad_norm': 4.756730079650879, 'learning_rate': 3.8343390016456395e-05, 'epoch': 23.31}\n",
      "{'loss': 1.8314, 'grad_norm': 5.009732723236084, 'learning_rate': 3.827482172243555e-05, 'epoch': 23.45}\n",
      "{'loss': 1.8489, 'grad_norm': 5.3414082527160645, 'learning_rate': 3.82062534284147e-05, 'epoch': 23.59}\n",
      "{'loss': 1.8542, 'grad_norm': 4.76619815826416, 'learning_rate': 3.813768513439386e-05, 'epoch': 23.72}\n",
      "{'loss': 1.8639, 'grad_norm': 5.249925136566162, 'learning_rate': 3.806911684037301e-05, 'epoch': 23.86}\n",
      "{'loss': 1.8802, 'grad_norm': 4.97225284576416, 'learning_rate': 3.800054854635216e-05, 'epoch': 24.0}\n",
      " 24%|███████▍                       | 87504/364600 [6:23:55<17:29:52,  4.40it/s][INFO|trainer.py:3388] 2024-05-12 20:01:03,895 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-87504\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 20:01:03,896 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-87504/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 20:01:03,896 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-87504/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 20:01:05,296 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-87504/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 20:01:05,297 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-87504/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 20:01:05,298 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-87504/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 20:01:05,299 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-87504/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 20:01:09,852 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 20:01:09,853 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 20:01:09,855 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 20:01:09,863 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-83858] due to args.save_total_limit\n",
      "{'loss': 1.7144, 'grad_norm': 5.291701793670654, 'learning_rate': 3.7931980252331326e-05, 'epoch': 24.14}\n",
      "{'loss': 1.7312, 'grad_norm': 5.1743340492248535, 'learning_rate': 3.786341195831048e-05, 'epoch': 24.27}\n",
      "{'loss': 1.7488, 'grad_norm': 5.3917646408081055, 'learning_rate': 3.779484366428964e-05, 'epoch': 24.41}\n",
      "{'loss': 1.7637, 'grad_norm': 4.806937217712402, 'learning_rate': 3.772627537026879e-05, 'epoch': 24.55}\n",
      "{'loss': 1.7668, 'grad_norm': 5.0730156898498535, 'learning_rate': 3.7657707076247944e-05, 'epoch': 24.68}\n",
      "{'loss': 1.7769, 'grad_norm': 4.786214828491211, 'learning_rate': 3.75891387822271e-05, 'epoch': 24.82}\n",
      "{'loss': 1.7903, 'grad_norm': 5.39318323135376, 'learning_rate': 3.752057048820625e-05, 'epoch': 24.96}\n",
      " 25%|███████▊                       | 91150/364600 [6:38:54<16:31:10,  4.60it/s][INFO|trainer.py:3388] 2024-05-12 20:16:03,072 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-91150\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 20:16:03,073 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-91150/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 20:16:03,073 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-91150/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 20:16:04,426 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-91150/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 20:16:04,427 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-91150/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 20:16:04,427 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-91150/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 20:16:04,428 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-91150/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 20:16:09,017 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 20:16:09,017 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 20:16:09,019 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 20:16:09,067 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-87504] due to args.save_total_limit\n",
      "{'loss': 1.6718, 'grad_norm': 4.981703281402588, 'learning_rate': 3.745200219418541e-05, 'epoch': 25.1}\n",
      "{'loss': 1.6542, 'grad_norm': 4.901900291442871, 'learning_rate': 3.738343390016457e-05, 'epoch': 25.23}\n",
      "{'loss': 1.6573, 'grad_norm': 5.158128261566162, 'learning_rate': 3.7314865606143724e-05, 'epoch': 25.37}\n",
      "{'loss': 1.6773, 'grad_norm': 4.649386882781982, 'learning_rate': 3.7246297312122874e-05, 'epoch': 25.51}\n",
      "{'loss': 1.687, 'grad_norm': 4.9402666091918945, 'learning_rate': 3.717772901810203e-05, 'epoch': 25.64}\n",
      "{'loss': 1.701, 'grad_norm': 5.1116180419921875, 'learning_rate': 3.7109160724081186e-05, 'epoch': 25.78}\n",
      "{'loss': 1.7042, 'grad_norm': 5.389803886413574, 'learning_rate': 3.704059243006034e-05, 'epoch': 25.92}\n",
      " 26%|████████                       | 94796/364600 [6:53:54<14:40:07,  5.11it/s][INFO|trainer.py:3388] 2024-05-12 20:31:02,773 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-94796\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 20:31:02,774 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-94796/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 20:31:02,774 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-94796/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 20:31:04,127 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-94796/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 20:31:04,128 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-94796/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 20:31:04,129 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-94796/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 20:31:04,130 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-94796/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 20:31:08,849 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 20:31:08,849 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 20:31:08,851 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 20:31:08,860 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-91150] due to args.save_total_limit\n",
      "{'loss': 1.637, 'grad_norm': 5.371042251586914, 'learning_rate': 3.69720241360395e-05, 'epoch': 26.06}\n",
      "{'loss': 1.5579, 'grad_norm': 5.292448997497559, 'learning_rate': 3.6903455842018655e-05, 'epoch': 26.19}\n",
      "{'loss': 1.5781, 'grad_norm': 5.034709453582764, 'learning_rate': 3.683488754799781e-05, 'epoch': 26.33}\n",
      "{'loss': 1.6006, 'grad_norm': 4.979785919189453, 'learning_rate': 3.676631925397696e-05, 'epoch': 26.47}\n",
      "{'loss': 1.6032, 'grad_norm': 4.940494537353516, 'learning_rate': 3.6697750959956116e-05, 'epoch': 26.6}\n",
      "{'loss': 1.6248, 'grad_norm': 5.339479923248291, 'learning_rate': 3.662918266593527e-05, 'epoch': 26.74}\n",
      "{'loss': 1.6189, 'grad_norm': 5.139049530029297, 'learning_rate': 3.656061437191443e-05, 'epoch': 26.88}\n",
      " 27%|████████▎                      | 98442/364600 [7:08:51<15:10:25,  4.87it/s][INFO|trainer.py:3388] 2024-05-12 20:45:59,923 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-98442\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 20:45:59,923 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-98442/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 20:45:59,924 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-98442/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 20:46:01,190 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-98442/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 20:46:01,191 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-98442/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 20:46:01,191 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-98442/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 20:46:01,192 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-98442/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 20:46:05,859 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 20:46:05,859 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 20:46:05,861 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 20:46:05,870 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-94796] due to args.save_total_limit\n",
      "{'loss': 1.6215, 'grad_norm': 4.733531951904297, 'learning_rate': 3.6492046077893585e-05, 'epoch': 27.02}\n",
      "{'loss': 1.4842, 'grad_norm': 5.294017791748047, 'learning_rate': 3.642347778387274e-05, 'epoch': 27.15}\n",
      "{'loss': 1.5071, 'grad_norm': 5.071205139160156, 'learning_rate': 3.63549094898519e-05, 'epoch': 27.29}\n",
      "{'loss': 1.5179, 'grad_norm': 5.08548641204834, 'learning_rate': 3.628634119583105e-05, 'epoch': 27.43}\n",
      "{'loss': 1.5282, 'grad_norm': 5.183330059051514, 'learning_rate': 3.62177729018102e-05, 'epoch': 27.56}\n",
      "{'loss': 1.5419, 'grad_norm': 4.851142406463623, 'learning_rate': 3.614920460778936e-05, 'epoch': 27.7}\n",
      "{'loss': 1.5537, 'grad_norm': 4.878331661224365, 'learning_rate': 3.6080636313768515e-05, 'epoch': 27.84}\n",
      "{'loss': 1.5562, 'grad_norm': 5.406539440155029, 'learning_rate': 3.601206801974767e-05, 'epoch': 27.98}\n",
      " 28%|████████▍                     | 102088/364600 [7:23:50<15:29:42,  4.71it/s][INFO|trainer.py:3388] 2024-05-12 21:00:59,541 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-102088\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 21:00:59,542 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-102088/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 21:00:59,542 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-102088/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 21:01:00,912 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-102088/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 21:01:00,913 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-102088/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 21:01:00,913 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-102088/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 21:01:00,914 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-102088/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 21:01:05,578 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 21:01:05,578 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 21:01:05,580 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 21:01:05,589 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-98442] due to args.save_total_limit\n",
      "{'loss': 1.4368, 'grad_norm': 5.543664455413818, 'learning_rate': 3.594349972572683e-05, 'epoch': 28.11}\n",
      "{'loss': 1.4252, 'grad_norm': 5.570579528808594, 'learning_rate': 3.5874931431705984e-05, 'epoch': 28.25}\n",
      "{'loss': 1.4427, 'grad_norm': 4.777440547943115, 'learning_rate': 3.580636313768513e-05, 'epoch': 28.39}\n",
      "{'loss': 1.4574, 'grad_norm': 4.820840835571289, 'learning_rate': 3.573779484366429e-05, 'epoch': 28.52}\n",
      "{'loss': 1.4651, 'grad_norm': 4.499929904937744, 'learning_rate': 3.5669226549643445e-05, 'epoch': 28.66}\n",
      "{'loss': 1.4721, 'grad_norm': 4.876035213470459, 'learning_rate': 3.56006582556226e-05, 'epoch': 28.8}\n",
      "{'loss': 1.4854, 'grad_norm': 5.974823951721191, 'learning_rate': 3.553208996160176e-05, 'epoch': 28.94}\n",
      " 29%|████████▋                     | 105734/364600 [7:38:52<13:54:30,  5.17it/s][INFO|trainer.py:3388] 2024-05-12 21:16:00,866 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-105734\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 21:16:00,867 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-105734/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 21:16:00,867 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-105734/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 21:16:02,186 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-105734/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 21:16:02,187 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-105734/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 21:16:02,187 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-105734/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 21:16:02,188 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-105734/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 21:16:06,758 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 21:16:06,758 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 21:16:06,760 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 21:16:06,769 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-102088] due to args.save_total_limit\n",
      "{'loss': 1.4078, 'grad_norm': 5.119105815887451, 'learning_rate': 3.5463521667580914e-05, 'epoch': 29.07}\n",
      "{'loss': 1.3521, 'grad_norm': 4.832869052886963, 'learning_rate': 3.539495337356007e-05, 'epoch': 29.21}\n",
      "{'loss': 1.3673, 'grad_norm': 5.020029544830322, 'learning_rate': 3.532638507953922e-05, 'epoch': 29.35}\n",
      "{'loss': 1.3926, 'grad_norm': 5.573171615600586, 'learning_rate': 3.5257816785518376e-05, 'epoch': 29.48}\n",
      "{'loss': 1.4032, 'grad_norm': 5.574306488037109, 'learning_rate': 3.518924849149753e-05, 'epoch': 29.62}\n",
      "{'loss': 1.4039, 'grad_norm': 5.316165924072266, 'learning_rate': 3.512068019747669e-05, 'epoch': 29.76}\n",
      "{'loss': 1.4099, 'grad_norm': 5.210799217224121, 'learning_rate': 3.5052111903455844e-05, 'epoch': 29.9}\n",
      " 30%|█████████                     | 109380/364600 [7:53:48<16:43:55,  4.24it/s][INFO|trainer.py:3388] 2024-05-12 21:30:57,250 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-109380\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 21:30:57,251 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-109380/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 21:30:57,251 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-109380/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 21:30:58,625 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-109380/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 21:30:58,626 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-109380/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 21:30:58,627 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-109380/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 21:30:58,628 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-109380/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 21:31:03,249 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 21:31:03,249 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 21:31:03,251 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 21:31:03,260 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-105734] due to args.save_total_limit\n",
      "{'loss': 1.3881, 'grad_norm': 4.973813056945801, 'learning_rate': 3.4983543609435e-05, 'epoch': 30.03}\n",
      "{'loss': 1.2853, 'grad_norm': 4.447306156158447, 'learning_rate': 3.4914975315414157e-05, 'epoch': 30.17}\n",
      "{'loss': 1.2992, 'grad_norm': 5.200187683105469, 'learning_rate': 3.4846407021393306e-05, 'epoch': 30.31}\n",
      "{'loss': 1.3248, 'grad_norm': 5.067360877990723, 'learning_rate': 3.477783872737246e-05, 'epoch': 30.44}\n",
      "{'loss': 1.326, 'grad_norm': 5.2174391746521, 'learning_rate': 3.470927043335162e-05, 'epoch': 30.58}\n",
      "{'loss': 1.3509, 'grad_norm': 5.6121392250061035, 'learning_rate': 3.4640702139330774e-05, 'epoch': 30.72}\n",
      "{'loss': 1.3437, 'grad_norm': 5.090517997741699, 'learning_rate': 3.457213384530993e-05, 'epoch': 30.86}\n",
      "{'loss': 1.3577, 'grad_norm': 4.977377414703369, 'learning_rate': 3.450356555128909e-05, 'epoch': 30.99}\n",
      " 31%|█████████▎                    | 113026/364600 [8:08:47<18:02:57,  3.87it/s][INFO|trainer.py:3388] 2024-05-12 21:45:55,810 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-113026\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 21:45:55,810 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-113026/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 21:45:55,811 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-113026/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 21:45:57,139 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-113026/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 21:45:57,140 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-113026/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 21:45:57,140 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-113026/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 21:45:57,141 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-113026/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 21:46:01,715 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 21:46:01,715 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 21:46:01,718 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 21:46:01,729 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-109380] due to args.save_total_limit\n",
      "{'loss': 1.2348, 'grad_norm': 5.1490478515625, 'learning_rate': 3.443499725726824e-05, 'epoch': 31.13}\n",
      "{'loss': 1.2272, 'grad_norm': 4.903263092041016, 'learning_rate': 3.436642896324739e-05, 'epoch': 31.27}\n",
      "{'loss': 1.2484, 'grad_norm': 5.068541049957275, 'learning_rate': 3.429786066922655e-05, 'epoch': 31.4}\n",
      "{'loss': 1.2704, 'grad_norm': 5.064205169677734, 'learning_rate': 3.4229292375205705e-05, 'epoch': 31.54}\n",
      "{'loss': 1.2753, 'grad_norm': 5.463748455047607, 'learning_rate': 3.416072408118486e-05, 'epoch': 31.68}\n",
      "{'loss': 1.2875, 'grad_norm': 4.637465476989746, 'learning_rate': 3.409215578716402e-05, 'epoch': 31.82}\n",
      "{'loss': 1.2919, 'grad_norm': 4.767406463623047, 'learning_rate': 3.402358749314317e-05, 'epoch': 31.95}\n",
      " 32%|█████████▌                    | 116672/364600 [8:23:47<16:05:04,  4.28it/s][INFO|trainer.py:3388] 2024-05-12 22:00:55,993 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-116672\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 22:00:55,994 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-116672/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 22:00:55,994 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-116672/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 22:00:57,282 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-116672/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 22:00:57,284 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-116672/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 22:00:57,284 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-116672/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 22:00:57,285 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-116672/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 22:01:01,960 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 22:01:01,960 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 22:01:01,962 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 22:01:01,971 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-113026] due to args.save_total_limit\n",
      "{'loss': 1.2129, 'grad_norm': 4.907227993011475, 'learning_rate': 3.395501919912233e-05, 'epoch': 32.09}\n",
      "{'loss': 1.1743, 'grad_norm': 4.724886417388916, 'learning_rate': 3.388645090510148e-05, 'epoch': 32.23}\n",
      "{'loss': 1.1865, 'grad_norm': 5.002569198608398, 'learning_rate': 3.3817882611080635e-05, 'epoch': 32.36}\n",
      "{'loss': 1.2088, 'grad_norm': 4.655109405517578, 'learning_rate': 3.374931431705979e-05, 'epoch': 32.5}\n",
      "{'loss': 1.2133, 'grad_norm': 5.301872730255127, 'learning_rate': 3.368074602303895e-05, 'epoch': 32.64}\n",
      "{'loss': 1.224, 'grad_norm': 5.0290846824646, 'learning_rate': 3.3612177729018103e-05, 'epoch': 32.78}\n",
      "{'loss': 1.2292, 'grad_norm': 5.073773384094238, 'learning_rate': 3.354360943499726e-05, 'epoch': 32.91}\n",
      " 33%|█████████▉                    | 120318/364600 [8:38:45<16:50:21,  4.03it/s][INFO|trainer.py:3388] 2024-05-12 22:15:54,202 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-120318\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 22:15:54,203 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-120318/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 22:15:54,203 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-120318/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 22:15:55,555 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-120318/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 22:15:55,557 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-120318/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 22:15:55,557 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-120318/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 22:15:55,558 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-120318/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 22:16:00,207 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 22:16:00,207 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 22:16:00,209 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 22:16:00,219 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-116672] due to args.save_total_limit\n",
      "{'loss': 1.1887, 'grad_norm': 5.129011631011963, 'learning_rate': 3.3475041140976416e-05, 'epoch': 33.05}\n",
      "{'loss': 1.1145, 'grad_norm': 5.234120845794678, 'learning_rate': 3.340647284695557e-05, 'epoch': 33.19}\n",
      "{'loss': 1.1246, 'grad_norm': 5.61316442489624, 'learning_rate': 3.333790455293472e-05, 'epoch': 33.32}\n",
      "{'loss': 1.1392, 'grad_norm': 5.373575210571289, 'learning_rate': 3.326933625891388e-05, 'epoch': 33.46}\n",
      "{'loss': 1.1565, 'grad_norm': 5.573062419891357, 'learning_rate': 3.3200767964893034e-05, 'epoch': 33.6}\n",
      "{'loss': 1.1566, 'grad_norm': 5.016828536987305, 'learning_rate': 3.313219967087219e-05, 'epoch': 33.74}\n",
      "{'loss': 1.1775, 'grad_norm': 5.493660926818848, 'learning_rate': 3.3063631376851346e-05, 'epoch': 33.87}\n",
      " 34%|██████████▏                   | 123964/364600 [8:53:45<16:33:55,  4.04it/s][INFO|trainer.py:3388] 2024-05-12 22:30:54,466 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-123964\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 22:30:54,467 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-123964/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 22:30:54,467 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-123964/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 22:30:55,866 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-123964/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 22:30:55,868 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-123964/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 22:30:55,868 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-123964/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 22:30:55,869 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-123964/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 22:31:00,513 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 22:31:00,514 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 22:31:00,515 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 22:31:00,520 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-120318] due to args.save_total_limit\n",
      "{'loss': 1.182, 'grad_norm': 4.961755275726318, 'learning_rate': 3.29950630828305e-05, 'epoch': 34.01}\n",
      "{'loss': 1.0434, 'grad_norm': 5.307010173797607, 'learning_rate': 3.292649478880966e-05, 'epoch': 34.15}\n",
      "{'loss': 1.0732, 'grad_norm': 5.011436462402344, 'learning_rate': 3.285792649478881e-05, 'epoch': 34.28}\n",
      "{'loss': 1.0886, 'grad_norm': 5.172646522521973, 'learning_rate': 3.2789358200767964e-05, 'epoch': 34.42}\n",
      "{'loss': 1.0978, 'grad_norm': 5.302252769470215, 'learning_rate': 3.272078990674713e-05, 'epoch': 34.56}\n",
      "{'loss': 1.1098, 'grad_norm': 5.635678768157959, 'learning_rate': 3.2652221612726276e-05, 'epoch': 34.7}\n",
      "{'loss': 1.1163, 'grad_norm': 5.217731475830078, 'learning_rate': 3.258365331870543e-05, 'epoch': 34.83}\n",
      "{'loss': 1.1212, 'grad_norm': 5.012636661529541, 'learning_rate': 3.251508502468459e-05, 'epoch': 34.97}\n",
      " 35%|██████████▌                   | 127610/364600 [9:08:45<16:22:40,  4.02it/s][INFO|trainer.py:3388] 2024-05-12 22:45:54,330 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-127610\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 22:45:54,331 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-127610/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 22:45:54,332 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-127610/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 22:45:55,701 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-127610/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 22:45:55,703 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-127610/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 22:45:55,703 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-127610/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 22:45:55,704 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-127610/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 22:46:00,415 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 22:46:00,415 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 22:46:00,416 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 22:46:00,424 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-123964] due to args.save_total_limit\n",
      "{'loss': 1.0247, 'grad_norm': 4.536286354064941, 'learning_rate': 3.2446516730663745e-05, 'epoch': 35.11}\n",
      "{'loss': 1.0127, 'grad_norm': 5.208780288696289, 'learning_rate': 3.2377948436642894e-05, 'epoch': 35.24}\n",
      "{'loss': 1.0279, 'grad_norm': 5.084893226623535, 'learning_rate': 3.230938014262205e-05, 'epoch': 35.38}\n",
      "{'loss': 1.0429, 'grad_norm': 4.847336769104004, 'learning_rate': 3.224081184860121e-05, 'epoch': 35.52}\n",
      "{'loss': 1.0581, 'grad_norm': 5.661252021789551, 'learning_rate': 3.217224355458036e-05, 'epoch': 35.66}\n",
      "{'loss': 1.0666, 'grad_norm': 5.379410743713379, 'learning_rate': 3.210367526055952e-05, 'epoch': 35.79}\n",
      "{'loss': 1.0739, 'grad_norm': 5.579956531524658, 'learning_rate': 3.2035106966538675e-05, 'epoch': 35.93}\n",
      " 36%|██████████▊                   | 131256/364600 [9:23:44<12:55:05,  5.02it/s][INFO|trainer.py:3388] 2024-05-12 23:00:53,546 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-131256\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 23:00:53,547 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-131256/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 23:00:53,547 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-131256/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 23:00:54,921 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-131256/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 23:00:54,922 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-131256/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 23:00:54,922 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-131256/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 23:00:54,923 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-131256/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 23:00:59,602 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 23:00:59,603 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 23:00:59,605 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 23:00:59,617 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-127610] due to args.save_total_limit\n",
      "{'loss': 1.01, 'grad_norm': 5.134979248046875, 'learning_rate': 3.196653867251783e-05, 'epoch': 36.07}\n",
      "{'loss': 0.9573, 'grad_norm': 5.556998252868652, 'learning_rate': 3.189797037849698e-05, 'epoch': 36.2}\n",
      "{'loss': 0.9718, 'grad_norm': 5.259885787963867, 'learning_rate': 3.182940208447614e-05, 'epoch': 36.34}\n",
      "{'loss': 0.9924, 'grad_norm': 5.2222208976745605, 'learning_rate': 3.17608337904553e-05, 'epoch': 36.48}\n",
      "{'loss': 0.9983, 'grad_norm': 5.009335041046143, 'learning_rate': 3.169226549643445e-05, 'epoch': 36.62}\n",
      "{'loss': 1.0232, 'grad_norm': 4.928483009338379, 'learning_rate': 3.1623697202413605e-05, 'epoch': 36.75}\n",
      "{'loss': 1.0157, 'grad_norm': 5.5725226402282715, 'learning_rate': 3.155512890839276e-05, 'epoch': 36.89}\n",
      " 37%|███████████                   | 134902/364600 [9:38:43<13:00:23,  4.91it/s][INFO|trainer.py:3388] 2024-05-12 23:15:52,054 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-134902\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 23:15:52,055 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-134902/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 23:15:52,055 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-134902/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 23:15:53,405 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-134902/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 23:15:53,406 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-134902/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 23:15:53,406 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-134902/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 23:15:53,407 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-134902/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 23:15:58,059 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 23:15:58,059 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 23:15:58,060 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 23:15:58,066 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-131256] due to args.save_total_limit\n",
      "{'loss': 1.0, 'grad_norm': 5.25609827041626, 'learning_rate': 3.148656061437192e-05, 'epoch': 37.03}\n",
      "{'loss': 0.9007, 'grad_norm': 5.325344085693359, 'learning_rate': 3.141799232035107e-05, 'epoch': 37.16}\n",
      "{'loss': 0.9298, 'grad_norm': 5.14201021194458, 'learning_rate': 3.134942402633022e-05, 'epoch': 37.3}\n",
      "{'loss': 0.941, 'grad_norm': 5.08565092086792, 'learning_rate': 3.1280855732309386e-05, 'epoch': 37.44}\n",
      "{'loss': 0.9441, 'grad_norm': 5.582076549530029, 'learning_rate': 3.1212287438288536e-05, 'epoch': 37.58}\n",
      "{'loss': 0.9642, 'grad_norm': 5.214138031005859, 'learning_rate': 3.114371914426769e-05, 'epoch': 37.71}\n",
      "{'loss': 0.9764, 'grad_norm': 5.962718963623047, 'learning_rate': 3.107515085024685e-05, 'epoch': 37.85}\n",
      "{'loss': 0.9785, 'grad_norm': 5.05949592590332, 'learning_rate': 3.1006582556226004e-05, 'epoch': 37.99}\n",
      " 38%|███████████▍                  | 138548/364600 [9:53:42<13:45:42,  4.56it/s][INFO|trainer.py:3388] 2024-05-12 23:30:51,294 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-138548\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 23:30:51,295 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-138548/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 23:30:51,295 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-138548/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 23:30:52,628 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-138548/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 23:30:52,629 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-138548/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 23:30:52,629 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-138548/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 23:30:52,630 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-138548/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 23:30:57,242 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 23:30:57,242 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 23:30:57,244 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 23:30:57,256 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-134902] due to args.save_total_limit\n",
      "{'loss': 0.8783, 'grad_norm': 5.427227020263672, 'learning_rate': 3.0938014262205153e-05, 'epoch': 38.12}\n",
      "{'loss': 0.8717, 'grad_norm': 5.215878009796143, 'learning_rate': 3.086944596818431e-05, 'epoch': 38.26}\n",
      "{'loss': 0.8879, 'grad_norm': 5.433798789978027, 'learning_rate': 3.080087767416347e-05, 'epoch': 38.4}\n",
      "{'loss': 0.9018, 'grad_norm': 5.417360782623291, 'learning_rate': 3.073230938014262e-05, 'epoch': 38.54}\n",
      "{'loss': 0.9187, 'grad_norm': 5.553948879241943, 'learning_rate': 3.066374108612178e-05, 'epoch': 38.67}\n",
      "{'loss': 0.9246, 'grad_norm': 5.142756938934326, 'learning_rate': 3.0595172792100934e-05, 'epoch': 38.81}\n",
      "{'loss': 0.9304, 'grad_norm': 5.797046184539795, 'learning_rate': 3.052660449808009e-05, 'epoch': 38.95}\n",
      " 39%|███████████▎                 | 142194/364600 [10:08:43<12:02:30,  5.13it/s][INFO|trainer.py:3388] 2024-05-12 23:45:52,383 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-142194\n",
      "[INFO|configuration_utils.py:471] 2024-05-12 23:45:52,384 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-142194/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-12 23:45:52,385 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-142194/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-12 23:45:53,741 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-142194/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 23:45:53,742 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-142194/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 23:45:53,743 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-142194/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 23:45:53,744 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-142194/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-12 23:45:58,313 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-12 23:45:58,313 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-12 23:45:58,315 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-12 23:45:58,324 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-138548] due to args.save_total_limit\n",
      "{'loss': 0.859, 'grad_norm': 4.4715986251831055, 'learning_rate': 3.0458036204059243e-05, 'epoch': 39.08}\n",
      "{'loss': 0.8293, 'grad_norm': 4.92647123336792, 'learning_rate': 3.03894679100384e-05, 'epoch': 39.22}\n",
      "{'loss': 0.8431, 'grad_norm': 5.064645767211914, 'learning_rate': 3.0320899616017556e-05, 'epoch': 39.36}\n",
      "{'loss': 0.8615, 'grad_norm': 5.243420600891113, 'learning_rate': 3.025233132199671e-05, 'epoch': 39.5}\n",
      "{'loss': 0.8714, 'grad_norm': 6.133671760559082, 'learning_rate': 3.0183763027975865e-05, 'epoch': 39.63}\n",
      "{'loss': 0.8801, 'grad_norm': 5.261296272277832, 'learning_rate': 3.011519473395502e-05, 'epoch': 39.77}\n",
      "{'loss': 0.8869, 'grad_norm': 5.25457239151001, 'learning_rate': 3.0046626439934177e-05, 'epoch': 39.91}\n",
      " 40%|███████████▌                 | 145840/364600 [10:23:54<14:17:59,  4.25it/s][INFO|trainer.py:3388] 2024-05-13 00:01:02,899 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-145840\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 00:01:02,900 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-145840/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 00:01:02,900 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-145840/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 00:01:04,246 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-145840/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 00:01:04,247 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-145840/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 00:01:04,247 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-145840/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 00:01:04,248 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-145840/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 00:01:08,898 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 00:01:08,899 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 00:01:08,900 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 00:01:08,909 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-142194] due to args.save_total_limit\n",
      "{'loss': 0.8569, 'grad_norm': 5.886989116668701, 'learning_rate': 2.997805814591333e-05, 'epoch': 40.04}\n",
      "{'loss': 0.7847, 'grad_norm': 4.354552745819092, 'learning_rate': 2.9909489851892486e-05, 'epoch': 40.18}\n",
      "{'loss': 0.8053, 'grad_norm': 5.114023208618164, 'learning_rate': 2.9840921557871642e-05, 'epoch': 40.32}\n",
      "{'loss': 0.8053, 'grad_norm': 5.665450572967529, 'learning_rate': 2.9772353263850798e-05, 'epoch': 40.46}\n",
      "{'loss': 0.8227, 'grad_norm': 4.803800106048584, 'learning_rate': 2.970378496982995e-05, 'epoch': 40.59}\n",
      "{'loss': 0.8375, 'grad_norm': 5.80670690536499, 'learning_rate': 2.9635216675809107e-05, 'epoch': 40.73}\n",
      "{'loss': 0.8358, 'grad_norm': 5.025584697723389, 'learning_rate': 2.9566648381788263e-05, 'epoch': 40.87}\n",
      " 41%|███████████▉                 | 149486/364600 [10:39:04<15:11:02,  3.94it/s][INFO|trainer.py:3388] 2024-05-13 00:16:13,485 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-149486\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 00:16:13,486 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-149486/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 00:16:13,486 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-149486/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 00:16:14,813 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-149486/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 00:16:14,815 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-149486/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 00:16:14,815 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-149486/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 00:16:14,816 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-149486/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 00:16:19,472 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 00:16:19,472 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 00:16:19,474 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 00:16:19,483 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-145840] due to args.save_total_limit\n",
      "{'loss': 0.8472, 'grad_norm': 4.726833343505859, 'learning_rate': 2.9498080087767416e-05, 'epoch': 41.0}\n",
      "{'loss': 0.7477, 'grad_norm': 5.068787097930908, 'learning_rate': 2.9429511793746572e-05, 'epoch': 41.14}\n",
      "{'loss': 0.7578, 'grad_norm': 4.701972484588623, 'learning_rate': 2.936094349972573e-05, 'epoch': 41.28}\n",
      "{'loss': 0.7685, 'grad_norm': 4.899438858032227, 'learning_rate': 2.9292375205704885e-05, 'epoch': 41.42}\n",
      "{'loss': 0.7812, 'grad_norm': 5.0015482902526855, 'learning_rate': 2.9223806911684037e-05, 'epoch': 41.55}\n",
      "{'loss': 0.7886, 'grad_norm': 4.952108860015869, 'learning_rate': 2.9155238617663194e-05, 'epoch': 41.69}\n",
      "{'loss': 0.7972, 'grad_norm': 5.88131046295166, 'learning_rate': 2.908667032364235e-05, 'epoch': 41.83}\n",
      "{'loss': 0.8097, 'grad_norm': 5.144876956939697, 'learning_rate': 2.9018102029621502e-05, 'epoch': 41.96}\n",
      " 42%|████████████▏                | 153132/364600 [10:54:22<16:12:28,  3.62it/s][INFO|trainer.py:3388] 2024-05-13 00:31:30,568 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-153132\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 00:31:30,569 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-153132/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 00:31:30,569 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-153132/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 00:31:31,940 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-153132/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 00:31:31,941 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-153132/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 00:31:31,941 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-153132/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 00:31:31,942 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-153132/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 00:31:36,510 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 00:31:36,510 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 00:31:36,513 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 00:31:36,524 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-149486] due to args.save_total_limit\n",
      "{'loss': 0.7389, 'grad_norm': 5.848343849182129, 'learning_rate': 2.894953373560066e-05, 'epoch': 42.1}\n",
      "{'loss': 0.7163, 'grad_norm': 5.04640007019043, 'learning_rate': 2.8880965441579815e-05, 'epoch': 42.24}\n",
      "{'loss': 0.732, 'grad_norm': 5.1840128898620605, 'learning_rate': 2.881239714755897e-05, 'epoch': 42.38}\n",
      "{'loss': 0.7431, 'grad_norm': 5.124771595001221, 'learning_rate': 2.8743828853538124e-05, 'epoch': 42.51}\n",
      "{'loss': 0.7582, 'grad_norm': 4.6923089027404785, 'learning_rate': 2.867526055951728e-05, 'epoch': 42.65}\n",
      "{'loss': 0.7561, 'grad_norm': 5.027599334716797, 'learning_rate': 2.8606692265496436e-05, 'epoch': 42.79}\n",
      "{'loss': 0.7642, 'grad_norm': 4.931192398071289, 'learning_rate': 2.853812397147559e-05, 'epoch': 42.92}\n",
      " 43%|████████████▍                | 156778/364600 [11:09:43<13:00:59,  4.43it/s][INFO|trainer.py:3388] 2024-05-13 00:46:51,791 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-156778\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 00:46:51,792 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-156778/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 00:46:51,792 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-156778/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 00:46:53,146 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-156778/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 00:46:53,147 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-156778/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 00:46:53,148 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-156778/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 00:46:53,149 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-156778/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 00:46:57,773 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 00:46:57,773 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 00:46:57,775 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 00:46:57,801 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-153132] due to args.save_total_limit\n",
      "{'loss': 0.7286, 'grad_norm': 4.427544593811035, 'learning_rate': 2.8469555677454745e-05, 'epoch': 43.06}\n",
      "{'loss': 0.6782, 'grad_norm': 5.119362831115723, 'learning_rate': 2.84009873834339e-05, 'epoch': 43.2}\n",
      "{'loss': 0.6933, 'grad_norm': 4.8863749504089355, 'learning_rate': 2.833241908941306e-05, 'epoch': 43.34}\n",
      "{'loss': 0.7039, 'grad_norm': 5.453842639923096, 'learning_rate': 2.826385079539221e-05, 'epoch': 43.47}\n",
      "{'loss': 0.711, 'grad_norm': 4.8158721923828125, 'learning_rate': 2.8195282501371366e-05, 'epoch': 43.61}\n",
      "{'loss': 0.7238, 'grad_norm': 5.3100905418396, 'learning_rate': 2.8126714207350523e-05, 'epoch': 43.75}\n",
      "{'loss': 0.7283, 'grad_norm': 4.8812031745910645, 'learning_rate': 2.8058145913329675e-05, 'epoch': 43.88}\n",
      " 44%|████████████▊                | 160424/364600 [11:25:01<12:37:04,  4.49it/s][INFO|trainer.py:3388] 2024-05-13 01:02:10,362 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-160424\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 01:02:10,362 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-160424/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 01:02:10,363 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-160424/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 01:02:11,671 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-160424/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 01:02:11,672 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-160424/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 01:02:11,673 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-160424/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 01:02:11,673 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-160424/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 01:02:16,246 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 01:02:16,246 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 01:02:16,247 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 01:02:16,273 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-156778] due to args.save_total_limit\n",
      "{'loss': 0.7262, 'grad_norm': 5.003659725189209, 'learning_rate': 2.798957761930883e-05, 'epoch': 44.02}\n",
      "{'loss': 0.6417, 'grad_norm': 5.185481548309326, 'learning_rate': 2.7921009325287988e-05, 'epoch': 44.16}\n",
      "{'loss': 0.6564, 'grad_norm': 4.771406173706055, 'learning_rate': 2.7852441031267147e-05, 'epoch': 44.3}\n",
      "{'loss': 0.6727, 'grad_norm': 5.313647270202637, 'learning_rate': 2.7783872737246297e-05, 'epoch': 44.43}\n",
      "{'loss': 0.6784, 'grad_norm': 5.134614944458008, 'learning_rate': 2.7715304443225453e-05, 'epoch': 44.57}\n",
      "{'loss': 0.6869, 'grad_norm': 4.888493537902832, 'learning_rate': 2.764673614920461e-05, 'epoch': 44.71}\n",
      "{'loss': 0.692, 'grad_norm': 5.336511135101318, 'learning_rate': 2.7578167855183762e-05, 'epoch': 44.84}\n",
      "{'loss': 0.7041, 'grad_norm': 5.053600311279297, 'learning_rate': 2.7509599561162918e-05, 'epoch': 44.98}\n",
      " 45%|█████████████                | 164070/364600 [11:40:21<12:15:06,  4.55it/s][INFO|trainer.py:3388] 2024-05-13 01:17:30,419 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-164070\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 01:17:30,420 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-164070/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 01:17:30,420 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-164070/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 01:17:31,777 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-164070/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 01:17:31,778 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-164070/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 01:17:31,778 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-164070/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 01:17:31,779 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-164070/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 01:17:36,430 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 01:17:36,431 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 01:17:36,432 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 01:17:36,442 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-160424] due to args.save_total_limit\n",
      "{'loss': 0.6192, 'grad_norm': 4.778295993804932, 'learning_rate': 2.7441031267142074e-05, 'epoch': 45.12}\n",
      "{'loss': 0.6225, 'grad_norm': 5.197367191314697, 'learning_rate': 2.7372462973121234e-05, 'epoch': 45.26}\n",
      "{'loss': 0.6342, 'grad_norm': 5.395830154418945, 'learning_rate': 2.7303894679100383e-05, 'epoch': 45.39}\n",
      "{'loss': 0.6448, 'grad_norm': 5.031848430633545, 'learning_rate': 2.723532638507954e-05, 'epoch': 45.53}\n",
      "{'loss': 0.6532, 'grad_norm': 5.896296977996826, 'learning_rate': 2.71667580910587e-05, 'epoch': 45.67}\n",
      "{'loss': 0.6583, 'grad_norm': 5.266870021820068, 'learning_rate': 2.7098189797037848e-05, 'epoch': 45.8}\n",
      "{'loss': 0.6648, 'grad_norm': 5.225521087646484, 'learning_rate': 2.7029621503017004e-05, 'epoch': 45.94}\n",
      " 46%|█████████████▎               | 167716/364600 [11:55:47<11:51:51,  4.61it/s][INFO|trainer.py:3388] 2024-05-13 01:32:55,980 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-167716\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 01:32:55,981 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-167716/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 01:32:55,981 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-167716/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 01:32:57,216 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-167716/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 01:32:57,217 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-167716/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 01:32:57,218 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-167716/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 01:32:57,218 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-167716/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 01:33:01,787 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 01:33:01,812 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 01:33:01,814 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 01:33:01,822 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-164070] due to args.save_total_limit\n",
      "{'loss': 0.6178, 'grad_norm': 5.04818058013916, 'learning_rate': 2.696105320899616e-05, 'epoch': 46.08}\n",
      "{'loss': 0.5913, 'grad_norm': 4.635532855987549, 'learning_rate': 2.689248491497532e-05, 'epoch': 46.22}\n",
      "{'loss': 0.5949, 'grad_norm': 5.3561906814575195, 'learning_rate': 2.682391662095447e-05, 'epoch': 46.35}\n",
      "{'loss': 0.6108, 'grad_norm': 5.117276191711426, 'learning_rate': 2.6755348326933626e-05, 'epoch': 46.49}\n",
      "{'loss': 0.6224, 'grad_norm': 5.213390350341797, 'learning_rate': 2.6686780032912785e-05, 'epoch': 46.63}\n",
      "{'loss': 0.6281, 'grad_norm': 5.088405609130859, 'learning_rate': 2.6618211738891935e-05, 'epoch': 46.76}\n",
      "{'loss': 0.6368, 'grad_norm': 5.051976680755615, 'learning_rate': 2.654964344487109e-05, 'epoch': 46.9}\n",
      " 47%|█████████████▋               | 171362/364600 [12:11:08<11:19:50,  4.74it/s][INFO|trainer.py:3388] 2024-05-13 01:48:17,447 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-171362\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 01:48:17,448 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-171362/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 01:48:17,448 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-171362/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 01:48:18,705 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-171362/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 01:48:18,706 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-171362/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 01:48:18,706 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-171362/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 01:48:18,707 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-171362/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 01:48:23,237 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 01:48:23,262 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 01:48:23,264 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 01:48:23,273 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-167716] due to args.save_total_limit\n",
      "{'loss': 0.6152, 'grad_norm': 4.881986141204834, 'learning_rate': 2.648107515085025e-05, 'epoch': 47.04}\n",
      "{'loss': 0.564, 'grad_norm': 5.066763401031494, 'learning_rate': 2.6412506856829406e-05, 'epoch': 47.17}\n",
      "{'loss': 0.5697, 'grad_norm': 4.73757791519165, 'learning_rate': 2.6343938562808556e-05, 'epoch': 47.31}\n",
      "{'loss': 0.5825, 'grad_norm': 4.839804172515869, 'learning_rate': 2.6275370268787712e-05, 'epoch': 47.45}\n",
      "{'loss': 0.5853, 'grad_norm': 5.461195945739746, 'learning_rate': 2.620680197476687e-05, 'epoch': 47.59}\n",
      "{'loss': 0.593, 'grad_norm': 4.896440029144287, 'learning_rate': 2.613823368074602e-05, 'epoch': 47.72}\n",
      "{'loss': 0.6, 'grad_norm': 4.847322463989258, 'learning_rate': 2.6069665386725177e-05, 'epoch': 47.86}\n",
      "{'loss': 0.6104, 'grad_norm': 4.478647708892822, 'learning_rate': 2.6001097092704337e-05, 'epoch': 48.0}\n",
      " 48%|█████████████▉               | 175008/364600 [12:26:31<12:45:39,  4.13it/s][INFO|trainer.py:3388] 2024-05-13 02:03:40,488 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-175008\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 02:03:40,489 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-175008/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 02:03:40,489 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-175008/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 02:03:41,779 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-175008/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 02:03:41,780 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-175008/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 02:03:41,780 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-175008/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 02:03:41,781 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-175008/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 02:03:46,432 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 02:03:46,433 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 02:03:46,435 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 02:03:46,446 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-171362] due to args.save_total_limit\n",
      "{'loss': 0.5294, 'grad_norm': 5.172453880310059, 'learning_rate': 2.5932528798683493e-05, 'epoch': 48.13}\n",
      "{'loss': 0.5392, 'grad_norm': 4.307365894317627, 'learning_rate': 2.5863960504662642e-05, 'epoch': 48.27}\n",
      "{'loss': 0.5521, 'grad_norm': 4.813899517059326, 'learning_rate': 2.57953922106418e-05, 'epoch': 48.41}\n",
      "{'loss': 0.5565, 'grad_norm': 5.233691215515137, 'learning_rate': 2.5726823916620958e-05, 'epoch': 48.55}\n",
      "{'loss': 0.5663, 'grad_norm': 5.3576979637146, 'learning_rate': 2.5658255622600114e-05, 'epoch': 48.68}\n",
      "{'loss': 0.5754, 'grad_norm': 5.31622314453125, 'learning_rate': 2.5589687328579264e-05, 'epoch': 48.82}\n",
      "{'loss': 0.58, 'grad_norm': 5.2634148597717285, 'learning_rate': 2.5521119034558423e-05, 'epoch': 48.96}\n",
      " 49%|██████████████▏              | 178654/364600 [12:42:05<11:08:00,  4.64it/s][INFO|trainer.py:3388] 2024-05-13 02:19:13,670 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-178654\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 02:19:13,671 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-178654/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 02:19:13,671 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-178654/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 02:19:14,987 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-178654/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 02:19:14,988 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-178654/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 02:19:14,988 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-178654/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 02:19:14,989 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-178654/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 02:19:19,699 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 02:19:19,699 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 02:19:19,701 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 02:19:19,710 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-175008] due to args.save_total_limit\n",
      "{'loss': 0.5266, 'grad_norm': 4.982797622680664, 'learning_rate': 2.545255074053758e-05, 'epoch': 49.09}\n",
      "{'loss': 0.5101, 'grad_norm': 4.663660526275635, 'learning_rate': 2.538398244651673e-05, 'epoch': 49.23}\n",
      "{'loss': 0.5206, 'grad_norm': 4.653820991516113, 'learning_rate': 2.5315414152495888e-05, 'epoch': 49.37}\n",
      "{'loss': 0.5349, 'grad_norm': 4.846981048583984, 'learning_rate': 2.5246845858475044e-05, 'epoch': 49.51}\n",
      "{'loss': 0.5378, 'grad_norm': 4.962299346923828, 'learning_rate': 2.51782775644542e-05, 'epoch': 49.64}\n",
      "{'loss': 0.5446, 'grad_norm': 4.924633979797363, 'learning_rate': 2.510970927043335e-05, 'epoch': 49.78}\n",
      "{'loss': 0.5535, 'grad_norm': 5.435749053955078, 'learning_rate': 2.504114097641251e-05, 'epoch': 49.92}\n",
      " 50%|██████████████▌              | 182300/364600 [12:57:50<10:12:43,  4.96it/s][INFO|trainer.py:3388] 2024-05-13 02:34:58,587 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-182300\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 02:34:58,588 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-182300/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 02:34:58,588 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-182300/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 02:34:59,922 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-182300/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 02:34:59,923 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-182300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 02:34:59,923 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-182300/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 02:34:59,924 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-182300/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 02:35:04,625 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 02:35:04,626 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 02:35:04,628 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 02:35:04,636 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-178654] due to args.save_total_limit\n",
      "{'loss': 0.5224, 'grad_norm': 4.581083297729492, 'learning_rate': 2.4972572682391662e-05, 'epoch': 50.05}\n",
      "{'loss': 0.4925, 'grad_norm': 4.435048580169678, 'learning_rate': 2.490400438837082e-05, 'epoch': 50.19}\n",
      "{'loss': 0.4966, 'grad_norm': 4.9870710372924805, 'learning_rate': 2.4835436094349975e-05, 'epoch': 50.33}\n",
      "{'loss': 0.505, 'grad_norm': 4.312280178070068, 'learning_rate': 2.476686780032913e-05, 'epoch': 50.47}\n",
      "{'loss': 0.508, 'grad_norm': 4.78123664855957, 'learning_rate': 2.4698299506308284e-05, 'epoch': 50.6}\n",
      "{'loss': 0.5149, 'grad_norm': 5.319374084472656, 'learning_rate': 2.462973121228744e-05, 'epoch': 50.74}\n",
      "{'loss': 0.5278, 'grad_norm': 4.26421594619751, 'learning_rate': 2.4561162918266596e-05, 'epoch': 50.88}\n",
      " 51%|██████████████▊              | 185946/364600 [13:13:24<10:19:37,  4.81it/s][INFO|trainer.py:3388] 2024-05-13 02:50:33,177 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-185946\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 02:50:33,178 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-185946/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 02:50:33,179 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-185946/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 02:50:34,497 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-185946/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 02:50:34,499 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-185946/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 02:50:34,499 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-185946/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 02:50:34,500 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-185946/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 02:50:39,214 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 02:50:39,214 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 02:50:39,216 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 02:50:39,225 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-182300] due to args.save_total_limit\n",
      "{'loss': 0.522, 'grad_norm': 4.891973495483398, 'learning_rate': 2.449259462424575e-05, 'epoch': 51.01}\n",
      "{'loss': 0.4602, 'grad_norm': 5.03622579574585, 'learning_rate': 2.4424026330224905e-05, 'epoch': 51.15}\n",
      "{'loss': 0.4689, 'grad_norm': 4.524442195892334, 'learning_rate': 2.435545803620406e-05, 'epoch': 51.29}\n",
      "{'loss': 0.4786, 'grad_norm': 4.18233060836792, 'learning_rate': 2.4286889742183217e-05, 'epoch': 51.43}\n",
      "{'loss': 0.4886, 'grad_norm': 4.806675434112549, 'learning_rate': 2.421832144816237e-05, 'epoch': 51.56}\n",
      "{'loss': 0.4901, 'grad_norm': 4.611050128936768, 'learning_rate': 2.4149753154141526e-05, 'epoch': 51.7}\n",
      "{'loss': 0.5021, 'grad_norm': 5.323733806610107, 'learning_rate': 2.4081184860120682e-05, 'epoch': 51.84}\n",
      "{'loss': 0.5053, 'grad_norm': 4.821100234985352, 'learning_rate': 2.4012616566099835e-05, 'epoch': 51.97}\n",
      " 52%|███████████████              | 189592/364600 [13:29:08<12:34:10,  3.87it/s][INFO|trainer.py:3388] 2024-05-13 03:06:17,373 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-189592\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 03:06:17,374 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-189592/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 03:06:17,374 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-189592/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 03:06:18,727 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-189592/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 03:06:18,729 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-189592/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 03:06:18,729 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-189592/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 03:06:18,730 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-189592/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 03:06:23,455 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 03:06:23,455 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 03:06:23,456 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 03:06:23,462 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-185946] due to args.save_total_limit\n",
      "{'loss': 0.4498, 'grad_norm': 4.823397159576416, 'learning_rate': 2.394404827207899e-05, 'epoch': 52.11}\n",
      "{'loss': 0.4506, 'grad_norm': 4.650783061981201, 'learning_rate': 2.3875479978058147e-05, 'epoch': 52.25}\n",
      "{'loss': 0.4492, 'grad_norm': 5.3509697914123535, 'learning_rate': 2.3806911684037304e-05, 'epoch': 52.39}\n",
      "{'loss': 0.4665, 'grad_norm': 5.251642227172852, 'learning_rate': 2.3738343390016456e-05, 'epoch': 52.52}\n",
      "{'loss': 0.4686, 'grad_norm': 4.471257209777832, 'learning_rate': 2.3669775095995613e-05, 'epoch': 52.66}\n",
      "{'loss': 0.4761, 'grad_norm': 4.814416885375977, 'learning_rate': 2.360120680197477e-05, 'epoch': 52.8}\n",
      "{'loss': 0.4826, 'grad_norm': 5.369185924530029, 'learning_rate': 2.353263850795392e-05, 'epoch': 52.93}\n",
      " 53%|███████████████▎             | 193238/364600 [13:44:49<11:15:55,  4.23it/s][INFO|trainer.py:3388] 2024-05-13 03:21:58,466 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-193238\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 03:21:58,466 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-193238/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 03:21:58,467 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-193238/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 03:21:59,742 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-193238/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 03:21:59,744 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-193238/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 03:21:59,744 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-193238/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 03:21:59,745 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-193238/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 03:22:04,320 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 03:22:04,320 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 03:22:04,343 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 03:22:04,352 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-189592] due to args.save_total_limit\n",
      "{'loss': 0.4517, 'grad_norm': 4.826727867126465, 'learning_rate': 2.3464070213933078e-05, 'epoch': 53.07}\n",
      "{'loss': 0.4241, 'grad_norm': 4.9067583084106445, 'learning_rate': 2.3395501919912234e-05, 'epoch': 53.21}\n",
      "{'loss': 0.4334, 'grad_norm': 5.361186981201172, 'learning_rate': 2.332693362589139e-05, 'epoch': 53.35}\n",
      "{'loss': 0.4379, 'grad_norm': 4.9540300369262695, 'learning_rate': 2.3258365331870543e-05, 'epoch': 53.48}\n",
      "{'loss': 0.4495, 'grad_norm': 5.23082971572876, 'learning_rate': 2.31897970378497e-05, 'epoch': 53.62}\n",
      "{'loss': 0.4521, 'grad_norm': 4.608271598815918, 'learning_rate': 2.3121228743828855e-05, 'epoch': 53.76}\n",
      "{'loss': 0.4616, 'grad_norm': 4.835067272186279, 'learning_rate': 2.305266044980801e-05, 'epoch': 53.89}\n",
      " 54%|████████████████▏             | 196884/364600 [14:00:37<9:39:51,  4.82it/s][INFO|trainer.py:3388] 2024-05-13 03:37:46,000 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-196884\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 03:37:46,001 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-196884/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 03:37:46,001 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-196884/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 03:37:47,291 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-196884/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 03:37:47,292 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-196884/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 03:37:47,292 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-196884/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 03:37:47,293 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-196884/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 03:37:51,893 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 03:37:51,921 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 03:37:51,923 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 03:37:51,934 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-193238] due to args.save_total_limit\n",
      "{'loss': 0.4483, 'grad_norm': 4.397408485412598, 'learning_rate': 2.2984092155787164e-05, 'epoch': 54.03}\n",
      "{'loss': 0.4038, 'grad_norm': 4.769198894500732, 'learning_rate': 2.291552386176632e-05, 'epoch': 54.17}\n",
      "{'loss': 0.4146, 'grad_norm': 4.403786659240723, 'learning_rate': 2.2846955567745476e-05, 'epoch': 54.31}\n",
      "{'loss': 0.4199, 'grad_norm': 4.6071696281433105, 'learning_rate': 2.277838727372463e-05, 'epoch': 54.44}\n",
      "{'loss': 0.4268, 'grad_norm': 4.638876438140869, 'learning_rate': 2.2709818979703785e-05, 'epoch': 54.58}\n",
      "{'loss': 0.4302, 'grad_norm': 4.671108722686768, 'learning_rate': 2.264125068568294e-05, 'epoch': 54.72}\n",
      "{'loss': 0.44, 'grad_norm': 5.265748977661133, 'learning_rate': 2.2572682391662098e-05, 'epoch': 54.85}\n",
      "{'loss': 0.4381, 'grad_norm': 5.179275989532471, 'learning_rate': 2.250411409764125e-05, 'epoch': 54.99}\n",
      " 55%|███████████████▉             | 200530/364600 [14:16:31<10:55:16,  4.17it/s][INFO|trainer.py:3388] 2024-05-13 03:53:40,040 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-200530\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 03:53:40,041 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-200530/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 03:53:40,041 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-200530/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 03:53:41,364 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-200530/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 03:53:41,365 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-200530/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 03:53:41,365 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-200530/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 03:53:41,366 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-200530/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 03:53:46,056 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 03:53:46,056 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 03:53:46,058 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 03:53:46,067 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-196884] due to args.save_total_limit\n",
      "{'loss': 0.3888, 'grad_norm': 4.084758758544922, 'learning_rate': 2.2435545803620407e-05, 'epoch': 55.13}\n",
      "{'loss': 0.3935, 'grad_norm': 4.465928554534912, 'learning_rate': 2.2366977509599563e-05, 'epoch': 55.27}\n",
      "{'loss': 0.3963, 'grad_norm': 4.657350540161133, 'learning_rate': 2.2298409215578716e-05, 'epoch': 55.4}\n",
      "{'loss': 0.4052, 'grad_norm': 4.591371059417725, 'learning_rate': 2.2229840921557872e-05, 'epoch': 55.54}\n",
      "{'loss': 0.4135, 'grad_norm': 4.821173191070557, 'learning_rate': 2.2161272627537028e-05, 'epoch': 55.68}\n",
      "{'loss': 0.4171, 'grad_norm': 4.650514125823975, 'learning_rate': 2.2092704333516184e-05, 'epoch': 55.81}\n",
      "{'loss': 0.4238, 'grad_norm': 4.952467441558838, 'learning_rate': 2.2024136039495337e-05, 'epoch': 55.95}\n",
      " 56%|████████████████▏            | 204176/364600 [14:32:25<10:33:07,  4.22it/s][INFO|trainer.py:3388] 2024-05-13 04:09:34,382 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-204176\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 04:09:34,383 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-204176/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 04:09:34,383 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-204176/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 04:09:35,748 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-204176/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 04:09:35,750 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-204176/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 04:09:35,750 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-204176/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 04:09:35,751 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-204176/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 04:09:40,485 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 04:09:40,485 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 04:09:40,487 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 04:09:40,495 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-200530] due to args.save_total_limit\n",
      "{'loss': 0.3891, 'grad_norm': 4.717243671417236, 'learning_rate': 2.1955567745474493e-05, 'epoch': 56.09}\n",
      "{'loss': 0.373, 'grad_norm': 4.069623947143555, 'learning_rate': 2.188699945145365e-05, 'epoch': 56.23}\n",
      "{'loss': 0.3799, 'grad_norm': 4.447889804840088, 'learning_rate': 2.1818431157432802e-05, 'epoch': 56.36}\n",
      "{'loss': 0.39, 'grad_norm': 4.514695644378662, 'learning_rate': 2.174986286341196e-05, 'epoch': 56.5}\n",
      "{'loss': 0.3919, 'grad_norm': 5.111133098602295, 'learning_rate': 2.1681294569391114e-05, 'epoch': 56.64}\n",
      "{'loss': 0.3965, 'grad_norm': 4.48080587387085, 'learning_rate': 2.161272627537027e-05, 'epoch': 56.77}\n",
      "{'loss': 0.4046, 'grad_norm': 4.876768589019775, 'learning_rate': 2.1544157981349423e-05, 'epoch': 56.91}\n",
      " 57%|████████████████▌            | 207822/364600 [14:48:19<10:02:43,  4.34it/s][INFO|trainer.py:3388] 2024-05-13 04:25:27,682 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-207822\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 04:25:27,683 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-207822/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 04:25:27,683 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-207822/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 04:25:29,001 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-207822/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 04:25:29,002 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-207822/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 04:25:29,002 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-207822/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 04:25:29,003 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-207822/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 04:25:33,565 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 04:25:33,566 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 04:25:33,568 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 04:25:33,576 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-204176] due to args.save_total_limit\n",
      "{'loss': 0.3817, 'grad_norm': 4.336927890777588, 'learning_rate': 2.1475589687328583e-05, 'epoch': 57.05}\n",
      "{'loss': 0.357, 'grad_norm': 4.570804595947266, 'learning_rate': 2.1407021393307736e-05, 'epoch': 57.19}\n",
      "{'loss': 0.3648, 'grad_norm': 4.742151737213135, 'learning_rate': 2.133845309928689e-05, 'epoch': 57.32}\n",
      "{'loss': 0.3716, 'grad_norm': 4.641295433044434, 'learning_rate': 2.1269884805266048e-05, 'epoch': 57.46}\n",
      "{'loss': 0.3731, 'grad_norm': 5.475332736968994, 'learning_rate': 2.12013165112452e-05, 'epoch': 57.6}\n",
      "{'loss': 0.3809, 'grad_norm': 4.712151050567627, 'learning_rate': 2.1132748217224357e-05, 'epoch': 57.73}\n",
      "{'loss': 0.3812, 'grad_norm': 4.409310817718506, 'learning_rate': 2.106417992320351e-05, 'epoch': 57.87}\n",
      " 58%|█████████████████▍            | 211468/364600 [15:04:12<9:53:18,  4.30it/s][INFO|trainer.py:3388] 2024-05-13 04:41:21,420 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-211468\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 04:41:21,421 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-211468/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 04:41:21,421 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-211468/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 04:41:22,747 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-211468/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 04:41:22,748 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-211468/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 04:41:22,748 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-211468/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 04:41:22,749 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-211468/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 04:41:27,333 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 04:41:27,333 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 04:41:27,335 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 04:41:27,344 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-207822] due to args.save_total_limit\n",
      "{'loss': 0.383, 'grad_norm': 4.162150859832764, 'learning_rate': 2.099561162918267e-05, 'epoch': 58.01}\n",
      "{'loss': 0.3407, 'grad_norm': 4.273313999176025, 'learning_rate': 2.0927043335161822e-05, 'epoch': 58.15}\n",
      "{'loss': 0.3489, 'grad_norm': 4.508772850036621, 'learning_rate': 2.0858475041140975e-05, 'epoch': 58.28}\n",
      "{'loss': 0.3526, 'grad_norm': 5.550928592681885, 'learning_rate': 2.0789906747120134e-05, 'epoch': 58.42}\n",
      "{'loss': 0.3576, 'grad_norm': 4.722227096557617, 'learning_rate': 2.0721338453099287e-05, 'epoch': 58.56}\n",
      "{'loss': 0.3605, 'grad_norm': 4.649284839630127, 'learning_rate': 2.0652770159078443e-05, 'epoch': 58.69}\n",
      "{'loss': 0.3655, 'grad_norm': 4.80319881439209, 'learning_rate': 2.05842018650576e-05, 'epoch': 58.83}\n",
      "{'loss': 0.3714, 'grad_norm': 5.22609806060791, 'learning_rate': 2.0515633571036756e-05, 'epoch': 58.97}\n",
      " 59%|█████████████████▋            | 215114/364600 [15:20:04<9:34:23,  4.34it/s][INFO|trainer.py:3388] 2024-05-13 04:57:12,710 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-215114\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 04:57:12,711 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-215114/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 04:57:12,711 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-215114/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 04:57:14,071 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-215114/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 04:57:14,072 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-215114/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 04:57:14,073 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-215114/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 04:57:14,074 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-215114/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 04:57:18,752 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 04:57:18,752 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 04:57:18,777 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 04:57:18,785 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-211468] due to args.save_total_limit\n",
      "{'loss': 0.3345, 'grad_norm': 5.241272926330566, 'learning_rate': 2.044706527701591e-05, 'epoch': 59.11}\n",
      "{'loss': 0.3314, 'grad_norm': 4.466114044189453, 'learning_rate': 2.037849698299506e-05, 'epoch': 59.24}\n",
      "{'loss': 0.3353, 'grad_norm': 4.289991855621338, 'learning_rate': 2.030992868897422e-05, 'epoch': 59.38}\n",
      "{'loss': 0.3385, 'grad_norm': 4.458993911743164, 'learning_rate': 2.0241360394953374e-05, 'epoch': 59.52}\n",
      "{'loss': 0.3452, 'grad_norm': 4.6871724128723145, 'learning_rate': 2.017279210093253e-05, 'epoch': 59.65}\n",
      "{'loss': 0.3476, 'grad_norm': 4.503798484802246, 'learning_rate': 2.0104223806911686e-05, 'epoch': 59.79}\n",
      "{'loss': 0.3553, 'grad_norm': 5.342411041259766, 'learning_rate': 2.0035655512890842e-05, 'epoch': 59.93}\n",
      " 60%|██████████████████            | 218760/364600 [15:36:04<8:49:04,  4.59it/s][INFO|trainer.py:3388] 2024-05-13 05:13:12,757 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-218760\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 05:13:12,758 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-218760/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 05:13:12,758 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-218760/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 05:13:14,122 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-218760/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 05:13:14,124 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-218760/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 05:13:14,124 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-218760/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 05:13:14,125 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-218760/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 05:13:18,786 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 05:13:18,787 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 05:13:18,788 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 05:13:18,797 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-215114] due to args.save_total_limit\n",
      "{'loss': 0.3331, 'grad_norm': 5.1502180099487305, 'learning_rate': 1.9967087218869995e-05, 'epoch': 60.07}\n",
      "{'loss': 0.3166, 'grad_norm': 4.446504592895508, 'learning_rate': 1.9898518924849148e-05, 'epoch': 60.2}\n",
      "{'loss': 0.3204, 'grad_norm': 4.185482025146484, 'learning_rate': 1.9829950630828307e-05, 'epoch': 60.34}\n",
      "{'loss': 0.3262, 'grad_norm': 4.356864929199219, 'learning_rate': 1.976138233680746e-05, 'epoch': 60.48}\n",
      "{'loss': 0.3311, 'grad_norm': 4.678393840789795, 'learning_rate': 1.9692814042786616e-05, 'epoch': 60.61}\n",
      "{'loss': 0.3354, 'grad_norm': 5.001060962677002, 'learning_rate': 1.9624245748765772e-05, 'epoch': 60.75}\n",
      "{'loss': 0.3404, 'grad_norm': 5.079350471496582, 'learning_rate': 1.955567745474493e-05, 'epoch': 60.89}\n",
      " 61%|██████████████████▎           | 222406/364600 [15:52:08<9:29:49,  4.16it/s][INFO|trainer.py:3388] 2024-05-13 05:29:16,602 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-222406\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 05:29:16,603 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-222406/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 05:29:16,604 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-222406/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 05:29:17,888 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-222406/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 05:29:17,890 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-222406/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 05:29:17,890 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-222406/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 05:29:17,892 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-222406/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 05:29:22,338 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 05:29:22,338 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 05:29:22,341 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 05:29:22,352 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-218760] due to args.save_total_limit\n",
      "{'loss': 0.331, 'grad_norm': 4.419836044311523, 'learning_rate': 1.948710916072408e-05, 'epoch': 61.03}\n",
      "{'loss': 0.3028, 'grad_norm': 4.383386611938477, 'learning_rate': 1.9418540866703238e-05, 'epoch': 61.16}\n",
      "{'loss': 0.3055, 'grad_norm': 4.333778381347656, 'learning_rate': 1.9349972572682394e-05, 'epoch': 61.3}\n",
      "{'loss': 0.3112, 'grad_norm': 4.988595008850098, 'learning_rate': 1.9281404278661547e-05, 'epoch': 61.44}\n",
      "{'loss': 0.3153, 'grad_norm': 5.163971900939941, 'learning_rate': 1.9212835984640703e-05, 'epoch': 61.57}\n",
      "{'loss': 0.3228, 'grad_norm': 3.907899856567383, 'learning_rate': 1.914426769061986e-05, 'epoch': 61.71}\n",
      "{'loss': 0.325, 'grad_norm': 4.212146282196045, 'learning_rate': 1.9075699396599015e-05, 'epoch': 61.85}\n",
      "{'loss': 0.3263, 'grad_norm': 4.616479873657227, 'learning_rate': 1.9007131102578168e-05, 'epoch': 61.99}\n",
      " 62%|██████████████████▌           | 226052/364600 [16:08:19<9:21:00,  4.12it/s][INFO|trainer.py:3388] 2024-05-13 05:45:28,269 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-226052\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 05:45:28,270 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-226052/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 05:45:28,270 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-226052/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 05:45:29,556 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-226052/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 05:45:29,557 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-226052/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 05:45:29,558 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-226052/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 05:45:29,559 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-226052/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 05:45:33,987 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 05:45:33,988 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 05:45:33,990 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 05:45:34,001 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-222406] due to args.save_total_limit\n",
      "{'loss': 0.2962, 'grad_norm': 4.422669887542725, 'learning_rate': 1.8938562808557324e-05, 'epoch': 62.12}\n",
      "{'loss': 0.2926, 'grad_norm': 4.242331027984619, 'learning_rate': 1.886999451453648e-05, 'epoch': 62.26}\n",
      "{'loss': 0.2985, 'grad_norm': 4.647274494171143, 'learning_rate': 1.8801426220515633e-05, 'epoch': 62.4}\n",
      "{'loss': 0.3027, 'grad_norm': 4.557641983032227, 'learning_rate': 1.873285792649479e-05, 'epoch': 62.53}\n",
      "{'loss': 0.3088, 'grad_norm': 4.458461284637451, 'learning_rate': 1.8664289632473945e-05, 'epoch': 62.67}\n",
      "{'loss': 0.3105, 'grad_norm': 4.6789727210998535, 'learning_rate': 1.85957213384531e-05, 'epoch': 62.81}\n",
      "{'loss': 0.3154, 'grad_norm': 4.642698287963867, 'learning_rate': 1.8527153044432254e-05, 'epoch': 62.95}\n",
      " 63%|██████████████████▉           | 229698/364600 [16:24:25<8:03:14,  4.65it/s][INFO|trainer.py:3388] 2024-05-13 06:01:34,384 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-229698\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 06:01:34,385 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-229698/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 06:01:34,386 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-229698/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 06:01:35,720 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-229698/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 06:01:35,721 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-229698/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 06:01:35,721 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-229698/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 06:01:35,722 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-229698/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 06:01:40,280 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 06:01:40,280 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 06:01:40,282 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 06:01:40,291 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-226052] due to args.save_total_limit\n",
      "{'loss': 0.2929, 'grad_norm': 4.549673557281494, 'learning_rate': 1.845858475041141e-05, 'epoch': 63.08}\n",
      "{'loss': 0.2838, 'grad_norm': 4.2093119621276855, 'learning_rate': 1.8390016456390567e-05, 'epoch': 63.22}\n",
      "{'loss': 0.2853, 'grad_norm': 4.682537078857422, 'learning_rate': 1.832144816236972e-05, 'epoch': 63.36}\n",
      "{'loss': 0.2885, 'grad_norm': 4.815731048583984, 'learning_rate': 1.8252879868348876e-05, 'epoch': 63.49}\n",
      "{'loss': 0.2957, 'grad_norm': 5.170729160308838, 'learning_rate': 1.818431157432803e-05, 'epoch': 63.63}\n",
      "{'loss': 0.2985, 'grad_norm': 4.020371913909912, 'learning_rate': 1.8115743280307188e-05, 'epoch': 63.77}\n",
      "{'loss': 0.3007, 'grad_norm': 4.983353137969971, 'learning_rate': 1.804717498628634e-05, 'epoch': 63.91}\n",
      " 64%|███████████████████▏          | 233344/364600 [16:40:31<7:37:30,  4.78it/s][INFO|trainer.py:3388] 2024-05-13 06:17:40,554 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-233344\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 06:17:40,555 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-233344/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 06:17:40,555 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-233344/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 06:17:41,855 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-233344/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 06:17:41,856 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-233344/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 06:17:41,856 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-233344/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 06:17:41,857 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-233344/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 06:17:46,324 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 06:17:46,325 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 06:17:46,327 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 06:17:46,351 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-229698] due to args.save_total_limit\n",
      "{'loss': 0.2887, 'grad_norm': 4.521115303039551, 'learning_rate': 1.7978606692265497e-05, 'epoch': 64.04}\n",
      "{'loss': 0.2695, 'grad_norm': 4.261961460113525, 'learning_rate': 1.7910038398244653e-05, 'epoch': 64.18}\n",
      "{'loss': 0.2726, 'grad_norm': 4.490432262420654, 'learning_rate': 1.7841470104223806e-05, 'epoch': 64.32}\n",
      "{'loss': 0.2774, 'grad_norm': 4.353551864624023, 'learning_rate': 1.7772901810202962e-05, 'epoch': 64.45}\n",
      "{'loss': 0.2837, 'grad_norm': 4.908097743988037, 'learning_rate': 1.7704333516182118e-05, 'epoch': 64.59}\n",
      "{'loss': 0.2836, 'grad_norm': 4.305734157562256, 'learning_rate': 1.7635765222161274e-05, 'epoch': 64.73}\n",
      "{'loss': 0.2897, 'grad_norm': 5.043435096740723, 'learning_rate': 1.7567196928140427e-05, 'epoch': 64.87}\n",
      " 65%|██████████████████▊          | 236990/364600 [16:56:44<10:08:32,  3.49it/s][INFO|trainer.py:3388] 2024-05-13 06:33:53,492 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-236990\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 06:33:53,493 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-236990/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 06:33:53,493 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-236990/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 06:33:54,806 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-236990/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 06:33:54,807 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-236990/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 06:33:54,807 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-236990/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 06:33:54,808 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-236990/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 06:33:59,301 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 06:33:59,302 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 06:33:59,304 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 06:33:59,312 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-233344] due to args.save_total_limit\n",
      "{'loss': 0.2911, 'grad_norm': 4.230961322784424, 'learning_rate': 1.7498628634119583e-05, 'epoch': 65.0}\n",
      "{'loss': 0.259, 'grad_norm': 4.4803056716918945, 'learning_rate': 1.743006034009874e-05, 'epoch': 65.14}\n",
      "{'loss': 0.2618, 'grad_norm': 3.8626787662506104, 'learning_rate': 1.7361492046077896e-05, 'epoch': 65.28}\n",
      "{'loss': 0.265, 'grad_norm': 4.65452766418457, 'learning_rate': 1.729292375205705e-05, 'epoch': 65.41}\n",
      "{'loss': 0.2709, 'grad_norm': 4.291559219360352, 'learning_rate': 1.7224355458036205e-05, 'epoch': 65.55}\n",
      "{'loss': 0.2721, 'grad_norm': 4.508846282958984, 'learning_rate': 1.715578716401536e-05, 'epoch': 65.69}\n",
      "{'loss': 0.2769, 'grad_norm': 5.093057632446289, 'learning_rate': 1.7087218869994513e-05, 'epoch': 65.83}\n",
      "{'loss': 0.2773, 'grad_norm': 4.549623012542725, 'learning_rate': 1.7018650575973673e-05, 'epoch': 65.96}\n",
      " 66%|███████████████████▊          | 240636/364600 [17:13:11<9:27:44,  3.64it/s][INFO|trainer.py:3388] 2024-05-13 06:50:19,940 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-240636\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 06:50:19,941 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-240636/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 06:50:19,941 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-240636/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 06:50:21,321 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-240636/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 06:50:21,322 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-240636/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 06:50:21,322 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-240636/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 06:50:21,323 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-240636/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 06:50:25,913 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 06:50:25,913 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 06:50:25,915 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 06:50:25,928 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-236990] due to args.save_total_limit\n",
      "{'loss': 0.2553, 'grad_norm': 4.05508279800415, 'learning_rate': 1.6950082281952826e-05, 'epoch': 66.1}\n",
      "{'loss': 0.2528, 'grad_norm': 3.7369630336761475, 'learning_rate': 1.6881513987931982e-05, 'epoch': 66.24}\n",
      "{'loss': 0.2526, 'grad_norm': 3.813990831375122, 'learning_rate': 1.6812945693911135e-05, 'epoch': 66.37}\n",
      "{'loss': 0.2592, 'grad_norm': 3.993372917175293, 'learning_rate': 1.674437739989029e-05, 'epoch': 66.51}\n",
      "{'loss': 0.2611, 'grad_norm': 4.96673059463501, 'learning_rate': 1.6675809105869447e-05, 'epoch': 66.65}\n",
      "{'loss': 0.2648, 'grad_norm': 4.10557746887207, 'learning_rate': 1.66072408118486e-05, 'epoch': 66.79}\n",
      "{'loss': 0.2656, 'grad_norm': 4.813425064086914, 'learning_rate': 1.653867251782776e-05, 'epoch': 66.92}\n",
      " 67%|████████████████████          | 244282/364600 [17:29:32<8:21:29,  4.00it/s][INFO|trainer.py:3388] 2024-05-13 07:06:40,950 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-244282\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 07:06:40,951 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-244282/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 07:06:40,951 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-244282/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 07:06:42,307 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-244282/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 07:06:42,308 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-244282/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 07:06:42,309 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-244282/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 07:06:42,310 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-244282/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 07:06:46,866 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 07:06:46,866 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 07:06:46,867 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 07:06:46,873 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-240636] due to args.save_total_limit\n",
      "{'loss': 0.2536, 'grad_norm': 4.064112186431885, 'learning_rate': 1.6470104223806912e-05, 'epoch': 67.06}\n",
      "{'loss': 0.2411, 'grad_norm': 4.719504356384277, 'learning_rate': 1.640153592978607e-05, 'epoch': 67.2}\n",
      "{'loss': 0.2505, 'grad_norm': 4.4745588302612305, 'learning_rate': 1.633296763576522e-05, 'epoch': 67.33}\n",
      "{'loss': 0.2493, 'grad_norm': 4.499454021453857, 'learning_rate': 1.6264399341744377e-05, 'epoch': 67.47}\n",
      "{'loss': 0.2503, 'grad_norm': 3.987778663635254, 'learning_rate': 1.6195831047723534e-05, 'epoch': 67.61}\n",
      "{'loss': 0.2552, 'grad_norm': 4.4290618896484375, 'learning_rate': 1.6127262753702686e-05, 'epoch': 67.75}\n",
      "{'loss': 0.2579, 'grad_norm': 4.531731605529785, 'learning_rate': 1.6058694459681846e-05, 'epoch': 67.88}\n",
      " 68%|████████████████████▍         | 247928/364600 [17:46:02<7:07:39,  4.55it/s][INFO|trainer.py:3388] 2024-05-13 07:23:11,563 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-247928\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 07:23:11,563 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-247928/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 07:23:11,564 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-247928/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 07:23:12,867 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-247928/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 07:23:12,868 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-247928/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 07:23:12,868 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-247928/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 07:23:12,869 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-247928/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 07:23:17,274 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 07:23:17,274 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 07:23:17,294 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 07:23:17,300 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-244282] due to args.save_total_limit\n",
      "{'loss': 0.2562, 'grad_norm': 3.8032639026641846, 'learning_rate': 1.5990126165661e-05, 'epoch': 68.02}\n",
      "{'loss': 0.234, 'grad_norm': 3.864058017730713, 'learning_rate': 1.5921557871640155e-05, 'epoch': 68.16}\n",
      "{'loss': 0.2362, 'grad_norm': 3.7496285438537598, 'learning_rate': 1.585298957761931e-05, 'epoch': 68.29}\n",
      "{'loss': 0.2406, 'grad_norm': 3.9640090465545654, 'learning_rate': 1.5784421283598467e-05, 'epoch': 68.43}\n",
      "{'loss': 0.2402, 'grad_norm': 4.273751258850098, 'learning_rate': 1.571585298957762e-05, 'epoch': 68.57}\n",
      "{'loss': 0.2437, 'grad_norm': 3.934805393218994, 'learning_rate': 1.5647284695556773e-05, 'epoch': 68.71}\n",
      "{'loss': 0.2467, 'grad_norm': 3.652498245239258, 'learning_rate': 1.5578716401535932e-05, 'epoch': 68.84}\n",
      "{'loss': 0.2489, 'grad_norm': 3.7606563568115234, 'learning_rate': 1.5510148107515085e-05, 'epoch': 68.98}\n",
      " 69%|████████████████████▋         | 251574/364600 [18:02:48<7:58:04,  3.94it/s][INFO|trainer.py:3388] 2024-05-13 07:39:56,861 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-251574\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 07:39:56,862 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-251574/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 07:39:56,862 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-251574/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 07:39:58,207 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-251574/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 07:39:58,208 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-251574/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 07:39:58,208 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-251574/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 07:39:58,209 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-251574/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 07:40:02,986 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 07:40:02,986 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 07:40:02,987 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 07:40:03,008 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-247928] due to args.save_total_limit\n",
      "{'loss': 0.2272, 'grad_norm': 4.354647636413574, 'learning_rate': 1.544157981349424e-05, 'epoch': 69.12}\n",
      "{'loss': 0.2274, 'grad_norm': 3.411524772644043, 'learning_rate': 1.5373011519473397e-05, 'epoch': 69.25}\n",
      "{'loss': 0.2305, 'grad_norm': 4.171504020690918, 'learning_rate': 1.5304443225452554e-05, 'epoch': 69.39}\n",
      "{'loss': 0.2319, 'grad_norm': 4.308210372924805, 'learning_rate': 1.5235874931431706e-05, 'epoch': 69.53}\n",
      "{'loss': 0.2344, 'grad_norm': 4.150519847869873, 'learning_rate': 1.516730663741086e-05, 'epoch': 69.67}\n",
      "{'loss': 0.2391, 'grad_norm': 4.316656112670898, 'learning_rate': 1.5098738343390017e-05, 'epoch': 69.8}\n",
      "{'loss': 0.2385, 'grad_norm': 4.44851541519165, 'learning_rate': 1.5030170049369171e-05, 'epoch': 69.94}\n",
      " 70%|█████████████████████         | 255220/364600 [18:19:29<7:18:48,  4.15it/s][INFO|trainer.py:3388] 2024-05-13 07:56:37,738 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-255220\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 07:56:37,739 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-255220/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 07:56:37,740 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-255220/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 07:56:39,001 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-255220/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 07:56:39,002 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-255220/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 07:56:39,002 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-255220/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 07:56:39,003 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-255220/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 07:56:43,550 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 07:56:43,550 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 07:56:43,552 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 07:56:43,561 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-251574] due to args.save_total_limit\n",
      "{'loss': 0.225, 'grad_norm': 4.209973335266113, 'learning_rate': 1.4961601755348328e-05, 'epoch': 70.08}\n",
      "{'loss': 0.2179, 'grad_norm': 4.037484169006348, 'learning_rate': 1.4893033461327482e-05, 'epoch': 70.21}\n",
      "{'loss': 0.2222, 'grad_norm': 3.6946587562561035, 'learning_rate': 1.482446516730664e-05, 'epoch': 70.35}\n",
      "{'loss': 0.2211, 'grad_norm': 4.2428717613220215, 'learning_rate': 1.4755896873285793e-05, 'epoch': 70.49}\n",
      "{'loss': 0.2259, 'grad_norm': 3.7683310508728027, 'learning_rate': 1.4687328579264947e-05, 'epoch': 70.63}\n",
      "{'loss': 0.2283, 'grad_norm': 4.147058486938477, 'learning_rate': 1.4618760285244103e-05, 'epoch': 70.76}\n",
      "{'loss': 0.2316, 'grad_norm': 4.305523872375488, 'learning_rate': 1.4550191991223258e-05, 'epoch': 70.9}\n",
      " 71%|█████████████████████▎        | 258866/364600 [18:36:18<7:05:42,  4.14it/s][INFO|trainer.py:3388] 2024-05-13 08:13:26,977 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-258866\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 08:13:26,978 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-258866/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 08:13:26,978 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-258866/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 08:13:28,208 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-258866/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 08:13:28,210 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-258866/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 08:13:28,210 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-258866/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 08:13:28,211 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-258866/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 08:13:32,683 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 08:13:32,683 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 08:13:32,685 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 08:13:32,694 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-255220] due to args.save_total_limit\n",
      "{'loss': 0.2254, 'grad_norm': 4.284609317779541, 'learning_rate': 1.4481623697202416e-05, 'epoch': 71.04}\n",
      "{'loss': 0.2084, 'grad_norm': 3.876636028289795, 'learning_rate': 1.4413055403181569e-05, 'epoch': 71.17}\n",
      "{'loss': 0.2141, 'grad_norm': 4.208460330963135, 'learning_rate': 1.4344487109160726e-05, 'epoch': 71.31}\n",
      "{'loss': 0.2146, 'grad_norm': 3.976590156555176, 'learning_rate': 1.427591881513988e-05, 'epoch': 71.45}\n",
      "{'loss': 0.2163, 'grad_norm': 3.778451442718506, 'learning_rate': 1.4207350521119034e-05, 'epoch': 71.59}\n",
      "{'loss': 0.2194, 'grad_norm': 4.75286340713501, 'learning_rate': 1.4138782227098192e-05, 'epoch': 71.72}\n",
      "{'loss': 0.2236, 'grad_norm': 3.755993366241455, 'learning_rate': 1.4070213933077344e-05, 'epoch': 71.86}\n",
      "{'loss': 0.224, 'grad_norm': 4.23431396484375, 'learning_rate': 1.4001645639056502e-05, 'epoch': 72.0}\n",
      " 72%|█████████████████████▌        | 262512/364600 [18:53:17<6:30:38,  4.36it/s][INFO|trainer.py:3388] 2024-05-13 08:30:26,112 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-262512\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 08:30:26,113 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-262512/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 08:30:26,113 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-262512/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 08:30:27,474 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-262512/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 08:30:27,475 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-262512/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 08:30:27,475 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-262512/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 08:30:27,476 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-262512/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 08:30:32,051 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 08:30:32,052 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 08:30:32,053 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 08:30:32,062 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-258866] due to args.save_total_limit\n",
      "{'loss': 0.2022, 'grad_norm': 4.001950263977051, 'learning_rate': 1.3933077345035655e-05, 'epoch': 72.13}\n",
      "{'loss': 0.2043, 'grad_norm': 3.7588768005371094, 'learning_rate': 1.3864509051014813e-05, 'epoch': 72.27}\n",
      "{'loss': 0.2065, 'grad_norm': 4.171288013458252, 'learning_rate': 1.3795940756993966e-05, 'epoch': 72.41}\n",
      "{'loss': 0.2081, 'grad_norm': 4.1884636878967285, 'learning_rate': 1.3727372462973123e-05, 'epoch': 72.55}\n",
      "{'loss': 0.212, 'grad_norm': 4.019055366516113, 'learning_rate': 1.3658804168952278e-05, 'epoch': 72.68}\n",
      "{'loss': 0.2128, 'grad_norm': 3.9061167240142822, 'learning_rate': 1.359023587493143e-05, 'epoch': 72.82}\n",
      "{'loss': 0.2149, 'grad_norm': 4.590092182159424, 'learning_rate': 1.3521667580910589e-05, 'epoch': 72.96}\n",
      " 73%|█████████████████████▉        | 266158/364600 [19:10:14<5:57:27,  4.59it/s][INFO|trainer.py:3388] 2024-05-13 08:47:23,262 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-266158\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 08:47:23,263 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-266158/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 08:47:23,263 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-266158/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 08:47:24,606 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-266158/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 08:47:24,608 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-266158/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 08:47:24,608 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-266158/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 08:47:24,609 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-266158/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 08:47:29,073 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 08:47:29,073 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 08:47:29,075 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 08:47:29,083 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-262512] due to args.save_total_limit\n",
      "{'loss': 0.2016, 'grad_norm': 4.069841384887695, 'learning_rate': 1.3453099286889741e-05, 'epoch': 73.09}\n",
      "{'loss': 0.1969, 'grad_norm': 3.7650821208953857, 'learning_rate': 1.33845309928689e-05, 'epoch': 73.23}\n",
      "{'loss': 0.1984, 'grad_norm': 3.8244950771331787, 'learning_rate': 1.3315962698848054e-05, 'epoch': 73.37}\n",
      "{'loss': 0.2026, 'grad_norm': 3.6921212673187256, 'learning_rate': 1.324739440482721e-05, 'epoch': 73.51}\n",
      "{'loss': 0.2036, 'grad_norm': 4.225021839141846, 'learning_rate': 1.3178826110806364e-05, 'epoch': 73.64}\n",
      "{'loss': 0.2052, 'grad_norm': 4.311788082122803, 'learning_rate': 1.3110257816785517e-05, 'epoch': 73.78}\n",
      "{'loss': 0.2081, 'grad_norm': 4.360690116882324, 'learning_rate': 1.3041689522764675e-05, 'epoch': 73.92}\n",
      " 74%|██████████████████████▏       | 269804/364600 [19:27:26<6:35:15,  4.00it/s][INFO|trainer.py:3388] 2024-05-13 09:04:35,250 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-269804\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 09:04:35,251 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-269804/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 09:04:35,251 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-269804/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 09:04:36,598 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-269804/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 09:04:36,599 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-269804/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 09:04:36,599 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-269804/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 09:04:36,600 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-269804/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 09:04:41,070 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 09:04:41,070 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 09:04:41,073 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 09:04:41,084 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-266158] due to args.save_total_limit\n",
      "{'loss': 0.2006, 'grad_norm': 3.889430522918701, 'learning_rate': 1.297312122874383e-05, 'epoch': 74.05}\n",
      "{'loss': 0.1903, 'grad_norm': 4.069758892059326, 'learning_rate': 1.2904552934722986e-05, 'epoch': 74.19}\n",
      "{'loss': 0.1921, 'grad_norm': 3.5697872638702393, 'learning_rate': 1.283598464070214e-05, 'epoch': 74.33}\n",
      "{'loss': 0.1976, 'grad_norm': 4.888301849365234, 'learning_rate': 1.2767416346681296e-05, 'epoch': 74.47}\n",
      "{'loss': 0.1956, 'grad_norm': 4.195688247680664, 'learning_rate': 1.269884805266045e-05, 'epoch': 74.6}\n",
      "{'loss': 0.1976, 'grad_norm': 3.5373120307922363, 'learning_rate': 1.2630279758639604e-05, 'epoch': 74.74}\n",
      "{'loss': 0.1993, 'grad_norm': 4.081260681152344, 'learning_rate': 1.2561711464618761e-05, 'epoch': 74.88}\n",
      " 75%|██████████████████████▌       | 273450/364600 [19:44:42<6:03:18,  4.18it/s][INFO|trainer.py:3388] 2024-05-13 09:21:50,939 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-273450\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 09:21:50,939 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-273450/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 09:21:50,940 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-273450/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 09:21:52,257 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-273450/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 09:21:52,258 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-273450/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 09:21:52,258 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-273450/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 09:21:52,259 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-273450/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 09:21:56,688 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 09:21:56,688 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 09:21:56,689 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 09:21:56,694 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-269804] due to args.save_total_limit\n",
      "{'loss': 0.1993, 'grad_norm': 3.637251615524292, 'learning_rate': 1.2493143170597916e-05, 'epoch': 75.01}\n",
      "{'loss': 0.1847, 'grad_norm': 4.6371355056762695, 'learning_rate': 1.2424574876577072e-05, 'epoch': 75.15}\n",
      "{'loss': 0.1862, 'grad_norm': 3.781407594680786, 'learning_rate': 1.2356006582556227e-05, 'epoch': 75.29}\n",
      "{'loss': 0.189, 'grad_norm': 3.249769926071167, 'learning_rate': 1.2287438288535381e-05, 'epoch': 75.43}\n",
      "{'loss': 0.1904, 'grad_norm': 3.62080717086792, 'learning_rate': 1.2218869994514537e-05, 'epoch': 75.56}\n",
      "{'loss': 0.1913, 'grad_norm': 3.6299779415130615, 'learning_rate': 1.2150301700493692e-05, 'epoch': 75.7}\n",
      "{'loss': 0.1916, 'grad_norm': 4.178566932678223, 'learning_rate': 1.2081733406472848e-05, 'epoch': 75.84}\n",
      "{'loss': 0.1932, 'grad_norm': 3.7569074630737305, 'learning_rate': 1.2013165112452002e-05, 'epoch': 75.97}\n",
      " 76%|██████████████████████▊       | 277096/364600 [20:01:46<5:33:11,  4.38it/s][INFO|trainer.py:3388] 2024-05-13 09:38:54,703 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-277096\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 09:38:54,703 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-277096/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 09:38:54,704 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-277096/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 09:38:56,039 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-277096/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 09:38:56,041 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-277096/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 09:38:56,041 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-277096/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 09:38:56,042 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-277096/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 09:39:00,409 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 09:39:00,409 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 09:39:00,411 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 09:39:00,419 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-273450] due to args.save_total_limit\n",
      "{'loss': 0.1789, 'grad_norm': 3.6671714782714844, 'learning_rate': 1.1944596818431158e-05, 'epoch': 76.11}\n",
      "{'loss': 0.1799, 'grad_norm': 4.360944747924805, 'learning_rate': 1.1876028524410313e-05, 'epoch': 76.25}\n",
      "{'loss': 0.1815, 'grad_norm': 4.378243446350098, 'learning_rate': 1.1807460230389467e-05, 'epoch': 76.39}\n",
      "{'loss': 0.1849, 'grad_norm': 3.7712574005126953, 'learning_rate': 1.1738891936368624e-05, 'epoch': 76.52}\n",
      "{'loss': 0.1851, 'grad_norm': 3.6135239601135254, 'learning_rate': 1.1670323642347778e-05, 'epoch': 76.66}\n",
      "{'loss': 0.187, 'grad_norm': 4.262831687927246, 'learning_rate': 1.1601755348326934e-05, 'epoch': 76.8}\n",
      "{'loss': 0.1867, 'grad_norm': 3.7981927394866943, 'learning_rate': 1.153318705430609e-05, 'epoch': 76.93}\n",
      " 77%|███████████████████████       | 280742/364600 [20:19:05<5:46:50,  4.03it/s][INFO|trainer.py:3388] 2024-05-13 09:56:14,547 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-280742\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 09:56:14,548 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-280742/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 09:56:14,548 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-280742/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 09:56:15,872 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-280742/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 09:56:15,873 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-280742/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 09:56:15,873 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-280742/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 09:56:15,874 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-280742/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 09:56:20,343 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 09:56:20,364 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 09:56:20,366 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 09:56:20,371 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-277096] due to args.save_total_limit\n",
      "{'loss': 0.1781, 'grad_norm': 3.799161434173584, 'learning_rate': 1.1464618760285245e-05, 'epoch': 77.07}\n",
      "{'loss': 0.1721, 'grad_norm': 3.511946201324463, 'learning_rate': 1.1396050466264401e-05, 'epoch': 77.21}\n",
      "{'loss': 0.1768, 'grad_norm': 3.6062841415405273, 'learning_rate': 1.1327482172243554e-05, 'epoch': 77.35}\n",
      "{'loss': 0.1761, 'grad_norm': 3.6229002475738525, 'learning_rate': 1.125891387822271e-05, 'epoch': 77.48}\n",
      "{'loss': 0.1775, 'grad_norm': 4.036831378936768, 'learning_rate': 1.1190345584201866e-05, 'epoch': 77.62}\n",
      "{'loss': 0.1794, 'grad_norm': 3.842072010040283, 'learning_rate': 1.112177729018102e-05, 'epoch': 77.76}\n",
      "{'loss': 0.1809, 'grad_norm': 4.432040691375732, 'learning_rate': 1.1053208996160177e-05, 'epoch': 77.89}\n",
      " 78%|███████████████████████▍      | 284388/364600 [20:36:24<5:59:43,  3.72it/s][INFO|trainer.py:3388] 2024-05-13 10:13:33,239 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-284388\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 10:13:33,240 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-284388/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 10:13:33,240 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-284388/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 10:13:34,587 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-284388/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 10:13:34,588 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-284388/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 10:13:34,588 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-284388/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 10:13:34,589 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-284388/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 10:13:39,072 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 10:13:39,073 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 10:13:39,074 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 10:13:39,083 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-280742] due to args.save_total_limit\n",
      "{'loss': 0.1777, 'grad_norm': 3.7242350578308105, 'learning_rate': 1.0984640702139331e-05, 'epoch': 78.03}\n",
      "{'loss': 0.1685, 'grad_norm': 3.5870072841644287, 'learning_rate': 1.0916072408118487e-05, 'epoch': 78.17}\n",
      "{'loss': 0.1691, 'grad_norm': 4.315713405609131, 'learning_rate': 1.0847504114097642e-05, 'epoch': 78.3}\n",
      "{'loss': 0.1694, 'grad_norm': 4.229913234710693, 'learning_rate': 1.0778935820076796e-05, 'epoch': 78.44}\n",
      "{'loss': 0.1722, 'grad_norm': 4.238448143005371, 'learning_rate': 1.0710367526055953e-05, 'epoch': 78.58}\n",
      "{'loss': 0.1739, 'grad_norm': 3.810060739517212, 'learning_rate': 1.0641799232035107e-05, 'epoch': 78.72}\n",
      "{'loss': 0.1743, 'grad_norm': 3.8846802711486816, 'learning_rate': 1.0573230938014263e-05, 'epoch': 78.85}\n",
      "{'loss': 0.1771, 'grad_norm': 3.194765567779541, 'learning_rate': 1.0504662643993418e-05, 'epoch': 78.99}\n",
      " 79%|███████████████████████▋      | 288034/364600 [20:53:42<5:37:32,  3.78it/s][INFO|trainer.py:3388] 2024-05-13 10:30:50,753 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-288034\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 10:30:50,754 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-288034/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 10:30:50,754 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-288034/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 10:30:52,048 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-288034/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 10:30:52,050 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-288034/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 10:30:52,050 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-288034/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 10:30:52,051 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-288034/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 10:30:56,595 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 10:30:56,595 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 10:30:56,597 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 10:30:56,603 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-284388] due to args.save_total_limit\n",
      "{'loss': 0.1636, 'grad_norm': 3.9391047954559326, 'learning_rate': 1.0436094349972574e-05, 'epoch': 79.13}\n",
      "{'loss': 0.1637, 'grad_norm': 4.282817840576172, 'learning_rate': 1.0367526055951728e-05, 'epoch': 79.26}\n",
      "{'loss': 0.167, 'grad_norm': 3.725553512573242, 'learning_rate': 1.0298957761930883e-05, 'epoch': 79.4}\n",
      "{'loss': 0.1674, 'grad_norm': 3.7785303592681885, 'learning_rate': 1.0230389467910039e-05, 'epoch': 79.54}\n",
      "{'loss': 0.1661, 'grad_norm': 3.667619228363037, 'learning_rate': 1.0161821173889193e-05, 'epoch': 79.68}\n",
      "{'loss': 0.1696, 'grad_norm': 3.732048273086548, 'learning_rate': 1.009325287986835e-05, 'epoch': 79.81}\n",
      "{'loss': 0.1697, 'grad_norm': 4.32537841796875, 'learning_rate': 1.0024684585847504e-05, 'epoch': 79.95}\n",
      " 80%|████████████████████████      | 291680/364600 [21:10:50<4:36:20,  4.40it/s][INFO|trainer.py:3388] 2024-05-13 10:47:59,132 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-291680\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 10:47:59,133 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-291680/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 10:47:59,133 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-291680/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 10:48:00,453 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-291680/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 10:48:00,454 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-291680/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 10:48:00,454 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-291680/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 10:48:00,455 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-291680/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 10:48:04,998 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 10:48:04,998 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 10:48:05,000 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 10:48:05,009 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-288034] due to args.save_total_limit\n",
      "{'loss': 0.1632, 'grad_norm': 3.7802329063415527, 'learning_rate': 9.95611629182666e-06, 'epoch': 80.09}\n",
      "{'loss': 0.1587, 'grad_norm': 4.236711025238037, 'learning_rate': 9.887547997805815e-06, 'epoch': 80.22}\n",
      "{'loss': 0.1588, 'grad_norm': 3.8807108402252197, 'learning_rate': 9.818979703784971e-06, 'epoch': 80.36}\n",
      "{'loss': 0.1598, 'grad_norm': 3.935448408126831, 'learning_rate': 9.750411409764125e-06, 'epoch': 80.5}\n",
      "{'loss': 0.1623, 'grad_norm': 3.9982056617736816, 'learning_rate': 9.68184311574328e-06, 'epoch': 80.64}\n",
      "{'loss': 0.1641, 'grad_norm': 4.14504337310791, 'learning_rate': 9.613274821722436e-06, 'epoch': 80.77}\n",
      "{'loss': 0.1647, 'grad_norm': 3.4991772174835205, 'learning_rate': 9.54470652770159e-06, 'epoch': 80.91}\n",
      " 81%|████████████████████████▎     | 295326/364600 [21:28:06<4:49:30,  3.99it/s][INFO|trainer.py:3388] 2024-05-13 11:05:14,900 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-295326\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 11:05:14,900 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-295326/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 11:05:14,901 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-295326/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 11:05:16,265 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-295326/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 11:05:16,267 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-295326/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 11:05:16,267 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-295326/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 11:05:16,268 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-295326/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 11:05:20,778 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 11:05:20,801 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 11:05:20,804 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 11:05:20,815 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-291680] due to args.save_total_limit\n",
      "{'loss': 0.1608, 'grad_norm': 3.483520030975342, 'learning_rate': 9.476138233680747e-06, 'epoch': 81.05}\n",
      "{'loss': 0.1542, 'grad_norm': 3.9691319465637207, 'learning_rate': 9.407569939659903e-06, 'epoch': 81.18}\n",
      "{'loss': 0.1551, 'grad_norm': 4.031587600708008, 'learning_rate': 9.339001645639057e-06, 'epoch': 81.32}\n",
      "{'loss': 0.1533, 'grad_norm': 3.9734628200531006, 'learning_rate': 9.270433351618212e-06, 'epoch': 81.46}\n",
      "{'loss': 0.1573, 'grad_norm': 3.245915651321411, 'learning_rate': 9.201865057597366e-06, 'epoch': 81.6}\n",
      "{'loss': 0.1575, 'grad_norm': 3.699833393096924, 'learning_rate': 9.133296763576522e-06, 'epoch': 81.73}\n",
      "{'loss': 0.1581, 'grad_norm': 3.8309028148651123, 'learning_rate': 9.064728469555677e-06, 'epoch': 81.87}\n",
      " 82%|████████████████████████▌     | 298972/364600 [21:45:20<4:09:35,  4.38it/s][INFO|trainer.py:3388] 2024-05-13 11:22:29,004 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-298972\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 11:22:29,005 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-298972/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 11:22:29,005 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-298972/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 11:22:30,377 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-298972/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 11:22:30,379 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-298972/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 11:22:30,379 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-298972/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 11:22:30,380 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-298972/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 11:22:34,940 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 11:22:34,941 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 11:22:34,942 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 11:22:34,951 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-295326] due to args.save_total_limit\n",
      "{'loss': 0.1593, 'grad_norm': 4.079482078552246, 'learning_rate': 8.996160175534833e-06, 'epoch': 82.01}\n",
      "{'loss': 0.1485, 'grad_norm': 3.2036027908325195, 'learning_rate': 8.92759188151399e-06, 'epoch': 82.14}\n",
      "{'loss': 0.1502, 'grad_norm': 3.7567873001098633, 'learning_rate': 8.859023587493144e-06, 'epoch': 82.28}\n",
      "{'loss': 0.1517, 'grad_norm': 4.391474723815918, 'learning_rate': 8.7904552934723e-06, 'epoch': 82.42}\n",
      "{'loss': 0.1535, 'grad_norm': 2.887322187423706, 'learning_rate': 8.721886999451453e-06, 'epoch': 82.56}\n",
      "{'loss': 0.1525, 'grad_norm': 3.5882978439331055, 'learning_rate': 8.653318705430609e-06, 'epoch': 82.69}\n",
      "{'loss': 0.1541, 'grad_norm': 3.38724946975708, 'learning_rate': 8.584750411409765e-06, 'epoch': 82.83}\n",
      "{'loss': 0.1539, 'grad_norm': 3.582343578338623, 'learning_rate': 8.51618211738892e-06, 'epoch': 82.97}\n",
      " 83%|████████████████████████▉     | 302618/364600 [22:02:34<4:41:08,  3.67it/s][INFO|trainer.py:3388] 2024-05-13 11:39:42,892 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-302618\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 11:39:42,893 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-302618/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 11:39:42,893 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-302618/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 11:39:44,172 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-302618/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 11:39:44,173 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-302618/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 11:39:44,173 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-302618/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 11:39:44,174 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-302618/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 11:39:48,534 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 11:39:48,560 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 11:39:48,562 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 11:39:48,570 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-298972] due to args.save_total_limit\n",
      "{'loss': 0.1472, 'grad_norm': 3.700831413269043, 'learning_rate': 8.447613823368076e-06, 'epoch': 83.1}\n",
      "{'loss': 0.1438, 'grad_norm': 3.810107707977295, 'learning_rate': 8.37904552934723e-06, 'epoch': 83.24}\n",
      "{'loss': 0.1468, 'grad_norm': 3.461057424545288, 'learning_rate': 8.310477235326386e-06, 'epoch': 83.38}\n",
      "{'loss': 0.147, 'grad_norm': 3.1016461849212646, 'learning_rate': 8.24190894130554e-06, 'epoch': 83.52}\n",
      "{'loss': 0.1484, 'grad_norm': 3.615780830383301, 'learning_rate': 8.173340647284695e-06, 'epoch': 83.65}\n",
      "{'loss': 0.1468, 'grad_norm': 3.3265013694763184, 'learning_rate': 8.104772353263851e-06, 'epoch': 83.79}\n",
      "{'loss': 0.1503, 'grad_norm': 3.722999334335327, 'learning_rate': 8.036204059243006e-06, 'epoch': 83.93}\n",
      " 84%|█████████████████████████▏    | 306264/364600 [22:19:50<3:55:00,  4.14it/s][INFO|trainer.py:3388] 2024-05-13 11:56:59,067 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-306264\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 11:56:59,068 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-306264/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 11:56:59,068 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-306264/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 11:57:00,375 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-306264/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 11:57:00,377 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-306264/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 11:57:00,377 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-306264/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 11:57:00,378 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-306264/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 11:57:04,701 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 11:57:04,701 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 11:57:04,703 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 11:57:04,712 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-302618] due to args.save_total_limit\n",
      "{'loss': 0.1465, 'grad_norm': 3.4315872192382812, 'learning_rate': 7.967635765222162e-06, 'epoch': 84.06}\n",
      "{'loss': 0.1414, 'grad_norm': 3.664315700531006, 'learning_rate': 7.899067471201317e-06, 'epoch': 84.2}\n",
      "{'loss': 0.1412, 'grad_norm': 2.992607831954956, 'learning_rate': 7.830499177180473e-06, 'epoch': 84.34}\n",
      "{'loss': 0.1425, 'grad_norm': 3.560657024383545, 'learning_rate': 7.761930883159627e-06, 'epoch': 84.48}\n",
      "{'loss': 0.145, 'grad_norm': 4.001883506774902, 'learning_rate': 7.693362589138782e-06, 'epoch': 84.61}\n",
      "{'loss': 0.1476, 'grad_norm': 3.371948480606079, 'learning_rate': 7.624794295117937e-06, 'epoch': 84.75}\n",
      "{'loss': 0.1455, 'grad_norm': 3.9280834197998047, 'learning_rate': 7.556226001097093e-06, 'epoch': 84.89}\n",
      " 85%|█████████████████████████▌    | 309910/364600 [22:36:51<3:35:39,  4.23it/s][INFO|trainer.py:3388] 2024-05-13 12:14:00,117 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-309910\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 12:14:00,118 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-309910/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 12:14:00,119 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-309910/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 12:14:01,489 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-309910/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 12:14:01,491 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-309910/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 12:14:01,491 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-309910/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 12:14:01,492 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-309910/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 12:14:06,062 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 12:14:06,062 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 12:14:06,064 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 12:14:06,073 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-306264] due to args.save_total_limit\n",
      "{'loss': 0.1434, 'grad_norm': 3.2914552688598633, 'learning_rate': 7.4876577070762485e-06, 'epoch': 85.02}\n",
      "{'loss': 0.138, 'grad_norm': 3.4161980152130127, 'learning_rate': 7.419089413055404e-06, 'epoch': 85.16}\n",
      "{'loss': 0.1393, 'grad_norm': 3.9036171436309814, 'learning_rate': 7.350521119034559e-06, 'epoch': 85.3}\n",
      "{'loss': 0.1389, 'grad_norm': 3.8328452110290527, 'learning_rate': 7.2819528250137145e-06, 'epoch': 85.44}\n",
      "{'loss': 0.1387, 'grad_norm': 3.2638742923736572, 'learning_rate': 7.21338453099287e-06, 'epoch': 85.57}\n",
      "{'loss': 0.1413, 'grad_norm': 3.8440749645233154, 'learning_rate': 7.144816236972024e-06, 'epoch': 85.71}\n",
      "{'loss': 0.1409, 'grad_norm': 4.172990798950195, 'learning_rate': 7.07624794295118e-06, 'epoch': 85.85}\n",
      "{'loss': 0.1412, 'grad_norm': 3.7025864124298096, 'learning_rate': 7.007679648930335e-06, 'epoch': 85.98}\n",
      " 86%|█████████████████████████▊    | 313556/364600 [22:53:44<3:12:14,  4.43it/s][INFO|trainer.py:3388] 2024-05-13 12:30:53,414 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-313556\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 12:30:53,415 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-313556/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 12:30:53,415 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-313556/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 12:30:54,726 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-313556/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 12:30:54,727 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-313556/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 12:30:54,727 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-313556/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 12:30:54,728 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-313556/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 12:30:59,154 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 12:30:59,154 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 12:30:59,157 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 12:30:59,165 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-309910] due to args.save_total_limit\n",
      "{'loss': 0.1346, 'grad_norm': 2.4466094970703125, 'learning_rate': 6.93911135490949e-06, 'epoch': 86.12}\n",
      "{'loss': 0.1346, 'grad_norm': 3.610511541366577, 'learning_rate': 6.870543060888646e-06, 'epoch': 86.26}\n",
      "{'loss': 0.1357, 'grad_norm': 3.2303617000579834, 'learning_rate': 6.801974766867801e-06, 'epoch': 86.4}\n",
      "{'loss': 0.1377, 'grad_norm': 3.74819016456604, 'learning_rate': 6.733406472846956e-06, 'epoch': 86.53}\n",
      "{'loss': 0.1374, 'grad_norm': 3.3001086711883545, 'learning_rate': 6.664838178826111e-06, 'epoch': 86.67}\n",
      "{'loss': 0.1381, 'grad_norm': 3.8687660694122314, 'learning_rate': 6.596269884805266e-06, 'epoch': 86.81}\n",
      "{'loss': 0.1374, 'grad_norm': 3.627427101135254, 'learning_rate': 6.527701590784421e-06, 'epoch': 86.94}\n",
      " 87%|██████████████████████████    | 317202/364600 [23:10:42<3:11:55,  4.12it/s][INFO|trainer.py:3388] 2024-05-13 12:47:50,950 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-317202\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 12:47:50,951 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-317202/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 12:47:50,951 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-317202/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 12:47:52,305 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-317202/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 12:47:52,306 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-317202/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 12:47:52,306 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-317202/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 12:47:52,307 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-317202/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 12:47:56,785 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 12:47:56,786 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 12:47:56,787 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 12:47:56,796 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-313556] due to args.save_total_limit\n",
      "{'loss': 0.1327, 'grad_norm': 3.2286431789398193, 'learning_rate': 6.459133296763577e-06, 'epoch': 87.08}\n",
      "{'loss': 0.1322, 'grad_norm': 2.8570611476898193, 'learning_rate': 6.390565002742732e-06, 'epoch': 87.22}\n",
      "{'loss': 0.1315, 'grad_norm': 3.3692467212677, 'learning_rate': 6.321996708721887e-06, 'epoch': 87.36}\n",
      "{'loss': 0.1327, 'grad_norm': 3.5185604095458984, 'learning_rate': 6.253428414701043e-06, 'epoch': 87.49}\n",
      "{'loss': 0.1328, 'grad_norm': 3.416106700897217, 'learning_rate': 6.184860120680198e-06, 'epoch': 87.63}\n",
      "{'loss': 0.1325, 'grad_norm': 2.7670998573303223, 'learning_rate': 6.116291826659353e-06, 'epoch': 87.77}\n",
      "{'loss': 0.1355, 'grad_norm': 3.5294463634490967, 'learning_rate': 6.047723532638509e-06, 'epoch': 87.9}\n",
      " 88%|██████████████████████████▍   | 320848/364600 [23:27:38<2:41:23,  4.52it/s][INFO|trainer.py:3388] 2024-05-13 13:04:47,317 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-320848\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 13:04:47,318 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-320848/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 13:04:47,318 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-320848/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 13:04:48,640 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-320848/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 13:04:48,641 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-320848/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 13:04:48,641 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-320848/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 13:04:48,642 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-320848/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 13:04:53,167 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 13:04:53,167 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 13:04:53,169 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 13:04:53,178 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-317202] due to args.save_total_limit\n",
      "{'loss': 0.1316, 'grad_norm': 2.728625535964966, 'learning_rate': 5.979155238617663e-06, 'epoch': 88.04}\n",
      "{'loss': 0.1279, 'grad_norm': 3.675401449203491, 'learning_rate': 5.9105869445968184e-06, 'epoch': 88.18}\n",
      "{'loss': 0.1286, 'grad_norm': 3.3878486156463623, 'learning_rate': 5.842018650575974e-06, 'epoch': 88.32}\n",
      "{'loss': 0.1285, 'grad_norm': 3.215028762817383, 'learning_rate': 5.773450356555129e-06, 'epoch': 88.45}\n",
      "{'loss': 0.1309, 'grad_norm': 3.3920953273773193, 'learning_rate': 5.704882062534284e-06, 'epoch': 88.59}\n",
      "{'loss': 0.1288, 'grad_norm': 4.03735876083374, 'learning_rate': 5.63631376851344e-06, 'epoch': 88.73}\n",
      "{'loss': 0.1314, 'grad_norm': 3.8700907230377197, 'learning_rate': 5.567745474492595e-06, 'epoch': 88.86}\n",
      " 89%|██████████████████████████▋   | 324494/364600 [23:44:28<2:51:25,  3.90it/s][INFO|trainer.py:3388] 2024-05-13 13:21:36,942 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-324494\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 13:21:36,942 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-324494/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 13:21:36,943 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-324494/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 13:21:38,290 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-324494/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 13:21:38,291 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-324494/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 13:21:38,291 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-324494/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 13:21:38,292 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-324494/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 13:21:42,676 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 13:21:42,676 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 13:21:42,678 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 13:21:42,690 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-320848] due to args.save_total_limit\n",
      "{'loss': 0.1296, 'grad_norm': 3.8290393352508545, 'learning_rate': 5.4991771804717495e-06, 'epoch': 89.0}\n",
      "{'loss': 0.1225, 'grad_norm': 3.1456034183502197, 'learning_rate': 5.430608886450905e-06, 'epoch': 89.14}\n",
      "{'loss': 0.1241, 'grad_norm': 3.4296352863311768, 'learning_rate': 5.362040592430061e-06, 'epoch': 89.28}\n",
      "{'loss': 0.1245, 'grad_norm': 3.2781150341033936, 'learning_rate': 5.293472298409216e-06, 'epoch': 89.41}\n",
      "{'loss': 0.1266, 'grad_norm': 2.664435625076294, 'learning_rate': 5.224904004388371e-06, 'epoch': 89.55}\n",
      "{'loss': 0.127, 'grad_norm': 4.348361015319824, 'learning_rate': 5.156335710367526e-06, 'epoch': 89.69}\n",
      "{'loss': 0.1255, 'grad_norm': 3.075655698776245, 'learning_rate': 5.0877674163466815e-06, 'epoch': 89.82}\n",
      "{'loss': 0.1269, 'grad_norm': 3.5324909687042236, 'learning_rate': 5.019199122325837e-06, 'epoch': 89.96}\n",
      " 90%|███████████████████████████   | 328140/364600 [24:01:23<2:27:13,  4.13it/s][INFO|trainer.py:3388] 2024-05-13 13:38:31,713 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-328140\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 13:38:31,714 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-328140/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 13:38:31,714 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-328140/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 13:38:33,102 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-328140/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 13:38:33,103 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-328140/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 13:38:33,103 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-328140/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 13:38:33,104 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-328140/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 13:38:37,642 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 13:38:37,642 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 13:38:37,644 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 13:38:37,653 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-324494] due to args.save_total_limit\n",
      "{'loss': 0.1237, 'grad_norm': 2.883422374725342, 'learning_rate': 4.950630828304992e-06, 'epoch': 90.1}\n",
      "{'loss': 0.1221, 'grad_norm': 3.225177049636841, 'learning_rate': 4.8820625342841474e-06, 'epoch': 90.24}\n",
      "{'loss': 0.1225, 'grad_norm': 3.199986457824707, 'learning_rate': 4.813494240263303e-06, 'epoch': 90.37}\n",
      "{'loss': 0.1231, 'grad_norm': 3.1552860736846924, 'learning_rate': 4.744925946242457e-06, 'epoch': 90.51}\n",
      "{'loss': 0.1227, 'grad_norm': 3.491950750350952, 'learning_rate': 4.6763576522216126e-06, 'epoch': 90.65}\n",
      "{'loss': 0.1235, 'grad_norm': 3.0924017429351807, 'learning_rate': 4.607789358200768e-06, 'epoch': 90.78}\n",
      "{'loss': 0.1217, 'grad_norm': 3.1873390674591064, 'learning_rate': 4.539221064179924e-06, 'epoch': 90.92}\n",
      " 91%|███████████████████████████▎  | 331786/364600 [24:18:13<2:04:11,  4.40it/s][INFO|trainer.py:3388] 2024-05-13 13:55:21,987 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-331786\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 13:55:21,987 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-331786/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 13:55:21,988 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-331786/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 13:55:23,305 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-331786/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 13:55:23,306 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-331786/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 13:55:23,307 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-331786/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 13:55:23,307 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-331786/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 13:55:27,714 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 13:55:27,714 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 13:55:27,716 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 13:55:27,725 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-328140] due to args.save_total_limit\n",
      "{'loss': 0.122, 'grad_norm': 3.9850494861602783, 'learning_rate': 4.4706527701590785e-06, 'epoch': 91.06}\n",
      "{'loss': 0.1207, 'grad_norm': 3.238954782485962, 'learning_rate': 4.402084476138234e-06, 'epoch': 91.2}\n",
      "{'loss': 0.1207, 'grad_norm': 2.354977607727051, 'learning_rate': 4.333516182117389e-06, 'epoch': 91.33}\n",
      "{'loss': 0.1195, 'grad_norm': 3.9573888778686523, 'learning_rate': 4.2649478880965445e-06, 'epoch': 91.47}\n",
      "{'loss': 0.1199, 'grad_norm': 3.099452495574951, 'learning_rate': 4.196379594075699e-06, 'epoch': 91.61}\n",
      "{'loss': 0.1191, 'grad_norm': 2.886826753616333, 'learning_rate': 4.127811300054855e-06, 'epoch': 91.74}\n",
      "{'loss': 0.121, 'grad_norm': 2.478618860244751, 'learning_rate': 4.0592430060340105e-06, 'epoch': 91.88}\n",
      " 92%|███████████████████████████▌  | 335432/364600 [24:34:58<1:56:02,  4.19it/s][INFO|trainer.py:3388] 2024-05-13 14:12:06,970 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-335432\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 14:12:06,971 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-335432/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 14:12:06,971 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-335432/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 14:12:08,281 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-335432/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 14:12:08,282 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-335432/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 14:12:08,282 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-335432/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 14:12:08,283 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-335432/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 14:12:12,649 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 14:12:12,649 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 14:12:12,651 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 14:12:12,663 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-331786] due to args.save_total_limit\n",
      "{'loss': 0.1201, 'grad_norm': 3.46500301361084, 'learning_rate': 3.990674712013166e-06, 'epoch': 92.02}\n",
      "{'loss': 0.1162, 'grad_norm': 2.743831157684326, 'learning_rate': 3.92210641799232e-06, 'epoch': 92.16}\n",
      "{'loss': 0.1159, 'grad_norm': 3.4375343322753906, 'learning_rate': 3.853538123971476e-06, 'epoch': 92.29}\n",
      "{'loss': 0.1183, 'grad_norm': 3.173588991165161, 'learning_rate': 3.7849698299506313e-06, 'epoch': 92.43}\n",
      "{'loss': 0.1167, 'grad_norm': 3.2577898502349854, 'learning_rate': 3.7164015359297867e-06, 'epoch': 92.57}\n",
      "{'loss': 0.1173, 'grad_norm': 3.3100554943084717, 'learning_rate': 3.647833241908941e-06, 'epoch': 92.7}\n",
      "{'loss': 0.1165, 'grad_norm': 3.179342269897461, 'learning_rate': 3.579264947888097e-06, 'epoch': 92.84}\n",
      "{'loss': 0.1176, 'grad_norm': 3.096334218978882, 'learning_rate': 3.510696653867252e-06, 'epoch': 92.98}\n",
      " 93%|███████████████████████████▉  | 339078/364600 [24:51:40<1:43:46,  4.10it/s][INFO|trainer.py:3388] 2024-05-13 14:28:49,058 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-339078\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 14:28:49,059 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-339078/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 14:28:49,059 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-339078/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 14:28:50,369 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-339078/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 14:28:50,370 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-339078/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 14:28:50,370 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-339078/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 14:28:50,371 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-339078/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 14:28:54,707 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 14:28:54,719 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 14:28:54,721 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 14:28:54,732 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-335432] due to args.save_total_limit\n",
      "{'loss': 0.1142, 'grad_norm': 2.9532058238983154, 'learning_rate': 3.4421283598464067e-06, 'epoch': 93.12}\n",
      "{'loss': 0.1143, 'grad_norm': 3.717654228210449, 'learning_rate': 3.3735600658255624e-06, 'epoch': 93.25}\n",
      "{'loss': 0.1146, 'grad_norm': 3.084181308746338, 'learning_rate': 3.3049917718047177e-06, 'epoch': 93.39}\n",
      "{'loss': 0.1147, 'grad_norm': 3.636079788208008, 'learning_rate': 3.236423477783873e-06, 'epoch': 93.53}\n",
      "{'loss': 0.1153, 'grad_norm': 2.80279278755188, 'learning_rate': 3.167855183763028e-06, 'epoch': 93.66}\n",
      "{'loss': 0.1133, 'grad_norm': 2.7597951889038086, 'learning_rate': 3.0992868897421833e-06, 'epoch': 93.8}\n",
      "{'loss': 0.1159, 'grad_norm': 3.1757214069366455, 'learning_rate': 3.0307185957213386e-06, 'epoch': 93.94}\n",
      " 94%|████████████████████████████▏ | 342724/364600 [25:08:22<1:24:41,  4.30it/s][INFO|trainer.py:3388] 2024-05-13 14:45:31,274 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-342724\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 14:45:31,275 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-342724/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 14:45:31,276 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-342724/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 14:45:32,697 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-342724/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 14:45:32,698 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-342724/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 14:45:32,698 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-342724/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 14:45:32,699 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-342724/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 14:45:37,325 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 14:45:37,325 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 14:45:37,327 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 14:45:37,336 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-339078] due to args.save_total_limit\n",
      "{'loss': 0.1129, 'grad_norm': 3.245447874069214, 'learning_rate': 2.962150301700494e-06, 'epoch': 94.08}\n",
      "{'loss': 0.1117, 'grad_norm': 2.7797350883483887, 'learning_rate': 2.8935820076796493e-06, 'epoch': 94.21}\n",
      "{'loss': 0.112, 'grad_norm': 3.2236897945404053, 'learning_rate': 2.825013713658804e-06, 'epoch': 94.35}\n",
      "{'loss': 0.1118, 'grad_norm': 3.792973756790161, 'learning_rate': 2.7564454196379595e-06, 'epoch': 94.49}\n",
      "{'loss': 0.1124, 'grad_norm': 2.6465868949890137, 'learning_rate': 2.687877125617115e-06, 'epoch': 94.62}\n",
      "{'loss': 0.113, 'grad_norm': 2.944362163543701, 'learning_rate': 2.61930883159627e-06, 'epoch': 94.76}\n",
      "{'loss': 0.1114, 'grad_norm': 3.0111756324768066, 'learning_rate': 2.550740537575425e-06, 'epoch': 94.9}\n",
      " 95%|████████████████████████████▌ | 346370/364600 [25:24:55<1:06:14,  4.59it/s][INFO|trainer.py:3388] 2024-05-13 15:02:03,728 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-346370\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 15:02:03,728 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-346370/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 15:02:03,729 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-346370/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 15:02:05,000 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-346370/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 15:02:05,001 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-346370/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 15:02:05,001 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-346370/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 15:02:05,002 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-346370/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 15:02:09,356 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 15:02:09,356 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 15:02:09,358 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 15:02:09,366 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-342724] due to args.save_total_limit\n",
      "{'loss': 0.1119, 'grad_norm': 3.691293954849243, 'learning_rate': 2.4821722435545808e-06, 'epoch': 95.04}\n",
      "{'loss': 0.1106, 'grad_norm': 2.5828378200531006, 'learning_rate': 2.4136039495337357e-06, 'epoch': 95.17}\n",
      "{'loss': 0.1091, 'grad_norm': 3.733536720275879, 'learning_rate': 2.3450356555128906e-06, 'epoch': 95.31}\n",
      "{'loss': 0.1102, 'grad_norm': 3.203916311264038, 'learning_rate': 2.2764673614920463e-06, 'epoch': 95.45}\n",
      "{'loss': 0.1123, 'grad_norm': 2.8628923892974854, 'learning_rate': 2.2078990674712012e-06, 'epoch': 95.58}\n",
      "{'loss': 0.1097, 'grad_norm': 3.761380195617676, 'learning_rate': 2.1393307734503565e-06, 'epoch': 95.72}\n",
      "{'loss': 0.1106, 'grad_norm': 2.951045036315918, 'learning_rate': 2.070762479429512e-06, 'epoch': 95.86}\n",
      "{'loss': 0.1098, 'grad_norm': 3.6867475509643555, 'learning_rate': 2.002194185408667e-06, 'epoch': 96.0}\n",
      " 96%|████████████████████████████▊ | 350016/364600 [25:41:33<1:05:09,  3.73it/s][INFO|trainer.py:3388] 2024-05-13 15:18:42,297 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-350016\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 15:18:42,298 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-350016/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 15:18:42,298 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-350016/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 15:18:43,611 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-350016/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 15:18:43,612 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-350016/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 15:18:43,612 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-350016/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 15:18:43,613 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-350016/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 15:18:47,975 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 15:18:47,975 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 15:18:47,977 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 15:18:47,986 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-346370] due to args.save_total_limit\n",
      "{'loss': 0.1077, 'grad_norm': 3.162787675857544, 'learning_rate': 1.933625891387822e-06, 'epoch': 96.13}\n",
      "{'loss': 0.1084, 'grad_norm': 3.5869784355163574, 'learning_rate': 1.8650575973669776e-06, 'epoch': 96.27}\n",
      "{'loss': 0.1075, 'grad_norm': 3.4423720836639404, 'learning_rate': 1.7964893033461327e-06, 'epoch': 96.41}\n",
      "{'loss': 0.1081, 'grad_norm': 3.4415297508239746, 'learning_rate': 1.727921009325288e-06, 'epoch': 96.54}\n",
      "{'loss': 0.1088, 'grad_norm': 2.9299986362457275, 'learning_rate': 1.6593527153044432e-06, 'epoch': 96.68}\n",
      "{'loss': 0.1073, 'grad_norm': 3.395812511444092, 'learning_rate': 1.5907844212835987e-06, 'epoch': 96.82}\n",
      "{'loss': 0.1086, 'grad_norm': 3.1126651763916016, 'learning_rate': 1.5222161272627538e-06, 'epoch': 96.96}\n",
      " 97%|███████████████████████████████ | 353662/364600 [25:58:09<44:49,  4.07it/s][INFO|trainer.py:3388] 2024-05-13 15:35:17,928 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-353662\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 15:35:17,929 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-353662/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 15:35:17,929 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-353662/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 15:35:19,222 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-353662/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 15:35:19,224 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-353662/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 15:35:19,224 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-353662/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 15:35:19,225 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-353662/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 15:35:23,715 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 15:35:23,715 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 15:35:23,717 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 15:35:23,722 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-350016] due to args.save_total_limit\n",
      "{'loss': 0.1067, 'grad_norm': 2.898881435394287, 'learning_rate': 1.453647833241909e-06, 'epoch': 97.09}\n",
      "{'loss': 0.1082, 'grad_norm': 3.300261974334717, 'learning_rate': 1.3850795392210643e-06, 'epoch': 97.23}\n",
      "{'loss': 0.1073, 'grad_norm': 3.1039366722106934, 'learning_rate': 1.3165112452002194e-06, 'epoch': 97.37}\n",
      "{'loss': 0.1075, 'grad_norm': 3.544015645980835, 'learning_rate': 1.2479429511793747e-06, 'epoch': 97.5}\n",
      "{'loss': 0.1075, 'grad_norm': 2.692314624786377, 'learning_rate': 1.17937465715853e-06, 'epoch': 97.64}\n",
      "{'loss': 0.1077, 'grad_norm': 2.966008186340332, 'learning_rate': 1.1108063631376851e-06, 'epoch': 97.78}\n",
      "{'loss': 0.1076, 'grad_norm': 2.9783902168273926, 'learning_rate': 1.0422380691168404e-06, 'epoch': 97.92}\n",
      " 98%|███████████████████████████████▎| 357308/364600 [26:14:27<27:40,  4.39it/s][INFO|trainer.py:3388] 2024-05-13 15:51:35,610 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-357308\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 15:51:35,610 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-357308/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 15:51:35,611 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-357308/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 15:51:36,936 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-357308/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 15:51:36,937 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-357308/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 15:51:36,937 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-357308/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 15:51:36,938 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-357308/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 15:51:41,483 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 15:51:41,484 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 15:51:41,486 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 15:51:41,498 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-353662] due to args.save_total_limit\n",
      "{'loss': 0.1052, 'grad_norm': 3.1313674449920654, 'learning_rate': 9.736697750959958e-07, 'epoch': 98.05}\n",
      "{'loss': 0.1038, 'grad_norm': 3.143101453781128, 'learning_rate': 9.05101481075151e-07, 'epoch': 98.19}\n",
      "{'loss': 0.1059, 'grad_norm': 3.30661940574646, 'learning_rate': 8.365331870543062e-07, 'epoch': 98.33}\n",
      "{'loss': 0.1062, 'grad_norm': 3.1109259128570557, 'learning_rate': 7.679648930334613e-07, 'epoch': 98.46}\n",
      "{'loss': 0.1037, 'grad_norm': 3.4787518978118896, 'learning_rate': 6.993965990126166e-07, 'epoch': 98.6}\n",
      "{'loss': 0.1056, 'grad_norm': 3.0321710109710693, 'learning_rate': 6.308283049917719e-07, 'epoch': 98.74}\n",
      "{'loss': 0.1058, 'grad_norm': 3.13843035697937, 'learning_rate': 5.622600109709271e-07, 'epoch': 98.88}\n",
      " 99%|███████████████████████████████▋| 360954/364600 [26:30:25<12:58,  4.68it/s][INFO|trainer.py:3388] 2024-05-13 16:07:34,038 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-360954\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 16:07:34,038 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-360954/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 16:07:34,039 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-360954/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 16:07:35,372 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-360954/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 16:07:35,373 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-360954/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 16:07:35,373 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-360954/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 16:07:35,374 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-360954/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 16:07:39,946 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 16:07:39,947 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 16:07:39,948 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 16:07:39,954 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-357308] due to args.save_total_limit\n",
      "{'loss': 0.1051, 'grad_norm': 2.8458125591278076, 'learning_rate': 4.936917169500823e-07, 'epoch': 99.01}\n",
      "{'loss': 0.1039, 'grad_norm': 2.781649589538574, 'learning_rate': 4.2512342292923756e-07, 'epoch': 99.15}\n",
      "{'loss': 0.1046, 'grad_norm': 3.680230140686035, 'learning_rate': 3.565551289083928e-07, 'epoch': 99.29}\n",
      "{'loss': 0.1064, 'grad_norm': 3.4057164192199707, 'learning_rate': 2.87986834887548e-07, 'epoch': 99.42}\n",
      "{'loss': 0.1056, 'grad_norm': 3.2353737354278564, 'learning_rate': 2.1941854086670326e-07, 'epoch': 99.56}\n",
      "{'loss': 0.1034, 'grad_norm': 3.273487091064453, 'learning_rate': 1.5085024684585848e-07, 'epoch': 99.7}\n",
      "{'loss': 0.1058, 'grad_norm': 2.404613733291626, 'learning_rate': 8.228195282501371e-08, 'epoch': 99.84}\n",
      "{'loss': 0.1047, 'grad_norm': 3.081162691116333, 'learning_rate': 1.3713658804168954e-08, 'epoch': 99.97}\n",
      "100%|████████████████████████████████| 364600/364600 [26:46:13<00:00,  3.94it/s][INFO|trainer.py:3388] 2024-05-13 16:23:22,360 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038/checkpoint-364600\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 16:23:22,361 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-364600/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 16:23:22,361 >> Configuration saved in /t5-big-finetuned-iwslt2038/checkpoint-364600/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 16:23:23,682 >> Model weights saved in /t5-big-finetuned-iwslt2038/checkpoint-364600/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 16:23:23,684 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/checkpoint-364600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 16:23:23,684 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/checkpoint-364600/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 16:23:23,685 >> Copy vocab file to /t5-big-finetuned-iwslt2038/checkpoint-364600/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 16:23:28,572 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 16:23:28,594 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 16:23:28,596 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3480] 2024-05-13 16:23:28,605 >> Deleting older checkpoint [/t5-big-finetuned-iwslt2038/checkpoint-360954] due to args.save_total_limit\n",
      "[INFO|trainer.py:2329] 2024-05-13 16:23:28,965 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 96380.4057, 'train_samples_per_second': 121.035, 'train_steps_per_second': 3.783, 'train_loss': 1.123260397659967, 'epoch': 100.0}\n",
      "100%|████████████████████████████████| 364600/364600 [26:46:20<00:00,  3.78it/s]\n",
      "[INFO|trainer.py:4148] 2024-05-13 16:23:28,966 >> Waiting for the current checkpoint push to be finished, this might take a couple of minutes.\n",
      "[INFO|trainer.py:3388] 2024-05-13 16:24:10,278 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 16:24:10,279 >> Configuration saved in /t5-big-finetuned-iwslt2038/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 16:24:10,279 >> Configuration saved in /t5-big-finetuned-iwslt2038/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 16:24:13,585 >> Model weights saved in /t5-big-finetuned-iwslt2038/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 16:24:13,587 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 16:24:13,587 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 16:24:13,589 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|trainer.py:3388] 2024-05-13 16:24:13,597 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 16:24:13,598 >> Configuration saved in /t5-big-finetuned-iwslt2038/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 16:24:13,599 >> Configuration saved in /t5-big-finetuned-iwslt2038/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 16:24:16,498 >> Model weights saved in /t5-big-finetuned-iwslt2038/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 16:24:16,499 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 16:24:16,499 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 16:24:16,501 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|modelcard.py:450] 2024-05-13 16:24:16,766 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Sequence-to-sequence Language Modeling', 'type': 'text2text-generation'}}\n",
      "***** train metrics *****\n",
      "  epoch                    =             100.0\n",
      "  total_flos               =      1031076719GF\n",
      "  train_loss               =            1.1233\n",
      "  train_runtime            = 1 day, 2:46:20.40\n",
      "  train_samples            =            116654\n",
      "  train_samples_per_second =           121.035\n",
      "  train_steps_per_second   =             3.783\n",
      "05/13/2024 16:24:19 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:3697] 2024-05-13 16:24:19,612 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3699] 2024-05-13 16:24:19,612 >>   Num examples = 888\n",
      "[INFO|trainer.py:3702] 2024-05-13 16:24:19,612 >>   Batch size = 64\n",
      "100%|███████████████████████████████████████████| 14/14 [00:09<00:00,  1.50it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =      100.0\n",
      "  eval_bleu               =      0.186\n",
      "  eval_gen_len            =    25.4842\n",
      "  eval_loss               =     4.1846\n",
      "  eval_runtime            = 0:00:09.75\n",
      "  eval_samples            =        888\n",
      "  eval_samples_per_second =     91.044\n",
      "  eval_steps_per_second   =      1.435\n",
      "[INFO|trainer.py:3697] 2024-05-13 16:24:29,368 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:3699] 2024-05-13 16:24:29,368 >>   Num examples = 1138\n",
      "[INFO|trainer.py:3702] 2024-05-13 16:24:29,368 >>   Batch size = 64\n",
      "100%|███████████████████████████████████████████| 18/18 [00:09<00:00,  1.88it/s]\n",
      "***** predict metrics *****\n",
      "  predict_bleu               =     0.1466\n",
      "  predict_gen_len            =    20.5026\n",
      "  predict_loss               =     5.1046\n",
      "  predict_runtime            = 0:00:10.54\n",
      "  predict_samples_per_second =    107.943\n",
      "  predict_steps_per_second   =      1.707\n",
      "05/13/2024 16:24:39 - INFO - __main__ - *** Predict ***\n",
      "[INFO|trainer.py:3697] 2024-05-13 16:24:39,912 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:3699] 2024-05-13 16:24:39,912 >>   Num examples = 1138\n",
      "[INFO|trainer.py:3702] 2024-05-13 16:24:39,912 >>   Batch size = 64\n",
      "100%|███████████████████████████████████████████| 18/18 [00:09<00:00,  1.88it/s]\n",
      "***** predict metrics *****\n",
      "  predict_bleu               =     0.1466\n",
      "  predict_gen_len            =    20.5026\n",
      "  predict_loss               =     5.1046\n",
      "  predict_runtime            = 0:00:10.52\n",
      "  predict_samples            =       1138\n",
      "  predict_samples_per_second =    108.148\n",
      "  predict_steps_per_second   =      1.711\n",
      "[INFO|trainer.py:3388] 2024-05-13 16:24:50,456 >> Saving model checkpoint to /t5-big-finetuned-iwslt2038\n",
      "[INFO|configuration_utils.py:471] 2024-05-13 16:24:50,457 >> Configuration saved in /t5-big-finetuned-iwslt2038/config.json\n",
      "[INFO|configuration_utils.py:695] 2024-05-13 16:24:50,457 >> Configuration saved in /t5-big-finetuned-iwslt2038/generation_config.json\n",
      "[INFO|modeling_utils.py:2595] 2024-05-13 16:24:53,698 >> Model weights saved in /t5-big-finetuned-iwslt2038/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2503] 2024-05-13 16:24:53,699 >> tokenizer config file saved in /t5-big-finetuned-iwslt2038/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2512] 2024-05-13 16:24:53,699 >> Special tokens file saved in /t5-big-finetuned-iwslt2038/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-05-13 16:24:53,700 >> Copy vocab file to /t5-big-finetuned-iwslt2038/spiece.model\n",
      "[INFO|modelcard.py:450] 2024-05-13 16:24:53,927 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Sequence-to-sequence Language Modeling', 'type': 'text2text-generation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 0.186}]}\n",
      "events.out.tfevents.1715617469.0d573eeffc83.798280.1: 100%|█| 465/465 [00:00<00:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] =\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "!cp run_translation_big_huggingface_example.py transformers-main/examples/pytorch/translation/run_translation_bigger.py\n",
    "!python transformers-main/examples/pytorch/translation/run_translation_bigger.py \\\n",
    "    --model_name_or_path google-t5/t5-small \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --do_predict \\\n",
    "    --source_lang de \\\n",
    "    --target_lang en \\\n",
    "    --source_prefix \"translate German to English: \" \\\n",
    "    --train_file data/iwslt17.de.en/train-de-en.json \\\n",
    "    --validation_file data/iwslt17.de.en/validation-de-en.json \\\n",
    "    --test_file data/iwslt17.de.en/test-de-en.json \\\n",
    "    --output_dir /t5-big-scatch-iwslt2017 \\\n",
    "    --per_device_train_batch_size=32 \\\n",
    "    --per_device_eval_batch_size=64 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --predict_with_generate    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O ./t5-small-scratch-iwslt2017/all_results.json https://huggingface.co/minseok0809/t5-small-scratch-iwslt2017/raw/main/all_results.json\n",
    "!wget -O ./t5-small-scratch-iwslt2017/generated_predictions.csv https://huggingface.co/minseok0809/t5-small-scratch-iwslt2017/raw/main/generated_predictions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O ./t5-big-scratch-iwslt2017/all_results.json https://huggingface.co/minseok0809/t5-big-scratch-iwslt2017/raw/main/all_results.json\n",
    "!wget -O ./t5-big-scratch-iwslt2017/generated_predictions.csv https://huggingface.co/minseok0809/t5-big-scratch-iwslt2017/raw/main/generated_predictions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Bleu: 0.1921\n"
     ]
    }
   ],
   "source": [
    "result_json_path = 't5-small-scratch-iwslt2017/all_results.json'\n",
    "\n",
    "with open(result_json_path) as f: \n",
    "    result_json = json.load(f) \n",
    "\n",
    "print(\"Test Bleu: {}\".format(result_json['predict_bleu']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Bleu: 0.1466\n"
     ]
    }
   ],
   "source": [
    "result_json_path = 't5-big-scratch-iwslt2017/all_results.json'\n",
    "\n",
    "with open(result_json_path) as f: \n",
    "    result_json = json.load(f) \n",
    "\n",
    "print(\"Test Bleu: {}\".format(result_json['predict_bleu']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[HuggingFace: Transformers on Translation tasks](https://github.com/huggingface/transformers/tree/main/examples/pytorch/tranlsation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
